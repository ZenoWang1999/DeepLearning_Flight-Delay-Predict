{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = pd.read_csv('data/data_encoded_scaled.csv')\n",
    "data_cleaned = pd.read_csv('data/data_cleaned.csv')\n",
    "data_feature = data_cleaned.drop(columns=['TOTAL_DELAY', 'DEP_DEL15'])\n",
    "data_target = data_cleaned['DEP_DEL15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = data_encoded_scaled.drop(columns=['RESIDUALS', 'DEP_DEL15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.968432919954903\n"
     ]
    }
   ],
   "source": [
    "sequence_days = 7\n",
    "daily_counts = data_feature.groupby(['MONTH', 'DAY_OF_MONTH', 'DEPARTING_AIRPORT']).size() # 一天一个机场的航班数\n",
    "average_rows = daily_counts.mean()\n",
    "\n",
    "print(average_rows*sequence_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_encoded_scaled.assign(TARGET=data_target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MONTH  DAY_OF_MONTH  DAY_OF_WEEK  DEP_TIME_BLK  DISTANCE_GROUP  \\\n",
      "0    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "1    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "2    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "3    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "4    0.0           0.0     0.166667      0.166667        0.285714   \n",
      "\n",
      "   SEGMENT_NUMBER  CONCURRENT_FLIGHTS  NUMBER_OF_SEATS  AIRPORT_FLIGHTS_MONTH  \\\n",
      "0           0.000            0.098592              1.0               0.115453   \n",
      "1           0.000            0.098592              1.0               0.115453   \n",
      "2           0.000            0.112676              1.0               0.111384   \n",
      "3           0.000            0.267606              1.0               0.333661   \n",
      "4           0.125            0.042254              1.0               0.028528   \n",
      "\n",
      "   AIRLINE_FLIGHTS_MONTH  ...  PREVIOUS_AIRPORT_3  PREVIOUS_AIRPORT_4  \\\n",
      "0               0.183515  ...                 0.0                 0.0   \n",
      "1               0.183515  ...                 0.0                 0.0   \n",
      "2               0.183515  ...                 0.0                 0.0   \n",
      "3               0.183515  ...                 0.0                 0.0   \n",
      "4               0.183515  ...                 0.0                 0.0   \n",
      "\n",
      "   PREVIOUS_AIRPORT_5  PREVIOUS_AIRPORT_6  PRCP  SNOW  SNWD      TMAX  \\\n",
      "0                 0.0                 1.0   0.0   0.0   0.0  0.353982   \n",
      "1                 0.0                 1.0   0.0   0.0   0.0  0.353982   \n",
      "2                 0.0                 1.0   0.0   0.0   0.0  0.451327   \n",
      "3                 0.0                 1.0   0.0   0.0   0.0  0.699115   \n",
      "4                 1.0                 0.0   0.0   0.0   0.0  0.460177   \n",
      "\n",
      "       AWND  TARGET  \n",
      "0  0.198638       0  \n",
      "1  0.198638       0  \n",
      "2  0.172291       0  \n",
      "3  0.370930       0  \n",
      "4  0.178804       0  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences shape: (41100, 28, 32)\n",
      "Target values shape: (41100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequence_length = int(round(average_rows*sequence_days))\n",
    "\n",
    "unique_dep_airport = data_feature['DEPARTING_AIRPORT'].unique()\n",
    "unique_flight_number = data_feature['FLIGHT_NUMBER'].unique()\n",
    "\n",
    "X_sequences = []\n",
    "y_targets = []\n",
    "\n",
    "for dep_airport in unique_dep_airport:\n",
    "    flight_data = data_full[data_feature['DEPARTING_AIRPORT'] == dep_airport]\n",
    "    flight_data_values = flight_data.iloc[:, :-1].values\n",
    "    flight_data_target = flight_data.iloc[:, -1].values\n",
    "    for i in range(len(flight_data) - sequence_length):\n",
    "        X_sequences.append(flight_data_values[i:i+sequence_length])\n",
    "        y_targets.append(flight_data_target[i+sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_targets = np.array(y_targets)\n",
    "\n",
    "print(\"Input sequences shape:\", X_sequences.shape)\n",
    "print(\"Target values shape:\", y_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = list(zip(X_sequences, y_targets))\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Separate the sequences and targets\n",
    "X_train, y_train = zip(*train_data)\n",
    "X_test, y_test = zip(*test_data)\n",
    "\n",
    "# Convert the results back to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "timesteps = X_train.shape[1]\n",
    "input_dim = X_train.shape[2]\n",
    "\n",
    "weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train) , y = y_train)\n",
    "class_weights = dict(zip(np.unique(y_train), weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 1.2006 - accuracy: 0.4816 - recall_3: 0.6364\n",
      "Epoch 1: val_loss improved from inf to 0.99428, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 7s 7ms/step - loss: 1.1998 - accuracy: 0.4819 - recall_3: 0.6365 - val_loss: 0.9943 - val_accuracy: 0.5421 - val_recall_3: 0.5967\n",
      "Epoch 2/300\n",
      " 27/822 [..............................] - ETA: 4s - loss: 1.0114 - accuracy: 0.5440 - recall_3: 0.6592"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "814/822 [============================>.] - ETA: 0s - loss: 0.8943 - accuracy: 0.5260 - recall_3: 0.6213\n",
      "Epoch 2: val_loss improved from 0.99428 to 0.82258, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.8933 - accuracy: 0.5261 - recall_3: 0.6220 - val_loss: 0.8226 - val_accuracy: 0.5137 - val_recall_3: 0.6614\n",
      "Epoch 3/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.7832 - accuracy: 0.5222 - recall_3: 0.6257\n",
      "Epoch 3: val_loss improved from 0.82258 to 0.76109, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.7833 - accuracy: 0.5220 - recall_3: 0.6258 - val_loss: 0.7611 - val_accuracy: 0.5067 - val_recall_3: 0.6855\n",
      "Epoch 4/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.7402 - accuracy: 0.5240 - recall_3: 0.6333\n",
      "Epoch 4: val_loss improved from 0.76109 to 0.71910, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.7405 - accuracy: 0.5238 - recall_3: 0.6321 - val_loss: 0.7191 - val_accuracy: 0.5575 - val_recall_3: 0.6224\n",
      "Epoch 5/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.7228 - accuracy: 0.5339 - recall_3: 0.6122\n",
      "Epoch 5: val_loss did not improve from 0.71910\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.7227 - accuracy: 0.5338 - recall_3: 0.6135 - val_loss: 0.7409 - val_accuracy: 0.4694 - val_recall_3: 0.7320\n",
      "Epoch 6/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.7136 - accuracy: 0.5249 - recall_3: 0.6391\n",
      "Epoch 6: val_loss improved from 0.71910 to 0.69937, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.7135 - accuracy: 0.5247 - recall_3: 0.6381 - val_loss: 0.6994 - val_accuracy: 0.5677 - val_recall_3: 0.6066\n",
      "Epoch 7/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.7067 - accuracy: 0.5401 - recall_3: 0.6267\n",
      "Epoch 7: val_loss did not improve from 0.69937\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.7066 - accuracy: 0.5406 - recall_3: 0.6270 - val_loss: 0.7107 - val_accuracy: 0.5193 - val_recall_3: 0.6788\n",
      "Epoch 8/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.7034 - accuracy: 0.5332 - recall_3: 0.6307\n",
      "Epoch 8: val_loss improved from 0.69937 to 0.68430, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.7034 - accuracy: 0.5332 - recall_3: 0.6307 - val_loss: 0.6843 - val_accuracy: 0.5873 - val_recall_3: 0.5668\n",
      "Epoch 9/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6992 - accuracy: 0.5368 - recall_3: 0.6208\n",
      "Epoch 9: val_loss did not improve from 0.68430\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6993 - accuracy: 0.5366 - recall_3: 0.6206 - val_loss: 0.6955 - val_accuracy: 0.5456 - val_recall_3: 0.6382\n",
      "Epoch 10/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6963 - accuracy: 0.5303 - recall_3: 0.6385\n",
      "Epoch 10: val_loss improved from 0.68430 to 0.68202, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6962 - accuracy: 0.5305 - recall_3: 0.6387 - val_loss: 0.6820 - val_accuracy: 0.5777 - val_recall_3: 0.5917\n",
      "Epoch 11/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6939 - accuracy: 0.5318 - recall_3: 0.6291\n",
      "Epoch 11: val_loss improved from 0.68202 to 0.66865, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6941 - accuracy: 0.5322 - recall_3: 0.6289 - val_loss: 0.6687 - val_accuracy: 0.6081 - val_recall_3: 0.5154\n",
      "Epoch 12/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6919 - accuracy: 0.5424 - recall_3: 0.6271\n",
      "Epoch 12: val_loss did not improve from 0.66865\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6921 - accuracy: 0.5425 - recall_3: 0.6268 - val_loss: 0.6856 - val_accuracy: 0.5546 - val_recall_3: 0.6290\n",
      "Epoch 13/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6917 - accuracy: 0.5311 - recall_3: 0.6240\n",
      "Epoch 13: val_loss did not improve from 0.66865\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6914 - accuracy: 0.5309 - recall_3: 0.6234 - val_loss: 0.6712 - val_accuracy: 0.5922 - val_recall_3: 0.5643\n",
      "Epoch 14/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6900 - accuracy: 0.5340 - recall_3: 0.6199\n",
      "Epoch 14: val_loss did not improve from 0.66865\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6898 - accuracy: 0.5338 - recall_3: 0.6202 - val_loss: 0.6802 - val_accuracy: 0.5661 - val_recall_3: 0.6116\n",
      "Epoch 15/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6881 - accuracy: 0.5384 - recall_3: 0.6296\n",
      "Epoch 15: val_loss did not improve from 0.66865\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6884 - accuracy: 0.5385 - recall_3: 0.6299 - val_loss: 0.6746 - val_accuracy: 0.5803 - val_recall_3: 0.5884\n",
      "Epoch 16/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6880 - accuracy: 0.5384 - recall_3: 0.6163\n",
      "Epoch 16: val_loss did not improve from 0.66865\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6882 - accuracy: 0.5381 - recall_3: 0.6155 - val_loss: 0.6916 - val_accuracy: 0.5240 - val_recall_3: 0.6631\n",
      "Epoch 17/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6869 - accuracy: 0.5385 - recall_3: 0.6219\n",
      "Epoch 17: val_loss did not improve from 0.66865\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6869 - accuracy: 0.5382 - recall_3: 0.6210 - val_loss: 0.6936 - val_accuracy: 0.5196 - val_recall_3: 0.6763\n",
      "Epoch 18/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6869 - accuracy: 0.5316 - recall_3: 0.6239\n",
      "Epoch 18: val_loss did not improve from 0.66865\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6868 - accuracy: 0.5315 - recall_3: 0.6242 - val_loss: 0.6925 - val_accuracy: 0.5154 - val_recall_3: 0.6797\n",
      "Epoch 19/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6872 - accuracy: 0.5344 - recall_3: 0.6236\n",
      "Epoch 19: val_loss did not improve from 0.66865\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6869 - accuracy: 0.5347 - recall_3: 0.6228 - val_loss: 0.6735 - val_accuracy: 0.5754 - val_recall_3: 0.5959\n",
      "Epoch 20/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6853 - accuracy: 0.5416 - recall_3: 0.6272\n",
      "Epoch 20: val_loss improved from 0.66865 to 0.66021, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6848 - accuracy: 0.5422 - recall_3: 0.6268 - val_loss: 0.6602 - val_accuracy: 0.6115 - val_recall_3: 0.5137\n",
      "Epoch 21/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6851 - accuracy: 0.5430 - recall_3: 0.6050\n",
      "Epoch 21: val_loss did not improve from 0.66021\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6851 - accuracy: 0.5425 - recall_3: 0.6059 - val_loss: 0.6806 - val_accuracy: 0.5549 - val_recall_3: 0.6166\n",
      "Epoch 22/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6853 - accuracy: 0.5356 - recall_3: 0.6151\n",
      "Epoch 22: val_loss did not improve from 0.66021\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6850 - accuracy: 0.5362 - recall_3: 0.6159 - val_loss: 0.6817 - val_accuracy: 0.5522 - val_recall_3: 0.6224\n",
      "Epoch 23/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6836 - accuracy: 0.5404 - recall_3: 0.6089\n",
      "Epoch 23: val_loss did not improve from 0.66021\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6840 - accuracy: 0.5407 - recall_3: 0.6089 - val_loss: 0.6907 - val_accuracy: 0.5216 - val_recall_3: 0.6747\n",
      "Epoch 24/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6841 - accuracy: 0.5352 - recall_3: 0.6351\n",
      "Epoch 24: val_loss did not improve from 0.66021\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6841 - accuracy: 0.5352 - recall_3: 0.6351 - val_loss: 0.6759 - val_accuracy: 0.5634 - val_recall_3: 0.6124\n",
      "Epoch 25/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6831 - accuracy: 0.5365 - recall_3: 0.6278\n",
      "Epoch 25: val_loss did not improve from 0.66021\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6834 - accuracy: 0.5363 - recall_3: 0.6279 - val_loss: 0.6703 - val_accuracy: 0.5766 - val_recall_3: 0.5851\n",
      "Epoch 26/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6837 - accuracy: 0.5423 - recall_3: 0.6155\n",
      "Epoch 26: val_loss did not improve from 0.66021\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6834 - accuracy: 0.5421 - recall_3: 0.6151 - val_loss: 0.6958 - val_accuracy: 0.5056 - val_recall_3: 0.6896\n",
      "Epoch 27/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6825 - accuracy: 0.5355 - recall_3: 0.6232\n",
      "Epoch 27: val_loss improved from 0.66021 to 0.65817, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6822 - accuracy: 0.5362 - recall_3: 0.6230 - val_loss: 0.6582 - val_accuracy: 0.6127 - val_recall_3: 0.5187\n",
      "Epoch 28/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6830 - accuracy: 0.5468 - recall_3: 0.6040\n",
      "Epoch 28: val_loss did not improve from 0.65817\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6830 - accuracy: 0.5469 - recall_3: 0.6038 - val_loss: 0.6841 - val_accuracy: 0.5427 - val_recall_3: 0.6465\n",
      "Epoch 29/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6824 - accuracy: 0.5362 - recall_3: 0.6284\n",
      "Epoch 29: val_loss did not improve from 0.65817\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6821 - accuracy: 0.5368 - recall_3: 0.6276 - val_loss: 0.6671 - val_accuracy: 0.5858 - val_recall_3: 0.5718\n",
      "Epoch 30/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6822 - accuracy: 0.5435 - recall_3: 0.6156\n",
      "Epoch 30: val_loss did not improve from 0.65817\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6822 - accuracy: 0.5434 - recall_3: 0.6151 - val_loss: 0.6786 - val_accuracy: 0.5529 - val_recall_3: 0.6299\n",
      "Epoch 31/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6833 - accuracy: 0.5406 - recall_3: 0.6216\n",
      "Epoch 31: val_loss did not improve from 0.65817\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6834 - accuracy: 0.5405 - recall_3: 0.6214 - val_loss: 0.6801 - val_accuracy: 0.5447 - val_recall_3: 0.6390\n",
      "Epoch 32/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6828 - accuracy: 0.5366 - recall_3: 0.6248\n",
      "Epoch 32: val_loss did not improve from 0.65817\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6828 - accuracy: 0.5366 - recall_3: 0.6248 - val_loss: 0.6763 - val_accuracy: 0.5575 - val_recall_3: 0.6174\n",
      "Epoch 33/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6822 - accuracy: 0.5407 - recall_3: 0.6227\n",
      "Epoch 33: val_loss did not improve from 0.65817\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6824 - accuracy: 0.5408 - recall_3: 0.6220 - val_loss: 0.6801 - val_accuracy: 0.5420 - val_recall_3: 0.6465\n",
      "Epoch 34/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.5366 - recall_3: 0.6275\n",
      "Epoch 34: val_loss did not improve from 0.65817\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6822 - accuracy: 0.5371 - recall_3: 0.6272 - val_loss: 0.6667 - val_accuracy: 0.5842 - val_recall_3: 0.5759\n",
      "Epoch 35/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.5477 - recall_3: 0.6072\n",
      "Epoch 35: val_loss did not improve from 0.65817\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6820 - accuracy: 0.5476 - recall_3: 0.6069 - val_loss: 0.7006 - val_accuracy: 0.4910 - val_recall_3: 0.7104\n",
      "Epoch 36/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.5313 - recall_3: 0.6257\n",
      "Epoch 36: val_loss did not improve from 0.65817\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6821 - accuracy: 0.5314 - recall_3: 0.6256 - val_loss: 0.6755 - val_accuracy: 0.5593 - val_recall_3: 0.6232\n",
      "Epoch 37/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6818 - accuracy: 0.5367 - recall_3: 0.6163\n",
      "Epoch 37: val_loss improved from 0.65817 to 0.65694, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6820 - accuracy: 0.5366 - recall_3: 0.6159 - val_loss: 0.6569 - val_accuracy: 0.6061 - val_recall_3: 0.5336\n",
      "Epoch 38/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6812 - accuracy: 0.5406 - recall_3: 0.6146\n",
      "Epoch 38: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6812 - accuracy: 0.5407 - recall_3: 0.6145 - val_loss: 0.6678 - val_accuracy: 0.5820 - val_recall_3: 0.5900\n",
      "Epoch 39/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6803 - accuracy: 0.5432 - recall_3: 0.6157\n",
      "Epoch 39: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6807 - accuracy: 0.5433 - recall_3: 0.6151 - val_loss: 0.6760 - val_accuracy: 0.5628 - val_recall_3: 0.6232\n",
      "Epoch 40/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.5367 - recall_3: 0.6136\n",
      "Epoch 40: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6816 - accuracy: 0.5366 - recall_3: 0.6133 - val_loss: 0.6825 - val_accuracy: 0.5391 - val_recall_3: 0.6473\n",
      "Epoch 41/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.5394 - recall_3: 0.6157\n",
      "Epoch 41: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6820 - accuracy: 0.5399 - recall_3: 0.6161 - val_loss: 0.6806 - val_accuracy: 0.5470 - val_recall_3: 0.6390\n",
      "Epoch 42/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6818 - accuracy: 0.5397 - recall_3: 0.6109\n",
      "Epoch 42: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6820 - accuracy: 0.5401 - recall_3: 0.6109 - val_loss: 0.6720 - val_accuracy: 0.5722 - val_recall_3: 0.5950\n",
      "Epoch 43/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6827 - accuracy: 0.5393 - recall_3: 0.6097\n",
      "Epoch 43: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6827 - accuracy: 0.5394 - recall_3: 0.6097 - val_loss: 0.6764 - val_accuracy: 0.5599 - val_recall_3: 0.6199\n",
      "Epoch 44/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6808 - accuracy: 0.5429 - recall_3: 0.6191\n",
      "Epoch 44: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6806 - accuracy: 0.5428 - recall_3: 0.6186 - val_loss: 0.6787 - val_accuracy: 0.5564 - val_recall_3: 0.6307\n",
      "Epoch 45/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.5386 - recall_3: 0.6218\n",
      "Epoch 45: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6815 - accuracy: 0.5387 - recall_3: 0.6214 - val_loss: 0.6638 - val_accuracy: 0.5917 - val_recall_3: 0.5627\n",
      "Epoch 46/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6807 - accuracy: 0.5457 - recall_3: 0.6176\n",
      "Epoch 46: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6810 - accuracy: 0.5453 - recall_3: 0.6180 - val_loss: 0.6818 - val_accuracy: 0.5452 - val_recall_3: 0.6365\n",
      "Epoch 47/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6811 - accuracy: 0.5435 - recall_3: 0.6173\n",
      "Epoch 47: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6808 - accuracy: 0.5439 - recall_3: 0.6184 - val_loss: 0.6843 - val_accuracy: 0.5371 - val_recall_3: 0.6506\n",
      "Epoch 48/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6813 - accuracy: 0.5468 - recall_3: 0.6101\n",
      "Epoch 48: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6814 - accuracy: 0.5470 - recall_3: 0.6099 - val_loss: 0.6878 - val_accuracy: 0.5243 - val_recall_3: 0.6714\n",
      "Epoch 49/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6814 - accuracy: 0.5354 - recall_3: 0.6202\n",
      "Epoch 49: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6814 - accuracy: 0.5354 - recall_3: 0.6202 - val_loss: 0.6787 - val_accuracy: 0.5512 - val_recall_3: 0.6340\n",
      "Epoch 50/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6812 - accuracy: 0.5432 - recall_3: 0.6153\n",
      "Epoch 50: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6814 - accuracy: 0.5431 - recall_3: 0.6155 - val_loss: 0.6922 - val_accuracy: 0.5099 - val_recall_3: 0.6838\n",
      "Epoch 51/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.5394 - recall_3: 0.6189\n",
      "Epoch 51: val_loss did not improve from 0.65694\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6813 - accuracy: 0.5397 - recall_3: 0.6186 - val_loss: 0.6673 - val_accuracy: 0.5794 - val_recall_3: 0.5900\n",
      "Epoch 52/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6811 - accuracy: 0.5398 - recall_3: 0.6094\n",
      "Epoch 52: val_loss improved from 0.65694 to 0.65230, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6813 - accuracy: 0.5404 - recall_3: 0.6083 - val_loss: 0.6523 - val_accuracy: 0.6229 - val_recall_3: 0.5054\n",
      "Epoch 53/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.5455 - recall_3: 0.6109\n",
      "Epoch 53: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6804 - accuracy: 0.5449 - recall_3: 0.6107 - val_loss: 0.6705 - val_accuracy: 0.5690 - val_recall_3: 0.6083\n",
      "Epoch 54/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6805 - accuracy: 0.5393 - recall_3: 0.6207\n",
      "Epoch 54: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6806 - accuracy: 0.5394 - recall_3: 0.6204 - val_loss: 0.6565 - val_accuracy: 0.6042 - val_recall_3: 0.5394\n",
      "Epoch 55/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.5418 - recall_3: 0.6054\n",
      "Epoch 55: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6814 - accuracy: 0.5420 - recall_3: 0.6052 - val_loss: 0.6834 - val_accuracy: 0.5350 - val_recall_3: 0.6539\n",
      "Epoch 56/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6806 - accuracy: 0.5419 - recall_3: 0.6127\n",
      "Epoch 56: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6808 - accuracy: 0.5416 - recall_3: 0.6129 - val_loss: 0.6816 - val_accuracy: 0.5415 - val_recall_3: 0.6473\n",
      "Epoch 57/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.5394 - recall_3: 0.6128\n",
      "Epoch 57: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6817 - accuracy: 0.5398 - recall_3: 0.6131 - val_loss: 0.6856 - val_accuracy: 0.5262 - val_recall_3: 0.6697\n",
      "Epoch 58/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.5408 - recall_3: 0.6127\n",
      "Epoch 58: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6804 - accuracy: 0.5404 - recall_3: 0.6129 - val_loss: 0.6925 - val_accuracy: 0.5085 - val_recall_3: 0.6938\n",
      "Epoch 59/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6815 - accuracy: 0.5366 - recall_3: 0.6208\n",
      "Epoch 59: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6813 - accuracy: 0.5367 - recall_3: 0.6204 - val_loss: 0.6734 - val_accuracy: 0.5625 - val_recall_3: 0.6149\n",
      "Epoch 60/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6815 - accuracy: 0.5405 - recall_3: 0.6153\n",
      "Epoch 60: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6814 - accuracy: 0.5400 - recall_3: 0.6155 - val_loss: 0.6858 - val_accuracy: 0.5313 - val_recall_3: 0.6647\n",
      "Epoch 61/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6796 - accuracy: 0.5425 - recall_3: 0.6099\n",
      "Epoch 61: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6800 - accuracy: 0.5426 - recall_3: 0.6107 - val_loss: 0.6843 - val_accuracy: 0.5365 - val_recall_3: 0.6515\n",
      "Epoch 62/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6800 - accuracy: 0.5408 - recall_3: 0.6217\n",
      "Epoch 62: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6801 - accuracy: 0.5408 - recall_3: 0.6224 - val_loss: 0.6875 - val_accuracy: 0.5239 - val_recall_3: 0.6689\n",
      "Epoch 63/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6802 - accuracy: 0.5405 - recall_3: 0.6187\n",
      "Epoch 63: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6804 - accuracy: 0.5405 - recall_3: 0.6190 - val_loss: 0.6880 - val_accuracy: 0.5210 - val_recall_3: 0.6689\n",
      "Epoch 64/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6810 - accuracy: 0.5294 - recall_3: 0.6304\n",
      "Epoch 64: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6804 - accuracy: 0.5296 - recall_3: 0.6291 - val_loss: 0.6544 - val_accuracy: 0.6077 - val_recall_3: 0.5170\n",
      "Epoch 65/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6812 - accuracy: 0.5415 - recall_3: 0.6145\n",
      "Epoch 65: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6807 - accuracy: 0.5423 - recall_3: 0.6141 - val_loss: 0.6540 - val_accuracy: 0.6115 - val_recall_3: 0.5178\n",
      "Epoch 66/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6813 - accuracy: 0.5453 - recall_3: 0.6026\n",
      "Epoch 66: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6812 - accuracy: 0.5447 - recall_3: 0.6024 - val_loss: 0.6836 - val_accuracy: 0.5345 - val_recall_3: 0.6548\n",
      "Epoch 67/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6800 - accuracy: 0.5448 - recall_3: 0.6279\n",
      "Epoch 67: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6798 - accuracy: 0.5443 - recall_3: 0.6283 - val_loss: 0.6908 - val_accuracy: 0.5160 - val_recall_3: 0.6822\n",
      "Epoch 68/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6805 - accuracy: 0.5406 - recall_3: 0.6153\n",
      "Epoch 68: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6807 - accuracy: 0.5409 - recall_3: 0.6147 - val_loss: 0.6764 - val_accuracy: 0.5575 - val_recall_3: 0.6216\n",
      "Epoch 69/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6801 - accuracy: 0.5391 - recall_3: 0.6233\n",
      "Epoch 69: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6802 - accuracy: 0.5391 - recall_3: 0.6232 - val_loss: 0.6554 - val_accuracy: 0.6128 - val_recall_3: 0.5278\n",
      "Epoch 70/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6810 - accuracy: 0.5412 - recall_3: 0.6064\n",
      "Epoch 70: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6812 - accuracy: 0.5419 - recall_3: 0.6057 - val_loss: 0.6720 - val_accuracy: 0.5703 - val_recall_3: 0.6033\n",
      "Epoch 71/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.5398 - recall_3: 0.6157\n",
      "Epoch 71: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6817 - accuracy: 0.5398 - recall_3: 0.6151 - val_loss: 0.6677 - val_accuracy: 0.5806 - val_recall_3: 0.5801\n",
      "Epoch 72/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6805 - accuracy: 0.5443 - recall_3: 0.6109\n",
      "Epoch 72: val_loss did not improve from 0.65230\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6804 - accuracy: 0.5449 - recall_3: 0.6117 - val_loss: 0.6752 - val_accuracy: 0.5602 - val_recall_3: 0.6199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c961a476a0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(32, input_shape=(timesteps, input_dim),\n",
    "                    kernel_regularizer=regularizers.l2(0.01),\n",
    "                    recurrent_regularizer=regularizers.l2(0.01),\n",
    "                    bias_regularizer=regularizers.l2(0.01),\n",
    "                     dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', Recall()])\n",
    "\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "checkpoint = ModelCheckpoint('model/best_model_GRUs.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 1s 2ms/step\n",
      "      Prediction  Actual\n",
      "0       0.303590       0\n",
      "1       0.407685       0\n",
      "2       0.482614       0\n",
      "3       0.452800       0\n",
      "4       0.463553       0\n",
      "...          ...     ...\n",
      "8215    0.301737       0\n",
      "8216    0.563992       0\n",
      "8217    0.567807       1\n",
      "8218    0.546217       0\n",
      "8219    0.477827       0\n",
      "\n",
      "[8220 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "best_model = load_model('model/best_model_GRUs.h5')\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Compare the predictions with the actual values\n",
    "comparison = pd.DataFrame({'Prediction': predictions.flatten(), 'Actual': y_test})\n",
    "\n",
    "# Print the comparison\n",
    "print(comparison)\n",
    "predictions_labels = [1 if p > 0.5 else 0 for p in predictions.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_labels)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.64      0.73      6661\n",
      "           1       0.25      0.52      0.34      1559\n",
      "\n",
      "    accuracy                           0.62      8220\n",
      "   macro avg       0.55      0.58      0.53      8220\n",
      "weighted avg       0.74      0.62      0.66      8220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, predictions_labels, output_dict=True)\n",
    "print(classification_report(y_test, predictions_labels))\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv('result/GRUs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
