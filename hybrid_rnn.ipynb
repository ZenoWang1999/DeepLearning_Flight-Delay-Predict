{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = pd.read_csv('data/data_encoded_scaled.csv')\n",
    "data_cleaned = pd.read_csv('data/data_cleaned.csv')\n",
    "data_feature = data_cleaned.drop(columns=['TOTAL_DELAY', 'DEP_DEL15'])\n",
    "data_target = data_cleaned['DEP_DEL15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = data_encoded_scaled.drop(columns=['DEP_DEL15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.968432919954903\n"
     ]
    }
   ],
   "source": [
    "sequence_days = 7\n",
    "daily_counts = data_feature.groupby(['MONTH', 'DAY_OF_MONTH', 'DEPARTING_AIRPORT']).size() # 一天一个机场的航班数\n",
    "average_rows = daily_counts.mean()\n",
    "\n",
    "print(average_rows*sequence_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_encoded_scaled.assign(TARGET=data_target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MONTH  DAY_OF_MONTH  DAY_OF_WEEK  DEP_TIME_BLK  DISTANCE_GROUP  \\\n",
      "0    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "1    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "2    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "3    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "4    0.0           0.0     0.166667      0.166667        0.285714   \n",
      "\n",
      "   SEGMENT_NUMBER  CONCURRENT_FLIGHTS  NUMBER_OF_SEATS  AIRPORT_FLIGHTS_MONTH  \\\n",
      "0           0.000            0.098592              1.0               0.115453   \n",
      "1           0.000            0.098592              1.0               0.115453   \n",
      "2           0.000            0.112676              1.0               0.111384   \n",
      "3           0.000            0.267606              1.0               0.333661   \n",
      "4           0.125            0.042254              1.0               0.028528   \n",
      "\n",
      "   AIRLINE_FLIGHTS_MONTH  ...  PREVIOUS_AIRPORT_4  PREVIOUS_AIRPORT_5  \\\n",
      "0               0.183515  ...                 0.0                 0.0   \n",
      "1               0.183515  ...                 0.0                 0.0   \n",
      "2               0.183515  ...                 0.0                 0.0   \n",
      "3               0.183515  ...                 0.0                 0.0   \n",
      "4               0.183515  ...                 0.0                 1.0   \n",
      "\n",
      "   PREVIOUS_AIRPORT_6  PRCP  SNOW  SNWD      TMAX      AWND  RESIDUALS  TARGET  \n",
      "0                 1.0   0.0   0.0   0.0  0.353982  0.198638   0.052138       0  \n",
      "1                 1.0   0.0   0.0   0.0  0.353982  0.198638   0.052775       0  \n",
      "2                 1.0   0.0   0.0   0.0  0.451327  0.172291   0.051816       0  \n",
      "3                 1.0   0.0   0.0   0.0  0.699115  0.370930   0.051928       0  \n",
      "4                 0.0   0.0   0.0   0.0  0.460177  0.178804   0.051987       0  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences shape: (41100, 28, 33)\n",
      "Target values shape: (41100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequence_length = int(round(average_rows*sequence_days))\n",
    "\n",
    "unique_dep_airport = data_feature['DEPARTING_AIRPORT'].unique()\n",
    "unique_flight_number = data_feature['FLIGHT_NUMBER'].unique()\n",
    "\n",
    "X_sequences = []\n",
    "y_targets = []\n",
    "\n",
    "for dep_airport in unique_dep_airport:\n",
    "    flight_data = data_full[data_feature['DEPARTING_AIRPORT'] == dep_airport]\n",
    "    flight_data_values = flight_data.iloc[:, :-1].values\n",
    "    flight_data_target = flight_data.iloc[:, -1].values\n",
    "    for i in range(len(flight_data) - sequence_length):\n",
    "        X_sequences.append(flight_data_values[i:i+sequence_length])\n",
    "        y_targets.append(flight_data_target[i+sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_targets = np.array(y_targets)\n",
    "\n",
    "print(\"Input sequences shape:\", X_sequences.shape)\n",
    "print(\"Target values shape:\", y_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = list(zip(X_sequences, y_targets))\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Separate the sequences and targets\n",
    "X_train, y_train = zip(*train_data)\n",
    "X_test, y_test = zip(*test_data)\n",
    "\n",
    "# Convert the results back to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "timesteps = X_train.shape[1]\n",
    "input_dim = X_train.shape[2]\n",
    "\n",
    "weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train) , y = y_train)\n",
    "class_weights = dict(zip(np.unique(y_train), weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "820/822 [============================>.] - ETA: 0s - loss: 1.3734 - accuracy: 0.4664 - recall: 0.5208\n",
      "Epoch 1: val_loss improved from inf to 1.27874, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 9s 9ms/step - loss: 1.3734 - accuracy: 0.4664 - recall: 0.5206 - val_loss: 1.2787 - val_accuracy: 0.4419 - val_recall: 0.5550\n",
      "Epoch 2/300\n",
      " 24/822 [..............................] - ETA: 5s - loss: 1.3369 - accuracy: 0.4909 - recall: 0.4783"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817/822 [============================>.] - ETA: 0s - loss: 1.2314 - accuracy: 0.5050 - recall: 0.5103\n",
      "Epoch 2: val_loss improved from 1.27874 to 1.16227, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 6s 7ms/step - loss: 1.2313 - accuracy: 0.5053 - recall: 0.5110 - val_loss: 1.1623 - val_accuracy: 0.5134 - val_recall: 0.5409\n",
      "Epoch 3/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 1.1337 - accuracy: 0.5147 - recall: 0.5192\n",
      "Epoch 3: val_loss improved from 1.16227 to 1.07421, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 6s 7ms/step - loss: 1.1333 - accuracy: 0.5145 - recall: 0.5187 - val_loss: 1.0742 - val_accuracy: 0.5332 - val_recall: 0.5749\n",
      "Epoch 4/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 1.0469 - accuracy: 0.5273 - recall: 0.5380\n",
      "Epoch 4: val_loss improved from 1.07421 to 1.00660, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 6s 7ms/step - loss: 1.0467 - accuracy: 0.5274 - recall: 0.5384 - val_loss: 1.0066 - val_accuracy: 0.5225 - val_recall: 0.6435\n",
      "Epoch 5/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.9751 - accuracy: 0.5313 - recall: 0.5620\n",
      "Epoch 5: val_loss improved from 1.00660 to 0.94142, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 6s 7ms/step - loss: 0.9749 - accuracy: 0.5310 - recall: 0.5622 - val_loss: 0.9414 - val_accuracy: 0.5263 - val_recall: 0.6518\n",
      "Epoch 6/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.9155 - accuracy: 0.5281 - recall: 0.5820\n",
      "Epoch 6: val_loss improved from 0.94142 to 0.86444, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 6s 7ms/step - loss: 0.9153 - accuracy: 0.5281 - recall: 0.5814 - val_loss: 0.8644 - val_accuracy: 0.5896 - val_recall: 0.5360\n",
      "Epoch 7/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.8636 - accuracy: 0.5436 - recall: 0.5782\n",
      "Epoch 7: val_loss improved from 0.86444 to 0.83790, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.8637 - accuracy: 0.5436 - recall: 0.5788 - val_loss: 0.8379 - val_accuracy: 0.5450 - val_recall: 0.6228\n",
      "Epoch 8/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.8225 - accuracy: 0.5354 - recall: 0.6009\n",
      "Epoch 8: val_loss improved from 0.83790 to 0.79101, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.8227 - accuracy: 0.5361 - recall: 0.6007 - val_loss: 0.7910 - val_accuracy: 0.5774 - val_recall: 0.5682\n",
      "Epoch 9/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7909 - accuracy: 0.5371 - recall: 0.5994\n",
      "Epoch 9: val_loss improved from 0.79101 to 0.76325, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7909 - accuracy: 0.5374 - recall: 0.5991 - val_loss: 0.7632 - val_accuracy: 0.5827 - val_recall: 0.5674\n",
      "Epoch 10/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7667 - accuracy: 0.5434 - recall: 0.5922\n",
      "Epoch 10: val_loss did not improve from 0.76325\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7664 - accuracy: 0.5434 - recall: 0.5929 - val_loss: 0.7753 - val_accuracy: 0.4854 - val_recall: 0.7105\n",
      "Epoch 11/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.7475 - accuracy: 0.5372 - recall: 0.6097\n",
      "Epoch 11: val_loss improved from 0.76325 to 0.74108, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7474 - accuracy: 0.5372 - recall: 0.6097 - val_loss: 0.7411 - val_accuracy: 0.5363 - val_recall: 0.6452\n",
      "Epoch 12/300\n",
      "805/822 [============================>.] - ETA: 0s - loss: 0.7328 - accuracy: 0.5410 - recall: 0.6124\n",
      "Epoch 12: val_loss improved from 0.74108 to 0.71320, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7324 - accuracy: 0.5411 - recall: 0.6122 - val_loss: 0.7132 - val_accuracy: 0.5782 - val_recall: 0.5699\n",
      "Epoch 13/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.7232 - accuracy: 0.5423 - recall: 0.5949\n",
      "Epoch 13: val_loss did not improve from 0.71320\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7223 - accuracy: 0.5422 - recall: 0.5958 - val_loss: 0.7192 - val_accuracy: 0.5383 - val_recall: 0.6468\n",
      "Epoch 14/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.7137 - accuracy: 0.5398 - recall: 0.6156\n",
      "Epoch 14: val_loss improved from 0.71320 to 0.68455, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7134 - accuracy: 0.5398 - recall: 0.6144 - val_loss: 0.6846 - val_accuracy: 0.6177 - val_recall: 0.5045\n",
      "Epoch 15/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.7077 - accuracy: 0.5447 - recall: 0.5955\n",
      "Epoch 15: val_loss did not improve from 0.68455\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7079 - accuracy: 0.5448 - recall: 0.5954 - val_loss: 0.7051 - val_accuracy: 0.5426 - val_recall: 0.6245\n",
      "Epoch 16/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.7039 - accuracy: 0.5483 - recall: 0.5990\n",
      "Epoch 16: val_loss did not improve from 0.68455\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.7037 - accuracy: 0.5481 - recall: 0.6001 - val_loss: 0.7136 - val_accuracy: 0.5058 - val_recall: 0.6857\n",
      "Epoch 17/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6996 - accuracy: 0.5423 - recall: 0.6103\n",
      "Epoch 17: val_loss did not improve from 0.68455\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6993 - accuracy: 0.5416 - recall: 0.6107 - val_loss: 0.7081 - val_accuracy: 0.5129 - val_recall: 0.6774\n",
      "Epoch 18/300\n",
      "803/822 [============================>.] - ETA: 0s - loss: 0.6971 - accuracy: 0.5412 - recall: 0.6171\n",
      "Epoch 18: val_loss improved from 0.68455 to 0.68357, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6969 - accuracy: 0.5421 - recall: 0.6159 - val_loss: 0.6836 - val_accuracy: 0.5804 - val_recall: 0.5707\n",
      "Epoch 19/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6941 - accuracy: 0.5445 - recall: 0.6013\n",
      "Epoch 19: val_loss did not improve from 0.68357\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6940 - accuracy: 0.5446 - recall: 0.6013 - val_loss: 0.6921 - val_accuracy: 0.5494 - val_recall: 0.6170\n",
      "Epoch 20/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6924 - accuracy: 0.5473 - recall: 0.6157\n",
      "Epoch 20: val_loss improved from 0.68357 to 0.68268, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6924 - accuracy: 0.5474 - recall: 0.6148 - val_loss: 0.6827 - val_accuracy: 0.5712 - val_recall: 0.5897\n",
      "Epoch 21/300\n",
      "808/822 [============================>.] - ETA: 0s - loss: 0.6905 - accuracy: 0.5469 - recall: 0.6000\n",
      "Epoch 21: val_loss did not improve from 0.68268\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6910 - accuracy: 0.5466 - recall: 0.5993 - val_loss: 0.6953 - val_accuracy: 0.5274 - val_recall: 0.6385\n",
      "Epoch 22/300\n",
      "804/822 [============================>.] - ETA: 0s - loss: 0.6917 - accuracy: 0.5467 - recall: 0.5937\n",
      "Epoch 22: val_loss did not improve from 0.68268\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6918 - accuracy: 0.5465 - recall: 0.5954 - val_loss: 0.6976 - val_accuracy: 0.5138 - val_recall: 0.6675\n",
      "Epoch 23/300\n",
      "811/822 [============================>.] - ETA: 0s - loss: 0.6885 - accuracy: 0.5420 - recall: 0.6102\n",
      "Epoch 23: val_loss did not improve from 0.68268\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6889 - accuracy: 0.5419 - recall: 0.6099 - val_loss: 0.6988 - val_accuracy: 0.5146 - val_recall: 0.6832\n",
      "Epoch 24/300\n",
      "812/822 [============================>.] - ETA: 0s - loss: 0.6887 - accuracy: 0.5456 - recall: 0.5992\n",
      "Epoch 24: val_loss did not improve from 0.68268\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6887 - accuracy: 0.5452 - recall: 0.5989 - val_loss: 0.7073 - val_accuracy: 0.4891 - val_recall: 0.6956\n",
      "Epoch 25/300\n",
      "805/822 [============================>.] - ETA: 0s - loss: 0.6873 - accuracy: 0.5458 - recall: 0.6051\n",
      "Epoch 25: val_loss did not improve from 0.68268\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6867 - accuracy: 0.5462 - recall: 0.6056 - val_loss: 0.6855 - val_accuracy: 0.5554 - val_recall: 0.6170\n",
      "Epoch 26/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6853 - accuracy: 0.5592 - recall: 0.5998\n",
      "Epoch 26: val_loss did not improve from 0.68268\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6852 - accuracy: 0.5589 - recall: 0.6007 - val_loss: 0.7111 - val_accuracy: 0.4796 - val_recall: 0.7179\n",
      "Epoch 27/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6853 - accuracy: 0.5422 - recall: 0.6128\n",
      "Epoch 27: val_loss improved from 0.68268 to 0.67359, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6853 - accuracy: 0.5422 - recall: 0.6128 - val_loss: 0.6736 - val_accuracy: 0.5832 - val_recall: 0.5757\n",
      "Epoch 28/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6849 - accuracy: 0.5563 - recall: 0.6013\n",
      "Epoch 28: val_loss did not improve from 0.67359\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6854 - accuracy: 0.5557 - recall: 0.6005 - val_loss: 0.6937 - val_accuracy: 0.5243 - val_recall: 0.6510\n",
      "Epoch 29/300\n",
      "805/822 [============================>.] - ETA: 0s - loss: 0.6870 - accuracy: 0.5425 - recall: 0.6152\n",
      "Epoch 29: val_loss improved from 0.67359 to 0.66932, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6860 - accuracy: 0.5432 - recall: 0.6148 - val_loss: 0.6693 - val_accuracy: 0.5949 - val_recall: 0.5542\n",
      "Epoch 30/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5519 - recall: 0.5904\n",
      "Epoch 30: val_loss did not improve from 0.66932\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6861 - accuracy: 0.5517 - recall: 0.5902 - val_loss: 0.6894 - val_accuracy: 0.5322 - val_recall: 0.6443\n",
      "Epoch 31/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6842 - accuracy: 0.5472 - recall: 0.6037\n",
      "Epoch 31: val_loss did not improve from 0.66932\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6843 - accuracy: 0.5472 - recall: 0.6040 - val_loss: 0.6845 - val_accuracy: 0.5476 - val_recall: 0.6228\n",
      "Epoch 32/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.5499 - recall: 0.6124\n",
      "Epoch 32: val_loss did not improve from 0.66932\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6826 - accuracy: 0.5499 - recall: 0.6124 - val_loss: 0.6783 - val_accuracy: 0.5630 - val_recall: 0.5988\n",
      "Epoch 33/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6835 - accuracy: 0.5488 - recall: 0.6145\n",
      "Epoch 33: val_loss improved from 0.66932 to 0.66763, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6828 - accuracy: 0.5490 - recall: 0.6142 - val_loss: 0.6676 - val_accuracy: 0.5918 - val_recall: 0.5575\n",
      "Epoch 34/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6844 - accuracy: 0.5473 - recall: 0.5967\n",
      "Epoch 34: val_loss did not improve from 0.66763\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6845 - accuracy: 0.5470 - recall: 0.5974 - val_loss: 0.6804 - val_accuracy: 0.5602 - val_recall: 0.6005\n",
      "Epoch 35/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.6843 - accuracy: 0.5468 - recall: 0.6008\n",
      "Epoch 35: val_loss did not improve from 0.66763\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6839 - accuracy: 0.5470 - recall: 0.6003 - val_loss: 0.6790 - val_accuracy: 0.5649 - val_recall: 0.6055\n",
      "Epoch 36/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6835 - accuracy: 0.5542 - recall: 0.5906\n",
      "Epoch 36: val_loss did not improve from 0.66763\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6837 - accuracy: 0.5541 - recall: 0.5915 - val_loss: 0.7188 - val_accuracy: 0.4542 - val_recall: 0.7527\n",
      "Epoch 37/300\n",
      "807/822 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.5489 - recall: 0.6151\n",
      "Epoch 37: val_loss did not improve from 0.66763\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6822 - accuracy: 0.5491 - recall: 0.6150 - val_loss: 0.6913 - val_accuracy: 0.5265 - val_recall: 0.6501\n",
      "Epoch 38/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6815 - accuracy: 0.5545 - recall: 0.5985\n",
      "Epoch 38: val_loss did not improve from 0.66763\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6817 - accuracy: 0.5542 - recall: 0.5980 - val_loss: 0.6918 - val_accuracy: 0.5208 - val_recall: 0.6551\n",
      "Epoch 39/300\n",
      "803/822 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.5463 - recall: 0.6080\n",
      "Epoch 39: val_loss improved from 0.66763 to 0.65401, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6828 - accuracy: 0.5479 - recall: 0.6027 - val_loss: 0.6540 - val_accuracy: 0.6256 - val_recall: 0.4872\n",
      "Epoch 40/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6823 - accuracy: 0.5545 - recall: 0.5931\n",
      "Epoch 40: val_loss did not improve from 0.65401\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6824 - accuracy: 0.5546 - recall: 0.5925 - val_loss: 0.6751 - val_accuracy: 0.5712 - val_recall: 0.5815\n",
      "Epoch 41/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6832 - accuracy: 0.5494 - recall: 0.5963\n",
      "Epoch 41: val_loss did not improve from 0.65401\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6830 - accuracy: 0.5485 - recall: 0.5952 - val_loss: 0.6844 - val_accuracy: 0.5426 - val_recall: 0.6286\n",
      "Epoch 42/300\n",
      "810/822 [============================>.] - ETA: 0s - loss: 0.6825 - accuracy: 0.5502 - recall: 0.6090\n",
      "Epoch 42: val_loss did not improve from 0.65401\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6826 - accuracy: 0.5508 - recall: 0.6062 - val_loss: 0.6569 - val_accuracy: 0.6147 - val_recall: 0.4988\n",
      "Epoch 43/300\n",
      "810/822 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5578 - recall: 0.5949\n",
      "Epoch 43: val_loss did not improve from 0.65401\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6818 - accuracy: 0.5579 - recall: 0.5943 - val_loss: 0.6733 - val_accuracy: 0.5759 - val_recall: 0.5914\n",
      "Epoch 44/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.6829 - accuracy: 0.5475 - recall: 0.5985\n",
      "Epoch 44: val_loss did not improve from 0.65401\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6827 - accuracy: 0.5473 - recall: 0.5980 - val_loss: 0.6839 - val_accuracy: 0.5462 - val_recall: 0.6270\n",
      "Epoch 45/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6820 - accuracy: 0.5520 - recall: 0.6080\n",
      "Epoch 45: val_loss did not improve from 0.65401\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6818 - accuracy: 0.5522 - recall: 0.6064 - val_loss: 0.6743 - val_accuracy: 0.5748 - val_recall: 0.5864\n",
      "Epoch 46/300\n",
      "811/822 [============================>.] - ETA: 0s - loss: 0.6834 - accuracy: 0.5496 - recall: 0.6037\n",
      "Epoch 46: val_loss did not improve from 0.65401\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6833 - accuracy: 0.5492 - recall: 0.6036 - val_loss: 0.6687 - val_accuracy: 0.5850 - val_recall: 0.5724\n",
      "Epoch 47/300\n",
      "812/822 [============================>.] - ETA: 0s - loss: 0.6817 - accuracy: 0.5532 - recall: 0.6103\n",
      "Epoch 47: val_loss did not improve from 0.65401\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6817 - accuracy: 0.5527 - recall: 0.6114 - val_loss: 0.6946 - val_accuracy: 0.5113 - val_recall: 0.6725\n",
      "Epoch 48/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6790 - accuracy: 0.5525 - recall: 0.6170\n",
      "Epoch 48: val_loss did not improve from 0.65401\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6801 - accuracy: 0.5530 - recall: 0.6159 - val_loss: 0.6735 - val_accuracy: 0.5733 - val_recall: 0.5955\n",
      "Epoch 49/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6820 - accuracy: 0.5465 - recall: 0.6017\n",
      "Epoch 49: val_loss did not improve from 0.65401\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6820 - accuracy: 0.5465 - recall: 0.6017 - val_loss: 0.6786 - val_accuracy: 0.5633 - val_recall: 0.6079\n",
      "Epoch 50/300\n",
      "812/822 [============================>.] - ETA: 0s - loss: 0.6838 - accuracy: 0.5465 - recall: 0.6013\n",
      "Epoch 50: val_loss did not improve from 0.65401\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6831 - accuracy: 0.5455 - recall: 0.6013 - val_loss: 0.6884 - val_accuracy: 0.5328 - val_recall: 0.6427\n",
      "Epoch 51/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6815 - accuracy: 0.5535 - recall: 0.5962\n",
      "Epoch 51: val_loss did not improve from 0.65401\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6818 - accuracy: 0.5535 - recall: 0.5958 - val_loss: 0.6887 - val_accuracy: 0.5262 - val_recall: 0.6402\n",
      "Epoch 52/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6817 - accuracy: 0.5462 - recall: 0.6234\n",
      "Epoch 52: val_loss improved from 0.65401 to 0.64648, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6813 - accuracy: 0.5476 - recall: 0.6195 - val_loss: 0.6465 - val_accuracy: 0.6331 - val_recall: 0.4789\n",
      "Epoch 53/300\n",
      "803/822 [============================>.] - ETA: 0s - loss: 0.6818 - accuracy: 0.5530 - recall: 0.5948\n",
      "Epoch 53: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6817 - accuracy: 0.5526 - recall: 0.5968 - val_loss: 0.6871 - val_accuracy: 0.5281 - val_recall: 0.6485\n",
      "Epoch 54/300\n",
      "808/822 [============================>.] - ETA: 0s - loss: 0.6807 - accuracy: 0.5504 - recall: 0.6034\n",
      "Epoch 54: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6808 - accuracy: 0.5503 - recall: 0.6050 - val_loss: 0.6886 - val_accuracy: 0.5278 - val_recall: 0.6460\n",
      "Epoch 55/300\n",
      "812/822 [============================>.] - ETA: 0s - loss: 0.6817 - accuracy: 0.5488 - recall: 0.6013\n",
      "Epoch 55: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6827 - accuracy: 0.5484 - recall: 0.5995 - val_loss: 0.6770 - val_accuracy: 0.5560 - val_recall: 0.6187\n",
      "Epoch 56/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6793 - accuracy: 0.5545 - recall: 0.6168\n",
      "Epoch 56: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6798 - accuracy: 0.5547 - recall: 0.6157 - val_loss: 0.6862 - val_accuracy: 0.5303 - val_recall: 0.6443\n",
      "Epoch 57/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6810 - accuracy: 0.5462 - recall: 0.6101\n",
      "Epoch 57: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6809 - accuracy: 0.5462 - recall: 0.6105 - val_loss: 0.7019 - val_accuracy: 0.4927 - val_recall: 0.7039\n",
      "Epoch 58/300\n",
      "812/822 [============================>.] - ETA: 0s - loss: 0.6822 - accuracy: 0.5482 - recall: 0.5998\n",
      "Epoch 58: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6822 - accuracy: 0.5482 - recall: 0.5999 - val_loss: 0.6930 - val_accuracy: 0.5161 - val_recall: 0.6576\n",
      "Epoch 59/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5496 - recall: 0.5996\n",
      "Epoch 59: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6810 - accuracy: 0.5488 - recall: 0.6009 - val_loss: 0.6886 - val_accuracy: 0.5257 - val_recall: 0.6526\n",
      "Epoch 60/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6828 - accuracy: 0.5455 - recall: 0.6121\n",
      "Epoch 60: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6819 - accuracy: 0.5459 - recall: 0.6132 - val_loss: 0.6777 - val_accuracy: 0.5520 - val_recall: 0.6278\n",
      "Epoch 61/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6807 - accuracy: 0.5517 - recall: 0.6036\n",
      "Epoch 61: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6810 - accuracy: 0.5520 - recall: 0.6038 - val_loss: 0.6771 - val_accuracy: 0.5617 - val_recall: 0.6063\n",
      "Epoch 62/300\n",
      "808/822 [============================>.] - ETA: 0s - loss: 0.6807 - accuracy: 0.5513 - recall: 0.5996\n",
      "Epoch 62: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6810 - accuracy: 0.5501 - recall: 0.5999 - val_loss: 0.7005 - val_accuracy: 0.4964 - val_recall: 0.6840\n",
      "Epoch 63/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.5526 - recall: 0.6001\n",
      "Epoch 63: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6822 - accuracy: 0.5525 - recall: 0.6007 - val_loss: 0.6907 - val_accuracy: 0.5287 - val_recall: 0.6493\n",
      "Epoch 64/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6795 - accuracy: 0.5561 - recall: 0.6092\n",
      "Epoch 64: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6795 - accuracy: 0.5562 - recall: 0.6093 - val_loss: 0.6639 - val_accuracy: 0.5973 - val_recall: 0.5484\n",
      "Epoch 65/300\n",
      "810/822 [============================>.] - ETA: 0s - loss: 0.6810 - accuracy: 0.5576 - recall: 0.5985\n",
      "Epoch 65: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6808 - accuracy: 0.5573 - recall: 0.5984 - val_loss: 0.6781 - val_accuracy: 0.5642 - val_recall: 0.6013\n",
      "Epoch 66/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.6802 - accuracy: 0.5617 - recall: 0.6057\n",
      "Epoch 66: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6798 - accuracy: 0.5616 - recall: 0.6062 - val_loss: 0.6884 - val_accuracy: 0.5365 - val_recall: 0.6286\n",
      "Epoch 67/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6811 - accuracy: 0.5611 - recall: 0.5909\n",
      "Epoch 67: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6811 - accuracy: 0.5611 - recall: 0.5909 - val_loss: 0.7046 - val_accuracy: 0.4889 - val_recall: 0.6940\n",
      "Epoch 68/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6808 - accuracy: 0.5547 - recall: 0.5982\n",
      "Epoch 68: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6819 - accuracy: 0.5538 - recall: 0.5991 - val_loss: 0.7089 - val_accuracy: 0.4775 - val_recall: 0.7262\n",
      "Epoch 69/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.5502 - recall: 0.6183\n",
      "Epoch 69: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6807 - accuracy: 0.5498 - recall: 0.6189 - val_loss: 0.6896 - val_accuracy: 0.5359 - val_recall: 0.6394\n",
      "Epoch 70/300\n",
      "812/822 [============================>.] - ETA: 0s - loss: 0.6806 - accuracy: 0.5558 - recall: 0.6103\n",
      "Epoch 70: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6807 - accuracy: 0.5555 - recall: 0.6101 - val_loss: 0.6987 - val_accuracy: 0.5103 - val_recall: 0.6675\n",
      "Epoch 71/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6812 - accuracy: 0.5529 - recall: 0.5965\n",
      "Epoch 71: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6813 - accuracy: 0.5527 - recall: 0.5954 - val_loss: 0.6710 - val_accuracy: 0.5803 - val_recall: 0.5757\n",
      "Epoch 72/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.5516 - recall: 0.6009\n",
      "Epoch 72: val_loss did not improve from 0.64648\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6815 - accuracy: 0.5512 - recall: 0.6015 - val_loss: 0.6977 - val_accuracy: 0.5065 - val_recall: 0.6675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x27b07cd82e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(32, input_shape=(timesteps, input_dim),\n",
    "                    kernel_regularizer=regularizers.l2(0.01),\n",
    "                    recurrent_regularizer=regularizers.l2(0.01),\n",
    "                    bias_regularizer=regularizers.l2(0.01),\n",
    "                     dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', Recall()])\n",
    "\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "checkpoint = ModelCheckpoint('model/best_model_hybrid_rnn.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 1ms/step\n",
      "      Prediction  Actual\n",
      "0       0.427006       0\n",
      "1       0.351665       0\n",
      "2       0.469974       1\n",
      "3       0.558956       0\n",
      "4       0.600636       1\n",
      "...          ...     ...\n",
      "8215    0.543805       0\n",
      "8216    0.295044       0\n",
      "8217    0.419467       0\n",
      "8218    0.526487       0\n",
      "8219    0.478604       0\n",
      "\n",
      "[8220 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "best_model = load_model('model/best_model_hybrid_rnn.h5')\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Compare the predictions with the actual values\n",
    "comparison = pd.DataFrame({'Prediction': predictions.flatten(), 'Actual': y_test})\n",
    "\n",
    "# Print the comparison\n",
    "print(comparison)\n",
    "predictions_labels = [1 if p > 0.5 else 0 for p in predictions.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_labels)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.68      0.75      6591\n",
      "           1       0.27      0.48      0.35      1629\n",
      "\n",
      "    accuracy                           0.64      8220\n",
      "   macro avg       0.56      0.58      0.55      8220\n",
      "weighted avg       0.73      0.64      0.67      8220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, predictions_labels, output_dict=True)\n",
    "print(classification_report(y_test, predictions_labels))\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv('result/hybrid_rnn.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
