{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = pd.read_csv('data/data_encoded_scaled.csv')\n",
    "data_cleaned = pd.read_csv('data/data_cleaned.csv')\n",
    "data_feature = data_cleaned.drop(columns=['TOTAL_DELAY', 'DEP_DEL15'])\n",
    "data_target = data_cleaned['DEP_DEL15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = data_encoded_scaled.drop(columns=['RESIDUALS', 'DEP_DEL15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.968432919954903\n"
     ]
    }
   ],
   "source": [
    "sequence_days = 7\n",
    "daily_counts = data_feature.groupby(['MONTH', 'DAY_OF_MONTH', 'DEPARTING_AIRPORT']).size()\n",
    "average_rows = daily_counts.mean()\n",
    "\n",
    "print(average_rows*sequence_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_encoded_scaled.assign(TARGET=data_target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MONTH  DAY_OF_MONTH  DAY_OF_WEEK  DEP_TIME_BLK  DISTANCE_GROUP  \\\n",
      "0    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "1    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "2    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "3    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "4    0.0           0.0     0.166667      0.166667        0.285714   \n",
      "\n",
      "   SEGMENT_NUMBER  CONCURRENT_FLIGHTS  NUMBER_OF_SEATS  AIRPORT_FLIGHTS_MONTH  \\\n",
      "0           0.000            0.098592              1.0               0.115453   \n",
      "1           0.000            0.098592              1.0               0.115453   \n",
      "2           0.000            0.112676              1.0               0.111384   \n",
      "3           0.000            0.267606              1.0               0.333661   \n",
      "4           0.125            0.042254              1.0               0.028528   \n",
      "\n",
      "   AIRLINE_FLIGHTS_MONTH  ...  PREVIOUS_AIRPORT_3  PREVIOUS_AIRPORT_4  \\\n",
      "0               0.183515  ...                 0.0                 0.0   \n",
      "1               0.183515  ...                 0.0                 0.0   \n",
      "2               0.183515  ...                 0.0                 0.0   \n",
      "3               0.183515  ...                 0.0                 0.0   \n",
      "4               0.183515  ...                 0.0                 0.0   \n",
      "\n",
      "   PREVIOUS_AIRPORT_5  PREVIOUS_AIRPORT_6  PRCP  SNOW  SNWD      TMAX  \\\n",
      "0                 0.0                 1.0   0.0   0.0   0.0  0.353982   \n",
      "1                 0.0                 1.0   0.0   0.0   0.0  0.353982   \n",
      "2                 0.0                 1.0   0.0   0.0   0.0  0.451327   \n",
      "3                 0.0                 1.0   0.0   0.0   0.0  0.699115   \n",
      "4                 1.0                 0.0   0.0   0.0   0.0  0.460177   \n",
      "\n",
      "       AWND  TARGET  \n",
      "0  0.198638       0  \n",
      "1  0.198638       0  \n",
      "2  0.172291       0  \n",
      "3  0.370930       0  \n",
      "4  0.178804       0  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences shape: (41100, 28, 32)\n",
      "Target values shape: (41100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequence_length = int(round(average_rows*sequence_days))\n",
    "\n",
    "unique_dep_airport = data_feature['DEPARTING_AIRPORT'].unique()\n",
    "unique_flight_number = data_feature['FLIGHT_NUMBER'].unique()\n",
    "\n",
    "X_sequences = []\n",
    "y_targets = []\n",
    "\n",
    "for dep_airport in unique_dep_airport:\n",
    "    flight_data = data_full[data_feature['DEPARTING_AIRPORT'] == dep_airport]\n",
    "    flight_data_values = flight_data.iloc[:, :-1].values\n",
    "    flight_data_target = flight_data.iloc[:, -1].values\n",
    "    for i in range(len(flight_data) - sequence_length):\n",
    "        X_sequences.append(flight_data_values[i:i+sequence_length])\n",
    "        y_targets.append(flight_data_target[i+sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_targets = np.array(y_targets)\n",
    "\n",
    "print(\"Input sequences shape:\", X_sequences.shape)\n",
    "print(\"Target values shape:\", y_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = list(zip(X_sequences, y_targets))\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Separate the sequences and targets\n",
    "X_train, y_train = zip(*train_data)\n",
    "X_test, y_test = zip(*test_data)\n",
    "\n",
    "# Convert the results back to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "timesteps = X_train.shape[1]\n",
    "input_dim = X_train.shape[2]\n",
    "\n",
    "weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train) , y = y_train)\n",
    "class_weights = dict(zip(np.unique(y_train), weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "819/822 [============================>.] - ETA: 0s - loss: 1.2232 - accuracy: 0.4994 - recall: 0.5765\n",
      "Epoch 1: val_loss improved from inf to 1.02952, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 13s 13ms/step - loss: 1.2223 - accuracy: 0.4991 - recall: 0.5764 - val_loss: 1.0295 - val_accuracy: 0.5021 - val_recall: 0.6672\n",
      "Epoch 2/300\n",
      " 14/822 [..............................] - ETA: 10s - loss: 0.9697 - accuracy: 0.4978 - recall: 0.6667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "821/822 [============================>.] - ETA: 0s - loss: 0.9026 - accuracy: 0.5399 - recall: 0.5683\n",
      "Epoch 2: val_loss improved from 1.02952 to 0.81766, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.9023 - accuracy: 0.5397 - recall: 0.5681 - val_loss: 0.8177 - val_accuracy: 0.5561 - val_recall: 0.5864\n",
      "Epoch 3/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.7819 - accuracy: 0.5508 - recall: 0.5732\n",
      "Epoch 3: val_loss improved from 0.81766 to 0.75178, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.7819 - accuracy: 0.5508 - recall: 0.5732 - val_loss: 0.7518 - val_accuracy: 0.5531 - val_recall: 0.6260\n",
      "Epoch 4/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7364 - accuracy: 0.5466 - recall: 0.5893\n",
      "Epoch 4: val_loss improved from 0.75178 to 0.71075, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.7364 - accuracy: 0.5467 - recall: 0.5886 - val_loss: 0.7108 - val_accuracy: 0.5917 - val_recall: 0.5515\n",
      "Epoch 5/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7170 - accuracy: 0.5511 - recall: 0.5808\n",
      "Epoch 5: val_loss did not improve from 0.71075\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.7171 - accuracy: 0.5515 - recall: 0.5821 - val_loss: 0.7168 - val_accuracy: 0.5415 - val_recall: 0.6506\n",
      "Epoch 6/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.7072 - accuracy: 0.5499 - recall: 0.5912\n",
      "Epoch 6: val_loss improved from 0.71075 to 0.71039, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.7072 - accuracy: 0.5498 - recall: 0.5911 - val_loss: 0.7104 - val_accuracy: 0.5362 - val_recall: 0.6577\n",
      "Epoch 7/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7013 - accuracy: 0.5489 - recall: 0.5912\n",
      "Epoch 7: val_loss improved from 0.71039 to 0.70704, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.7015 - accuracy: 0.5490 - recall: 0.5917 - val_loss: 0.7070 - val_accuracy: 0.5333 - val_recall: 0.6783\n",
      "Epoch 8/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6971 - accuracy: 0.5496 - recall: 0.5920\n",
      "Epoch 8: val_loss improved from 0.70704 to 0.70475, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6969 - accuracy: 0.5497 - recall: 0.5923 - val_loss: 0.7048 - val_accuracy: 0.5257 - val_recall: 0.6783\n",
      "Epoch 9/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.5462 - recall: 0.5911\n",
      "Epoch 9: val_loss improved from 0.70475 to 0.69375, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6936 - accuracy: 0.5463 - recall: 0.5909 - val_loss: 0.6938 - val_accuracy: 0.5468 - val_recall: 0.6434\n",
      "Epoch 10/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6900 - accuracy: 0.5461 - recall: 0.5989\n",
      "Epoch 10: val_loss improved from 0.69375 to 0.68804, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6903 - accuracy: 0.5460 - recall: 0.5982 - val_loss: 0.6880 - val_accuracy: 0.5576 - val_recall: 0.6252\n",
      "Epoch 11/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6894 - accuracy: 0.5521 - recall: 0.5915\n",
      "Epoch 11: val_loss did not improve from 0.68804\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.6898 - accuracy: 0.5521 - recall: 0.5919 - val_loss: 0.6958 - val_accuracy: 0.5303 - val_recall: 0.6751\n",
      "Epoch 12/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6879 - accuracy: 0.5425 - recall: 0.6003\n",
      "Epoch 12: val_loss improved from 0.68804 to 0.68269, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.6881 - accuracy: 0.5427 - recall: 0.6002 - val_loss: 0.6827 - val_accuracy: 0.5610 - val_recall: 0.6133\n",
      "Epoch 13/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6874 - accuracy: 0.5475 - recall: 0.6110\n",
      "Epoch 13: val_loss improved from 0.68269 to 0.68190, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6873 - accuracy: 0.5476 - recall: 0.6112 - val_loss: 0.6819 - val_accuracy: 0.5575 - val_recall: 0.6197\n",
      "Epoch 14/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6861 - accuracy: 0.5474 - recall: 0.5907\n",
      "Epoch 14: val_loss improved from 0.68190 to 0.67578, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6861 - accuracy: 0.5474 - recall: 0.5907 - val_loss: 0.6758 - val_accuracy: 0.5748 - val_recall: 0.5959\n",
      "Epoch 15/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6845 - accuracy: 0.5493 - recall: 0.5947\n",
      "Epoch 15: val_loss improved from 0.67578 to 0.66887, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6844 - accuracy: 0.5493 - recall: 0.5949 - val_loss: 0.6689 - val_accuracy: 0.5973 - val_recall: 0.5666\n",
      "Epoch 16/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6841 - accuracy: 0.5543 - recall: 0.5947\n",
      "Epoch 16: val_loss improved from 0.66887 to 0.66789, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.6841 - accuracy: 0.5543 - recall: 0.5947 - val_loss: 0.6679 - val_accuracy: 0.5937 - val_recall: 0.5697\n",
      "Epoch 17/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6831 - accuracy: 0.5588 - recall: 0.5860\n",
      "Epoch 17: val_loss did not improve from 0.66789\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.6831 - accuracy: 0.5588 - recall: 0.5860 - val_loss: 0.6873 - val_accuracy: 0.5400 - val_recall: 0.6529\n",
      "Epoch 18/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6827 - accuracy: 0.5515 - recall: 0.5955\n",
      "Epoch 18: val_loss did not improve from 0.66789\n",
      "822/822 [==============================] - 10s 13ms/step - loss: 0.6829 - accuracy: 0.5512 - recall: 0.5949 - val_loss: 0.6897 - val_accuracy: 0.5327 - val_recall: 0.6616\n",
      "Epoch 19/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6815 - accuracy: 0.5529 - recall: 0.5926\n",
      "Epoch 19: val_loss did not improve from 0.66789\n",
      "822/822 [==============================] - 7s 9ms/step - loss: 0.6821 - accuracy: 0.5522 - recall: 0.5917 - val_loss: 0.6915 - val_accuracy: 0.5236 - val_recall: 0.6751\n",
      "Epoch 20/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6817 - accuracy: 0.5491 - recall: 0.6020\n",
      "Epoch 20: val_loss did not improve from 0.66789\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6818 - accuracy: 0.5491 - recall: 0.6024 - val_loss: 0.6810 - val_accuracy: 0.5564 - val_recall: 0.6331\n",
      "Epoch 21/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6811 - accuracy: 0.5520 - recall: 0.6035\n",
      "Epoch 21: val_loss improved from 0.66789 to 0.66779, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6811 - accuracy: 0.5517 - recall: 0.6032 - val_loss: 0.6678 - val_accuracy: 0.5884 - val_recall: 0.5697\n",
      "Epoch 22/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6820 - accuracy: 0.5471 - recall: 0.5904\n",
      "Epoch 22: val_loss did not improve from 0.66779\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6823 - accuracy: 0.5460 - recall: 0.5903 - val_loss: 0.6789 - val_accuracy: 0.5616 - val_recall: 0.6165\n",
      "Epoch 23/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6810 - accuracy: 0.5511 - recall: 0.6003\n",
      "Epoch 23: val_loss did not improve from 0.66779\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6809 - accuracy: 0.5511 - recall: 0.6010 - val_loss: 0.6882 - val_accuracy: 0.5339 - val_recall: 0.6656\n",
      "Epoch 24/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6811 - accuracy: 0.5511 - recall: 0.5887\n",
      "Epoch 24: val_loss did not improve from 0.66779\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6815 - accuracy: 0.5511 - recall: 0.5878 - val_loss: 0.6765 - val_accuracy: 0.5655 - val_recall: 0.6078\n",
      "Epoch 25/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6807 - accuracy: 0.5511 - recall: 0.5909\n",
      "Epoch 25: val_loss did not improve from 0.66779\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6808 - accuracy: 0.5510 - recall: 0.5907 - val_loss: 0.6725 - val_accuracy: 0.5741 - val_recall: 0.5959\n",
      "Epoch 26/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5524 - recall: 0.6038\n",
      "Epoch 26: val_loss did not improve from 0.66779\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6805 - accuracy: 0.5523 - recall: 0.6039 - val_loss: 0.6804 - val_accuracy: 0.5528 - val_recall: 0.6284\n",
      "Epoch 27/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6794 - accuracy: 0.5551 - recall: 0.5972\n",
      "Epoch 27: val_loss did not improve from 0.66779\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6794 - accuracy: 0.5547 - recall: 0.5980 - val_loss: 0.6800 - val_accuracy: 0.5484 - val_recall: 0.6371\n",
      "Epoch 28/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6809 - accuracy: 0.5481 - recall: 0.5825\n",
      "Epoch 28: val_loss improved from 0.66779 to 0.66766, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6809 - accuracy: 0.5481 - recall: 0.5825 - val_loss: 0.6677 - val_accuracy: 0.5876 - val_recall: 0.5903\n",
      "Epoch 29/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6791 - accuracy: 0.5501 - recall: 0.5984\n",
      "Epoch 29: val_loss did not improve from 0.66766\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6791 - accuracy: 0.5500 - recall: 0.5988 - val_loss: 0.6811 - val_accuracy: 0.5453 - val_recall: 0.6529\n",
      "Epoch 30/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6801 - accuracy: 0.5520 - recall: 0.5961\n",
      "Epoch 30: val_loss did not improve from 0.66766\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6801 - accuracy: 0.5520 - recall: 0.5961 - val_loss: 0.6723 - val_accuracy: 0.5709 - val_recall: 0.6165\n",
      "Epoch 31/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6795 - accuracy: 0.5471 - recall: 0.6042\n",
      "Epoch 31: val_loss improved from 0.66766 to 0.66229, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6796 - accuracy: 0.5474 - recall: 0.6039 - val_loss: 0.6623 - val_accuracy: 0.5973 - val_recall: 0.5674\n",
      "Epoch 32/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6801 - accuracy: 0.5547 - recall: 0.5904\n",
      "Epoch 32: val_loss did not improve from 0.66229\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6797 - accuracy: 0.5543 - recall: 0.5909 - val_loss: 0.6865 - val_accuracy: 0.5306 - val_recall: 0.6640\n",
      "Epoch 33/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6801 - accuracy: 0.5525 - recall: 0.6003\n",
      "Epoch 33: val_loss improved from 0.66229 to 0.65941, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6796 - accuracy: 0.5531 - recall: 0.5992 - val_loss: 0.6594 - val_accuracy: 0.6069 - val_recall: 0.5523\n",
      "Epoch 34/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6794 - accuracy: 0.5548 - recall: 0.5953\n",
      "Epoch 34: val_loss did not improve from 0.65941\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6790 - accuracy: 0.5547 - recall: 0.5953 - val_loss: 0.6655 - val_accuracy: 0.5890 - val_recall: 0.5808\n",
      "Epoch 35/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6799 - accuracy: 0.5560 - recall: 0.5832\n",
      "Epoch 35: val_loss improved from 0.65941 to 0.65902, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6797 - accuracy: 0.5560 - recall: 0.5829 - val_loss: 0.6590 - val_accuracy: 0.6107 - val_recall: 0.5452\n",
      "Epoch 36/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6784 - accuracy: 0.5597 - recall: 0.5914\n",
      "Epoch 36: val_loss did not improve from 0.65902\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6784 - accuracy: 0.5596 - recall: 0.5915 - val_loss: 0.6776 - val_accuracy: 0.5561 - val_recall: 0.6387\n",
      "Epoch 37/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6801 - accuracy: 0.5512 - recall: 0.5951\n",
      "Epoch 37: val_loss did not improve from 0.65902\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6797 - accuracy: 0.5511 - recall: 0.5947 - val_loss: 0.6694 - val_accuracy: 0.5771 - val_recall: 0.5935\n",
      "Epoch 38/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6790 - accuracy: 0.5572 - recall: 0.5862\n",
      "Epoch 38: val_loss did not improve from 0.65902\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6794 - accuracy: 0.5562 - recall: 0.5848 - val_loss: 0.6828 - val_accuracy: 0.5458 - val_recall: 0.6466\n",
      "Epoch 39/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6791 - accuracy: 0.5461 - recall: 0.5959\n",
      "Epoch 39: val_loss improved from 0.65902 to 0.64525, saving model to model\\best_model_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6789 - accuracy: 0.5471 - recall: 0.5961 - val_loss: 0.6452 - val_accuracy: 0.6469 - val_recall: 0.4628\n",
      "Epoch 40/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6788 - accuracy: 0.5581 - recall: 0.5795\n",
      "Epoch 40: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6792 - accuracy: 0.5583 - recall: 0.5795 - val_loss: 0.6615 - val_accuracy: 0.5946 - val_recall: 0.5737\n",
      "Epoch 41/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6786 - accuracy: 0.5532 - recall: 0.5971\n",
      "Epoch 41: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6784 - accuracy: 0.5534 - recall: 0.5965 - val_loss: 0.6567 - val_accuracy: 0.6051 - val_recall: 0.5452\n",
      "Epoch 42/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6790 - accuracy: 0.5561 - recall: 0.5903\n",
      "Epoch 42: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6790 - accuracy: 0.5561 - recall: 0.5903 - val_loss: 0.6600 - val_accuracy: 0.6049 - val_recall: 0.5563\n",
      "Epoch 43/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.5525 - recall: 0.5972\n",
      "Epoch 43: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6781 - accuracy: 0.5523 - recall: 0.5970 - val_loss: 0.6546 - val_accuracy: 0.6153 - val_recall: 0.5468\n",
      "Epoch 44/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6792 - accuracy: 0.5549 - recall: 0.5859\n",
      "Epoch 44: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6792 - accuracy: 0.5546 - recall: 0.5856 - val_loss: 0.6731 - val_accuracy: 0.5671 - val_recall: 0.6236\n",
      "Epoch 45/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6790 - accuracy: 0.5553 - recall: 0.5866\n",
      "Epoch 45: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6790 - accuracy: 0.5553 - recall: 0.5866 - val_loss: 0.6800 - val_accuracy: 0.5502 - val_recall: 0.6474\n",
      "Epoch 46/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6790 - accuracy: 0.5528 - recall: 0.5987\n",
      "Epoch 46: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6785 - accuracy: 0.5534 - recall: 0.5996 - val_loss: 0.6676 - val_accuracy: 0.5836 - val_recall: 0.5895\n",
      "Epoch 47/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6797 - accuracy: 0.5554 - recall: 0.5820\n",
      "Epoch 47: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6796 - accuracy: 0.5553 - recall: 0.5823 - val_loss: 0.6923 - val_accuracy: 0.5141 - val_recall: 0.7044\n",
      "Epoch 48/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6772 - accuracy: 0.5585 - recall: 0.5939\n",
      "Epoch 48: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6771 - accuracy: 0.5583 - recall: 0.5939 - val_loss: 0.6767 - val_accuracy: 0.5590 - val_recall: 0.6244\n",
      "Epoch 49/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6789 - accuracy: 0.5519 - recall: 0.5829\n",
      "Epoch 49: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6789 - accuracy: 0.5520 - recall: 0.5829 - val_loss: 0.6767 - val_accuracy: 0.5596 - val_recall: 0.6339\n",
      "Epoch 50/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6776 - accuracy: 0.5591 - recall: 0.5921\n",
      "Epoch 50: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6776 - accuracy: 0.5591 - recall: 0.5921 - val_loss: 0.6913 - val_accuracy: 0.5152 - val_recall: 0.6902\n",
      "Epoch 51/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6797 - accuracy: 0.5461 - recall: 0.5919\n",
      "Epoch 51: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6796 - accuracy: 0.5465 - recall: 0.5919 - val_loss: 0.6631 - val_accuracy: 0.5914 - val_recall: 0.5753\n",
      "Epoch 52/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6795 - accuracy: 0.5519 - recall: 0.5922\n",
      "Epoch 52: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6795 - accuracy: 0.5519 - recall: 0.5929 - val_loss: 0.6956 - val_accuracy: 0.5015 - val_recall: 0.7108\n",
      "Epoch 53/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6782 - accuracy: 0.5490 - recall: 0.6051\n",
      "Epoch 53: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6782 - accuracy: 0.5490 - recall: 0.6051 - val_loss: 0.6593 - val_accuracy: 0.6060 - val_recall: 0.5602\n",
      "Epoch 54/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6791 - accuracy: 0.5543 - recall: 0.5824\n",
      "Epoch 54: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6789 - accuracy: 0.5542 - recall: 0.5821 - val_loss: 0.6846 - val_accuracy: 0.5332 - val_recall: 0.6743\n",
      "Epoch 55/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6789 - accuracy: 0.5488 - recall: 0.6020\n",
      "Epoch 55: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6786 - accuracy: 0.5489 - recall: 0.6016 - val_loss: 0.6759 - val_accuracy: 0.5558 - val_recall: 0.6292\n",
      "Epoch 56/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.5595 - recall: 0.5973\n",
      "Epoch 56: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6781 - accuracy: 0.5594 - recall: 0.5974 - val_loss: 0.6818 - val_accuracy: 0.5418 - val_recall: 0.6498\n",
      "Epoch 57/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6790 - accuracy: 0.5542 - recall: 0.5915\n",
      "Epoch 57: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6790 - accuracy: 0.5542 - recall: 0.5915 - val_loss: 0.6738 - val_accuracy: 0.5623 - val_recall: 0.6149\n",
      "Epoch 58/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6782 - accuracy: 0.5526 - recall: 0.6071\n",
      "Epoch 58: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6782 - accuracy: 0.5528 - recall: 0.6069 - val_loss: 0.6602 - val_accuracy: 0.6049 - val_recall: 0.5563\n",
      "Epoch 59/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6791 - accuracy: 0.5524 - recall: 0.5852\n",
      "Epoch 59: val_loss did not improve from 0.64525\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6792 - accuracy: 0.5522 - recall: 0.5846 - val_loss: 0.6545 - val_accuracy: 0.6229 - val_recall: 0.5246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x19706ac82b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(32, input_shape=(timesteps, input_dim),\n",
    "                    kernel_regularizer=regularizers.l2(0.01),\n",
    "                    recurrent_regularizer=regularizers.l2(0.01),\n",
    "                    bias_regularizer=regularizers.l2(0.01),\n",
    "                     dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', Recall()])\n",
    "\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "checkpoint = ModelCheckpoint('model/best_model_GRUs.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 1s 5ms/step\n",
      "      Prediction  Actual\n",
      "0       0.513968       0\n",
      "1       0.561190       0\n",
      "2       0.494488       0\n",
      "3       0.564398       0\n",
      "4       0.523057       0\n",
      "...          ...     ...\n",
      "8215    0.437075       0\n",
      "8216    0.408005       0\n",
      "8217    0.555621       0\n",
      "8218    0.379312       0\n",
      "8219    0.432868       0\n",
      "\n",
      "[8220 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "best_model = load_model('model/best_model_GRUs.h5')\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Compare the predictions with the actual values\n",
    "comparison = pd.DataFrame({'Prediction': predictions.flatten(), 'Actual': y_test})\n",
    "\n",
    "# Print the comparison\n",
    "print(comparison)\n",
    "predictions_labels = [1 if p > 0.5 else 0 for p in predictions.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxkAAAJaCAYAAABDWIqJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABIwUlEQVR4nO3deXhU5d3G8XsSyJAEJqxZkMUIFYhsAhbyVhEkEjRaEKhSEYKANjSgJGymRURcglhFqGxKJVRARStUiYoxGKgSFqORVSqIBoVJQAwjINlm3j8s0zNlS5jDDInfT69zvZlznjnnd9L3ovy4z/Mci8vlcgkAAAAATBLg7wIAAAAA1Cw0GQAAAABMRZMBAAAAwFQ0GQAAAABMRZMBAAAAwFQ0GQAAAABMRZMBAAAAwFQ0GQAAAABMRZMBAAAAwFS1/F3ApVB25Ct/lwAApprbZZq/SwAAU00oWObvEs7Jl3+XrN34Kp9dy5dIMgAAAACYqkYmGQAAAMBFc1b4u4JqjyQDAAAAgKlIMgAAAAAjl9PfFVR7JBkAAAAATEWSAQAAABg5STK8RZIBAAAAwFQkGQAAAICBizkZXiPJAAAAAGAqmgwAAADAyOn03XaRZs6cKYvFovHjx7v39erVSxaLxWNLSkry+F5BQYESEhIUEhKi8PBwTZo0SeXl5R5jcnJy1KVLF1mtVrVu3VoZGRlVro/HpQAAAIBqZOvWrVq0aJE6dux4xrH77rtPM2bMcH8OCQlx/1xRUaGEhARFRkZq48aNOnTokIYPH67atWvrySeflCTt379fCQkJSkpK0vLly5Wdna3Ro0crKipK8fHxla6RJAMAAAAwcjl9t1XR8ePHNXToUL344otq0KDBGcdDQkIUGRnp3mw2m/vY+++/r127dmnZsmXq3LmzbrnlFj322GOaN2+eSktLJUkLFy5UdHS0nnnmGbVr105jx47V4MGDNXv27CrVSZMBAAAA+ElJSYkcDofHVlJScs7xycnJSkhIUFxc3FmPL1++XI0bN1b79u2VlpamkydPuo/l5uaqQ4cOioiIcO+Lj4+Xw+HQzp073WP+99zx8fHKzc2t0n3xuBQAAABg5Kzw2aXS09P16KOPeux75JFHNH369DPGvvrqq/r000+1devWs57r7rvvVsuWLdW0aVNt27ZNU6ZM0Z49e/Tmm29Kkux2u0eDIcn92W63n3eMw+HQTz/9pODg4ErdF00GAAAA4CdpaWlKTU312Ge1Ws8Yd+DAAT344IPKyspSnTp1znqu+++/3/1zhw4dFBUVpT59+mjfvn1q1aqVuYVfAI9LAQAAAH5itVpls9k8trM1GXl5eSoqKlKXLl1Uq1Yt1apVS+vXr9fcuXNVq1YtVVScmb50795dkrR3715JUmRkpAoLCz3GnP4cGRl53jE2m63SKYZEkwEAAAB4ugwnfvfp00fbt29Xfn6+e+vWrZuGDh2q/Px8BQYGnvGd/Px8SVJUVJQkKTY2Vtu3b1dRUZF7TFZWlmw2m2JiYtxjsrOzPc6TlZWl2NjYKv0KeVwKAAAAuMzVq1dP7du399gXGhqqRo0aqX379tq3b59WrFihW2+9VY0aNdK2bduUkpKinj17upe67du3r2JiYjRs2DDNmjVLdrtdU6dOVXJysjs9SUpK0vPPP6/Jkydr5MiRWrdunVauXKnMzMwq1UuTAQAAABh58ZI8fwkKCtIHH3yg5557TidOnFDz5s01aNAgTZ061T0mMDBQa9as0ZgxYxQbG6vQ0FAlJiZ6vFcjOjpamZmZSklJ0Zw5c9SsWTMtXry4Su/IkCSLy+VymXZ3l4myI1/5uwQAMNXcLtP8XQIAmGpCwTJ/l3BOpV9t8dm1gq76tc+u5UskGQAAAICB6yJekgdPTPwGAAAAYCqSDAAAAMCoGs7JuNyQZAAAAAAwFUkGAAAAYMScDK+RZAAAAAAwFUkGAAAAYOSs8HcF1R5JBgAAAABTkWQAAAAARszJ8BpJBgAAAABTkWQAAAAARrwnw2skGQAAAABMRZIBAAAAGDEnw2skGQAAAABMRZMBAAAAwFQ8LgUAAAAYMfHbayQZAAAAAExFkgEAAAAYuFwV/i6h2iPJAAAAAGAqkgwAAADAiCVsvUaSAQAAAMBUJBkAAACAEatLeY0kAwAAAICpSDIAAAAAI+ZkeI0kAwAAAICpSDIAAAAAIyfvyfAWSQYAAAAAU5FkAAAAAEbMyfAaSQYAAAAAU5FkAAAAAEa8J8NrJBkAAAAATEWSAQAAABgxJ8NrJBkAAAAATEWSAQAAABgxJ8NrJBkAAAAATEWTAQAAAMBUPC4FAAAAGPG4lNdIMgAAAACYiiQDAAAAMHC5KvxdQrVHkgEAAADAVCQZAAAAgBFzMrxGkgEAAADAVCQZAAAAgJGLJMNbJBkAAAAATEWSAQAAABgxJ8NrJBkAAAAATEWSAQAAABgxJ8NrJBkAAAAATEWSAQAAABgxJ8NrJBkAAAAATEWSAQAAABgxJ8NrJBkAAAAATEWSAQAAABgxJ8NrJBkAAAAATEWTAQAAAMBUPC4FAAAAGPG4lNdIMgAAAACYiiQDAAAAMGIJW6+RZAAAAADVzMyZM2WxWDR+/Hj3vlOnTik5OVmNGjVS3bp1NWjQIBUWFnp8r6CgQAkJCQoJCVF4eLgmTZqk8vJyjzE5OTnq0qWLrFarWrdurYyMjCrXR5MBAAAAGDmdvtsuwtatW7Vo0SJ17NjRY39KSorefvttvf7661q/fr0OHjyogQMHuo9XVFQoISFBpaWl2rhxo5YuXaqMjAxNmzbNPWb//v1KSEhQ7969lZ+fr/Hjx2v06NFau3ZtlWqkyQAAAACqiePHj2vo0KF68cUX1aBBA/f+Y8eO6W9/+5ueffZZ3XTTTeratauWLFmijRs3atOmTZKk999/X7t27dKyZcvUuXNn3XLLLXrsscc0b948lZaWSpIWLlyo6OhoPfPMM2rXrp3Gjh2rwYMHa/bs2VWqkyYDAAAAMHI5fbaVlJTI4XB4bCUlJecsLTk5WQkJCYqLi/PYn5eXp7KyMo/9bdu2VYsWLZSbmytJys3NVYcOHRQREeEeEx8fL4fDoZ07d7rH/O+54+Pj3eeoLJoMAAAAwE/S09MVFhbmsaWnp5917KuvvqpPP/30rMftdruCgoJUv359j/0RERGy2+3uMcYG4/Tx08fON8bhcOinn36q9H2xuhQAAABg5MP3ZKSlpSk1NdVjn9VqPWPcgQMH9OCDDyorK0t16tTxVXkXjSQDAAAA8BOr1Sqbzeaxna3JyMvLU1FRkbp06aJatWqpVq1aWr9+vebOnatatWopIiJCpaWlKi4u9vheYWGhIiMjJUmRkZFnrDZ1+vOFxthsNgUHB1f6vmgyAAAAACMfzsmorD59+mj79u3Kz893b926ddPQoUPdP9euXVvZ2dnu7+zZs0cFBQWKjY2VJMXGxmr79u0qKipyj8nKypLNZlNMTIx7jPEcp8ecPkdl8bgUAAAAcJmrV6+e2rdv77EvNDRUjRo1cu8fNWqUUlNT1bBhQ9lsNo0bN06xsbHq0aOHJKlv376KiYnRsGHDNGvWLNntdk2dOlXJycnu9CQpKUnPP/+8Jk+erJEjR2rdunVauXKlMjMzq1QvTQYAAABg5MM5GWaaPXu2AgICNGjQIJWUlCg+Pl7z5893Hw8MDNSaNWs0ZswYxcbGKjQ0VImJiZoxY4Z7THR0tDIzM5WSkqI5c+aoWbNmWrx4seLj46tUi8XlcrlMu7PLRNmRr/xdAgCYam6XaRceBADVyISCZf4u4Zx+euNxn10rePBUn13Ll0gyAAAAAKNqmmRcTpj4DQAAAMBUJBkAAACAUc2bTeBzJBkAAAAATEWSAQAAABgxJ8NrJBkAAAAATEWTAQAAAMBUPC4FAAAAGPG4lNdIMgAAAACYiiQDAAAAMHKRZHiLJAMAAACAqUgyAAAAACPmZHiNJAMAAACAqUgyAAAAACOXy98VVHskGQAAAABMRZIBAAAAGDEnw2skGQAAAABMRZIBAAAAGJFkeI0kAwAAAICpSDIAAAAAI9747TWSDAAAAACmIskAAAAADFxO3pPhLZIMAAAAAKYiyQAAAACMWF3KayQZAAAAAExFkwEAAADAVDwuBQAAABixhK3XSDIAAAAAmIokAwAAADBiCVuvkWQAAAAAMBVJBgAAAGDEErZeI8kAAAAAYCqSDAAAAMCIJMNrJBkAAAAATEWSAQAAABi5WF3KWyQZAAAAAExFkgEAAAAYMSfDayQZAAAAAExFkgEAAAAY8cZvr9FkAP+x+OWVem7hEt3zu/56aHySe3/+jt2au2iptu/6QgEBAWr7q1ZaNPtx1bFaJUnHHD/qyWfnK+fjzQoICFBcr98o7cEkhYQES5K+O1So+MEjzrje8kXPqlP7dj65NwC/HL9Ovl2/6nedGraKUvmpUh3M+1Ib0l/TD18dco8JtNZWr6l3q81veygwqLa+Xr9N2VMzdPKIwz0mouNVuiHtLkW0v1KSZM/fpw1PvqrDuwskSQ2uilLck/eq0a+ukLVesI4XFeuL1RuV+9wqOcsrfHrPAC4/NBmApO279+j1f76jq1tHe+zP37FbSalTNXrYXfpTyhgFBgZqz96vFGCxuMdMeXSWDh85qhefe1Ll5eWa+uRsTZ81V7OmT/E41+I5T6p1dEv357Aw26W9KQC/SM26t1P+0izZt32lgMBAXT/5Tg1eNkVL+kxR+U8lkqRe04bqqps66+0xf1XJjyfVZ0aifvvCeL06cIYkqXaIVYNenqR9WZ8p+88ZCqgVoP9LHaRBL0/WCz0elLO8Qs7ycu36x0cq2vG1TjlOKDympW6eOUqWgAB9NGulP38FgPdczMnwFk0GfvFOnvxJDz36tKZPeVCLlr7icWzWnEUaOri/Rg+7070vumUz98/7vi7QR5s+0auL56h9u6slSX9KGaMxE6dpYvJohTdp5B5b32ZT40YNL/HdAPile3P4LI/P701YpD/mL1BEhyv13ZY9CqoXrA539VLmA/N0YOMuSdLaiS/o3g+fVtS1rXTos31q2LqpghvU08Zn3tCPh45KknJnv6nErJmyXdFYxd8U6ljBYR0rOOy+zo/ffa9mPdrpil+38d3NArhs+XXi95EjRzRr1izdcccdio2NVWxsrO644w49/fTTOnz48IVPAJjg8WfmqWfsdYq97lqP/d//UKxtu/aoYYMwDf1Dqnre9nuNSJ6kTz/f4R7z+Y7dstWr624wJKlHt2sVEGDRtl1feJxv7EOPqmfCEA0bM0Ef/mvTpb0pAPgPa70QSdKp4hOSpIgO0QoMqqWCj3a6xxzdd0iOb48oqsuv3J9/Ovqj2g/ppYDagaplra32Q3rp+y+/07Fvz/6/z/VbRii6V0d9u2n3Jb4jwAecLt9tNZTfkoytW7cqPj5eISEhiouL09VX//yXtMLCQs2dO1czZ87U2rVr1a1bt/Oep6SkRCUlJR77AkpKZP3P8/LA+bzzQY52/3ufXl0854xj33738/PL819aroljR6vtr67SW+9ma9SDaVr98kK1bH6Fjnz/gxrWD/P4Xq1agQqrV09Hjv4gSQoJrqNJ4+7TtR1iZAmw6IOcj/VA2gzNTZ+m3jf0uPQ3CeCXy2JRr+n36Lute/T9v7+VJIU2CVN5SZlKHCc9hp44ckyh4T//eVZ24pReu/MJ9V+coh4PDJAkFe+3641hT8lV4fkYye/fnKbw9leqVp0gfb58nT5+5h+X/r4AXPb81mSMGzdOv/vd77Rw4UJZDM+3S5LL5VJSUpLGjRun3Nzc854nPT1djz76qMe+qZMe0LTJD5peM2qWQ4WHNfO5RXrxuSdltQadcdz5n7d9/q7/rbojoa8kqd3VrbUpL19vrnlfKWPurdR1GtQPU+KQge7PHdq1UdGRo1qy4g2aDACXVJ/HE9X46mZ6ddBjVfpeLWttxT89Wgc/+bcyx85TQKBF3f6QoIEZE7X8tmkqLylzj12T/Lxq162j8HYt1fPPv9d1f7hVWxdmmn0rgE+5eE+G1/zWZHz++efKyMg4o8GQJIvFopSUFF177bVn+aantLQ0paameuwL+PE70+pEzbVrz5c6+kOx7hw51r2vosKpvPwdeuXNt/X2ihclSa2iW3h876qWLWQvLJIkNW7UQEeLj3kcLy+v0LEff1Tjhg3Oee2OMW2Uu/VTs24FAM5w04zhatXnWr36u8d13H7Uvf/E4WOqZa0tqy3EI80IbRymE0U//3nWdsD/ydasiVYMeFT6zz+4ZI6bp7HbF6lV367a8/Z/H/k8PWfj6JcHZQkM0M0zR+qTF96RqwY/BgLgwvzWZERGRmrLli1q27btWY9v2bJFERERFzyP1Wo949GostIjptSImq1H185a9fICj31Tn3hW0S2ba9Q9v1PzK6IU3riRvv7mW48x3xz4Vtf3uE6S1Kl9Ozl+PK6dX3ypa9r+/Czz5rx8OZ0udYw5+/9vS9IXX36lJkwCB3CJ3DRjuFr366aVdz4hxwHPORSF2/erorRcLX5zjb58d6ukn5ejtTVrrEOffilJqh0cJJfL5W4wJMnldMnlkiwBZ/7j4GmWAIsCagXKEhAgl5NlbIFfMr81GRMnTtT999+vvLw89enTx91QFBYWKjs7Wy+++KL+8pe/+Ks8/AKEhoboV1dd6bEvOLiO6tvqufffe/cgzfvbMrX5VbTa/qqV/vnOB9r/zbd69vE/S5JaXdlC1/fopulPzdG0SeNUVl6uJ2cv0C1xN7pXlvrnO1mqXbu22l7dSpL0Qc7HWpX5vh59iEf6AJivz+Mj1LZ/rP45erZKT5xSSJOf51mUOk6qvKRMpT/+pO2v5ajXw0N1qvi4So7/pD6PDtfBT/6tQ5/tkyR9868d6vmn36vP4yP0Wcb7sgRY9Os/3i5neYUO5P48sbvtgP+Ts7xCR744oIrSMkV0vErXT7lTe97ezHsyUP2RxHnNb01GcnKyGjdurNmzZ2v+/PmqqPj5D6TAwEB17dpVGRkZuvPOOy9wFuDSGnbXHSopLdNTc1+Qw/Gjrm59lV587gm1aNbUPeapRybriWfna9QDaQoIsCiu12/0p/FjPM6zMGOFDtmLFBgYqOiWzfWXGQ+pb+8bfH07AH4BOg+PkyTd9fpUj/3vpS7Szjf+JUnKmbFccrp0+6IHVSuolr5ev10fTM1wjz2675BWj3pWsePv0O9XPSKXy6Wind/ozeGzdKKoWJLkqqjQr8fcpgbRkZLFIsd3R5S/NEt5i9/zyX0CuLxZXC6X31u1srIyHTny8yNOjRs3Vu3atb0735GvzCgLAC4bc7tM83cJAGCqCQXL/F3COZ14/B6fXSt06uX7e/DGZfEyvtq1aysqKsrfZQAAAAAwwWXRZAAAAACXDeZkeM2vb/wGAAAAUPOQZAAAAABGvIzPayQZAAAAAExFkgEAAAAYMSfDayQZAAAAAExFkgEAAAAYuZiT4S2SDAAAAKAaWLBggTp27CibzSabzabY2Fi9++677uO9evWSxWLx2JKSkjzOUVBQoISEBIWEhCg8PFyTJk1SeXm5x5icnBx16dJFVqtVrVu3VkZGRpVrJckAAAAAjC7TORnNmjXTzJkz9atf/Uoul0tLly5V//799dlnn+maa66RJN13332aMWOG+zshISHunysqKpSQkKDIyEht3LhRhw4d0vDhw1W7dm09+eSTkqT9+/crISFBSUlJWr58ubKzszV69GhFRUUpPj6+0rXSZAAAAADVwO233+7x+YknntCCBQu0adMmd5MREhKiyMjIs37//fff165du/TBBx8oIiJCnTt31mOPPaYpU6Zo+vTpCgoK0sKFCxUdHa1nnnlGktSuXTt99NFHmj17dpWaDB6XAgAAAAxcTqfPtpKSEjkcDo+tpKTkgjVWVFTo1Vdf1YkTJxQbG+vev3z5cjVu3Fjt27dXWlqaTp486T6Wm5urDh06KCIiwr0vPj5eDodDO3fudI+Ji4vzuFZ8fLxyc3Or9DukyQAAAAD8JD09XWFhYR5benr6Ocdv375ddevWldVqVVJSklatWqWYmBhJ0t13361ly5bpww8/VFpaml5++WXdc8897u/a7XaPBkOS+7Pdbj/vGIfDoZ9++qnS98XjUgAAAICRD+dkpKWlKTU11WOf1Wo95/g2bdooPz9fx44d0xtvvKHExEStX79eMTExuv/++93jOnTooKioKPXp00f79u1Tq1atLtk9nA1NBgAAAOAnVqv1vE3F/woKClLr1q0lSV27dtXWrVs1Z84cLVq06Iyx3bt3lyTt3btXrVq1UmRkpLZs2eIxprCwUJLc8zgiIyPd+4xjbDabgoODK10nj0sBAAAA1ZTzP/M6ziY/P1+SFBUVJUmKjY3V9u3bVVRU5B6TlZUlm83mfuQqNjZW2dnZHufJysrymPdRGSQZAAAAgNFluoRtWlqabrnlFrVo0UI//vijVqxYoZycHK1du1b79u3TihUrdOutt6pRo0batm2bUlJS1LNnT3Xs2FGS1LdvX8XExGjYsGGaNWuW7Ha7pk6dquTkZHeakpSUpOeff16TJ0/WyJEjtW7dOq1cuVKZmZlVqpUmAwAAAKgGioqKNHz4cB06dEhhYWHq2LGj1q5dq5tvvlkHDhzQBx98oOeee04nTpxQ8+bNNWjQIE2dOtX9/cDAQK1Zs0ZjxoxRbGysQkNDlZiY6PFejejoaGVmZiolJUVz5sxRs2bNtHjx4iotXytJFpfLdXm2al4oO/KVv0sAAFPN7TLN3yUAgKkmFCzzdwnndHxif59dq+5f/umza/kSczIAAAAAmIrHpQAAAACjy3RORnVCkgEAAADAVCQZAAAAgIGLJMNrJBkAAAAATEWSAQAAABiRZHiNJAMAAACAqUgyAAAAACOn098VVHskGQAAAABMRZIBAAAAGDEnw2skGQAAAABMRZIBAAAAGJFkeI0kAwAAAICpSDIAAAAAA5eLJMNbJBkAAAAATEWSAQAAABgxJ8NrJBkAAAAATEWTAQAAAMBUPC4FAAAAGPG4lNdIMgAAAACYiiQDAAAAMHCRZHiNJAMAAACAqUgyAAAAACOSDK+RZAAAAAAwFUkGAAAAYOT0dwHVH0kGAAAAAFORZAAAAAAGrC7lPZIMAAAAAKYiyQAAAACMSDK8RpIBAAAAwFQkGQAAAIARq0t5jSQDAAAAgKlIMgAAAAADVpfyHkkGAAAAAFORZAAAAABGzMnwGkkGAAAAAFPRZAAAAAAwFY9LAQAAAAZM/PYeSQYAAAAAU5FkAAAAAEZM/PYaSQYAAAAAU5FkAAAAAAYukgyvkWQAAAAAMBVJBgAAAGBEkuE1kgwAAAAApiLJAAAAAAyYk+E9kgwAAAAApiLJAAAAAIxIMrxGkgEAAADAVCQZAAAAgAFzMrxHkgEAAADAVCQZAAAAgAFJhvdIMgAAAACYiiQDAAAAMCDJ8B5JBgAAAABTkWQAAAAARi6Lvyuo9kgyAAAAAJiKJgMAAACAqWgyAAAAAAOX03dbVSxYsEAdO3aUzWaTzWZTbGys3n33XffxU6dOKTk5WY0aNVLdunU1aNAgFRYWepyjoKBACQkJCgkJUXh4uCZNmqTy8nKPMTk5OerSpYusVqtat26tjIyMKv8OaTIAAACAaqBZs2aaOXOm8vLy9Mknn+imm25S//79tXPnTklSSkqK3n77bb3++utav369Dh48qIEDB7q/X1FRoYSEBJWWlmrjxo1aunSpMjIyNG3aNPeY/fv3KyEhQb1791Z+fr7Gjx+v0aNHa+3atVWq1eJyuVzm3Pblo+zIV/4uAQBMNbfLtAsPAoBqZELBMn+XcE6Hru/ts2tFffShV99v2LChnn76aQ0ePFhNmjTRihUrNHjwYEnSF198oXbt2ik3N1c9evTQu+++q9tuu00HDx5URESEJGnhwoWaMmWKDh8+rKCgIE2ZMkWZmZnasWOH+xpDhgxRcXGx3nvvvUrXRZIBAAAA+ElJSYkcDofHVlJScsHvVVRU6NVXX9WJEycUGxurvLw8lZWVKS4uzj2mbdu2atGihXJzcyVJubm56tChg7vBkKT4+Hg5HA53GpKbm+txjtNjTp+jsmgyAAAAAANfzslIT09XWFiYx5aenn7O2rZv3666devKarUqKSlJq1atUkxMjOx2u4KCglS/fn2P8REREbLb7ZIku93u0WCcPn762PnGOBwO/fTTT5X+HfKeDAAAAMBP0tLSlJqa6rHParWec3ybNm2Un5+vY8eO6Y033lBiYqLWr19/qcusMpoMAAAAwMDlw5fxWa3W8zYV/ysoKEitW7eWJHXt2lVbt27VnDlzdNddd6m0tFTFxcUeaUZhYaEiIyMlSZGRkdqyZYvH+U6vPmUc878rUhUWFspmsyk4OLjSdfK4FAAAAFBNOZ1OlZSUqGvXrqpdu7ays7Pdx/bs2aOCggLFxsZKkmJjY7V9+3YVFRW5x2RlZclmsykmJsY9xniO02NOn6OySDIAAAAAg6q+v8JX0tLSdMstt6hFixb68ccftWLFCuXk5Gjt2rUKCwvTqFGjlJqaqoYNG8pms2ncuHGKjY1Vjx49JEl9+/ZVTEyMhg0bplmzZslut2vq1KlKTk52pylJSUl6/vnnNXnyZI0cOVLr1q3TypUrlZmZWaVaaTIAAACAaqCoqEjDhw/XoUOHFBYWpo4dO2rt2rW6+eabJUmzZ89WQECABg0apJKSEsXHx2v+/Pnu7wcGBmrNmjUaM2aMYmNjFRoaqsTERM2YMcM9Jjo6WpmZmUpJSdGcOXPUrFkzLV68WPHx8VWqlfdkAEA1wHsyANQ0l/N7Mg5c18dn12q+NfvCg6oh5mQAAAAAMBWPSwEAAAAGNe85H98jyQAAAABgKpIMAAAAwMDl9N17MmoqkgwAAAAApiLJAAAAAAxIMrxHkgEAAADAVDQZAAAAAEzF41IAAACAAUvYeo8kAwAAAICpSDIAAAAAAyZ+e48kAwAAAICpSDIAAAAAA5eLJMNbJBkAAAAATEWSAQAAABi4nP6uoPojyQAAAABgKpIMAAAAwMDJnAyvkWQAAAAAMBVJBgAAAGDA6lLeI8kAAAAAYCqSDAAAAMCAN357jyQDAAAAgKlIMgAAAAADl8vfFVR/JBkAAAAATEWSAQAAABgwJ8N7F91klJaWqqioSE6n53vXW7Ro4XVRAAAAAKqvKjcZX375pUaOHKmNGzd67He5XLJYLKqoqDCtOAAAAMDXeOO396rcZIwYMUK1atXSmjVrFBUVJYuF/xIAAAAA/FeVm4z8/Hzl5eWpbdu2l6IeAAAAANVclZuMmJgYHTly5FLUAgAAAPidi8elvFapJWwdDod7e+qppzR58mTl5OTo+++/9zjmcDgudb0AAAAALnOVSjLq16/vMffC5XKpT58+HmOY+A0AAICagJfxea9STcaHH354qesAAAAAUENUqsm48cYb3T8XFBSoefPmZ6wq5XK5dODAAXOrAwAAAHyMJWy9V6k5GUbR0dE6fPjwGfuPHj2q6OhoU4oCAAAAUH1VeXWp03Mv/tfx48dVp04dU4oCAAAA/IXVpbxX6SYjNTVVkmSxWPTwww8rJCTEfayiokKbN29W586dTS8QAAAAQPVS6Sbjs88+k/RzkrF9+3YFBQW5jwUFBalTp06aOHGi+RUCAAAAPsTqUt6rdJNxeoWpe++9V3PmzJHNZrtkRQEAAACovqo8J2PJkiWXog4AAADgssDqUt6rcpNx0003nff4unXrLroYAAAAANVflZuMTp06eXwuKytTfn6+duzYocTERNMK80ajlnH+LgEATHWyrMTfJQCAqSb4u4DzYHUp71W5yZg9e/ZZ90+fPl3Hjx/3uiAAAAAA1VuVX8Z3Lvfcc49eeukls04HAAAA+IXTZfHZVlOZ1mTk5ubyMj4AAAAAVX9cauDAgR6fXS6XDh06pE8++UQPP/ywaYUBAAAA/sBrMrxX5SYjLCzM43NAQIDatGmjGTNmqG/fvqYVBgAAAKB6qlKTUVFRoXvvvVcdOnRQgwYNLlVNAAAAAKqxKs3JCAwMVN++fVVcXHyJygEAAAD8i4nf3qvyxO/27dvrq6++uhS1AAAAAKgBqtxkPP7445o4caLWrFmjQ4cOyeFweGwAAABAdeZyWXy21VSVnpMxY8YMTZgwQbfeeqsk6be//a0slv/+YlwulywWiyoqKsyvEgAAAEC1YXG5XJVapSswMFCHDh3S7t27zzvuxhtvNKUwb9hCr/J3CQBgqpNlJf4uAQBMVV76nb9LOKd/RQ722bVusL/hs2v5UqWTjNO9yOXQRAAAAAC4fFVpCVvj41EAAABATeQSf+f1VpWajKuvvvqCjcbRo0e9KggAAABA9ValJuPRRx89443fAAAAQE3irNSMZZxPlZqMIUOGKDw8/FLVAgAAAKAGqPR7MpiPAQAAgF8Cpyw+26oiPT1d1113nerVq6fw8HANGDBAe/bs8RjTq1cvWSwWjy0pKcljTEFBgRISEhQSEqLw8HBNmjRJ5eXlHmNycnLUpUsXWa1WtW7dWhkZGVWqtdJNRiVXugUAAABwCaxfv17JycnatGmTsrKyVFZWpr59++rEiRMe4+677z4dOnTIvc2aNct9rKKiQgkJCSotLdXGjRu1dOlSZWRkaNq0ae4x+/fvV0JCgnr37q38/HyNHz9eo0eP1tq1aytda6Xfk1Gd8J4MADUN78kAUNNczu/JyI64y2fX6lP42kV/9/DhwwoPD9f69evVs2dPST8nGZ07d9Zzzz131u+8++67uu2223Tw4EFFRERIkhYuXKgpU6bo8OHDCgoK0pQpU5SZmakdO3a4vzdkyBAVFxfrvffeq1RtlU4yAAAAAJirpKREDofDYyspqdw/LB07dkyS1LBhQ4/9y5cvV+PGjdW+fXulpaXp5MmT7mO5ubnq0KGDu8GQpPj4eDkcDu3cudM9Ji4uzuOc8fHxys3NrfR90WQAAAAABk4fbunp6QoLC/PY0tPTL1yj06nx48frN7/5jdq3b+/ef/fdd2vZsmX68MMPlZaWppdffln33HOP+7jdbvdoMCS5P9vt9vOOcTgc+umnny5Ym1TF1aUAAAAAmCctLU2pqake+6xW6wW/l5ycrB07duijjz7y2H///fe7f+7QoYOioqLUp08f7du3T61atTKn6EqgyQAAAAAMfPnGb6vVWqmmwmjs2LFas2aNNmzYoGbNmp13bPfu3SVJe/fuVatWrRQZGaktW7Z4jCksLJQkRUZGuv/v6X3GMTabTcHBwZWqkcelAAAAgGrA5XJp7NixWrVqldatW6fo6OgLfic/P1+SFBUVJUmKjY3V9u3bVVRU5B6TlZUlm82mmJgY95js7GyP82RlZSk2NrbStdJkAAAAAAa+nJNRFcnJyVq2bJlWrFihevXqyW63y263u+dJ7Nu3T4899pjy8vL09ddf66233tLw4cPVs2dPdezYUZLUt29fxcTEaNiwYfr888+1du1aTZ06VcnJye5EJSkpSV999ZUmT56sL774QvPnz9fKlSuVkpJS6VpZwhYAqgGWsAVQ01zOS9i+FzHEZ9fqV/hqpcee6+XYS5Ys0YgRI3TgwAHdc8892rFjh06cOKHmzZvrjjvu0NSpU2Wz2dzjv/nmG40ZM0Y5OTkKDQ1VYmKiZs6cqVq1/juTIicnRykpKdq1a5eaNWumhx9+WCNGjKh8rTQZAHD5o8kAUNPQZPysKk1GdcLEbwAAAMCgqo8x4UzMyQAAAABgKpIMAAAAwMCXS9jWVCQZAAAAAExFkgEAAAAYOAkyvEaSAQAAAMBUJBkAAACAgZM5GV4jyQAAAABgKpIMAAAAwKDGvanaD0gyAAAAAJiKJAMAAAAw4I3f3iPJAAAAAGAqkgwAAADAwGlhdSlvkWQAAAAAMBVJBgAAAGDA6lLeI8kAAAAAYCqSDAAAAMCA1aW8R5IBAAAAwFQ0GQAAAABMxeNSAAAAgIGTFWy9RpIBAAAAwFQkGQAAAICBU0QZ3iLJAAAAAGAqkgwAAADAgJfxeY8kAwAAAICpSDIAAAAAA1aX8h5JBgAAAABTkWQAAAAABk5/F1ADkGQAAAAAMBVJBgAAAGDA6lLeI8kAAAAAYCqSDAAAAMCA1aW8R5IBAAAAwFQkGQAAAIABq0t5jyQDAAAAgKlIMgAAAAADkgzvkWQAAAAAMBVJBgAAAGDgYnUpr5FkAAAAADAVTQYAAAAAU/G4FAAAAGDAxG/vkWQAAAAAMBVJBgAAAGBAkuE9kgwAAAAApiLJAAAAAAxc/i6gBiDJAAAAAGAqkgwAAADAwMnL+LxGkgEAAADAVCQZAAAAgAGrS3mPJAMAAACAqUgyAAAAAAOSDO+RZAAAAAAwFUkGAAAAYMB7MrxHkgEAAADAVCQZAAAAgAHvyfAeSQYAAAAAU5FkAAAAAAasLuU9kgwAAAAApqLJAAAAAKqB9PR0XXfddapXr57Cw8M1YMAA7dmzx2PMqVOnlJycrEaNGqlu3boaNGiQCgsLPcYUFBQoISFBISEhCg8P16RJk1ReXu4xJicnR126dJHValXr1q2VkZFRpVppMgAAAAADlw+3qli/fr2Sk5O1adMmZWVlqaysTH379tWJEyfcY1JSUvT222/r9ddf1/r163Xw4EENHDjQfbyiokIJCQkqLS3Vxo0btXTpUmVkZGjatGnuMfv371dCQoJ69+6t/Px8jR8/XqNHj9batWsrXavF5XLVuKWAbaFX+bsEADDVybISf5cAAKYqL/3O3yWcU3rLe3x2rbRvll30dw8fPqzw8HCtX79ePXv21LFjx9SkSROtWLFCgwcPliR98cUXateunXJzc9WjRw+9++67uu2223Tw4EFFRERIkhYuXKgpU6bo8OHDCgoK0pQpU5SZmakdO3a4rzVkyBAVFxfrvffeq1RtJBkAAACAgVMun20lJSVyOBweW0lJ5f5h6dixY5Kkhg0bSpLy8vJUVlamuLg495i2bduqRYsWys3NlSTl5uaqQ4cO7gZDkuLj4+VwOLRz5073GOM5To85fY7KoMkAAAAA/CQ9PV1hYWEeW3p6+gW/53Q6NX78eP3mN79R+/btJUl2u11BQUGqX7++x9iIiAjZ7Xb3GGODcfr46WPnG+NwOPTTTz9V6r5YwhYAAAAw8OUStmlpaUpNTfXYZ7VaL/i95ORk7dixQx999NGlKs0rNBkAAACAn1it1ko1FUZjx47VmjVrtGHDBjVr1sy9PzIyUqWlpSouLvZIMwoLCxUZGekes2XLFo/znV59yjjmf1ekKiwslM1mU3BwcKVq5HEpAAAAwOByXV3K5XJp7NixWrVqldatW6fo6GiP4127dlXt2rWVnZ3t3rdnzx4VFBQoNjZWkhQbG6vt27erqKjIPSYrK0s2m00xMTHuMcZznB5z+hyVQZIBAAAAVAPJyclasWKF/vnPf6pevXruORRhYWEKDg5WWFiYRo0apdTUVDVs2FA2m03jxo1TbGysevToIUnq27evYmJiNGzYMM2aNUt2u11Tp05VcnKyO1FJSkrS888/r8mTJ2vkyJFat26dVq5cqczMzErXyhK2AFANsIQtgJrmcl7CdnrLob671jfLKz3WYrGcdf+SJUs0YsQIST+/jG/ChAl65ZVXVFJSovj4eM2fP9/9KJQkffPNNxozZoxycnIUGhqqxMREzZw5U7Vq/Td/yMnJUUpKinbt2qVmzZrp4Ycfdl+jUrXSZADA5Y8mA0BNQ5Pxn2tVocmoTnhcCgAAADBwnj0wQBUw8RsAAACAqUgyAAAAAANnldd9wv8iyQAAAABgKpIMAAAAwIAcw3skGQAAAABMRZIBAAAAGDj9XUANQJIBAAAAwFQkGQAAAIABq0t5jyQDAAAAgKloMgAAAACYiselAAAAAAMelvIeSQYAAAAAU5FkAAAAAAYsYes9kgwAAAAApiLJAAAAAAxYwtZ7JBkAAAAATEWSAQAAABiQY3iPJAMAAACAqUgyAAAAAANWl/IeSQYAAAAAU5FkAAAAAAYuZmV4jSQDAAAAgKlIMgAAAAAD5mR4jyQDAAAAgKlIMgAAAAAD3vjtPZIMAAAAAKYiyQAAAAAMyDG8R5IBAAAAwFQ0GQAAAABMxeNSAAAAgAETv71HkgEAAADAVDQZgEFAQICmPpyibTvXq/DILn2+/UNNnjLWY0zanx7UJ59m6VDRDn3z7Wf655qX1a1bJ/fxFi2u0PPzZ3qc409/Hq/atWv7+nYAQHv/vUnlpd+dsc2d84QkyWq1au6cJ1R4aIeKj/5bK197QeHhjT3O0bx5U721+u9yFO/VwW8/11PpUxUYGOiP2wF8wunDrabicSnAICU1SaNGD1XS/ZO0e/e/dW2Xjpq/8Ck5HD9q4YKlkqS9e/dr4oTp+np/geoE11Hy2JFa9dbf1bljb31/5KiubtNKAQEBGv/An/XVvm/ULuZq/XVeukJCgzX1T+l+vkMAvzQ9/u9Wj4ag/TVttfa9V/WPf6yRJD3zl+m69ZY+GvL7P+jYMYfmznlCb6xcrJ69Bkj6+R9f3vrn31VoP6wbbuyvqMhwLXlpjsrKyzX14Zn+uCUA1YDF5XLVuIfObKFX+bsEVFMr31isoqIjGvvHh9z7Xl4+X6dOndJ9o1LP+p169erqO/s23Z5wj9bnbDzrmAfG36dRo4eqU/tel6Js/AKcLCvxdwmoIZ75y6NKuLWP2sZcL5utnuwHt+me4WP15puZkqQ2bVpp5/YN+s31t2vzlk/VL763/rl6qZq37KKioiOSpPvvG6b0J/+kyKYdVVZW5s/bQTVWXvqdv0s4p9FXDvbZtRZ//YbPruVLPC4FGGze9Klu7PV/at06WpLUvkNbxf5fN2W9v/6s42vXrq0RI4eouNih7dt3n/O8YbZ6+uGHY5ekZgCorNq1a2vo3QOVsfQ1SVLXLh0VFBSk7Ox/ucfs2bNP33zzrXr06CpJ6tGjq7bv+MLdYEjS+1k5Cguz6ZprrvbtDQCoNnhcCjB49pkFqmerq08+y1JFRYUCAwM149FntPK1f3qM69fvJr20dI5CQoJltxdpwO3DdfT7H856zquuaqn7kxI19U9P+uIWAOCc+vfvp/r1bVr695WSpIjIJiopKdGxYw6PcUVFhxUZ2eTnMRFNVFR42ON44X8+R0aES9p56QsHfKwmz5Xwlcs6yThw4IBGjhx53jElJSVyOBweWw18Agw+MnBQgu6867cade943fCb3yrp/ol64IHRunvoQI9xGzbk6vrY23TzTYP1QdYGZbz8VzVu0uiM80VFRejN1Uu0etU7Wprxmq9uAwDOauSIIXpv7Yc6dKjQ36UAqOEu6ybj6NGjWrp06XnHpKenKywszGMrLSv2TYGocR574iHNfmaR/vHGGu3auUevvrJa855/SakTxniMO3nyJ3311TfaujVfY//4kCrKKzQ88U6PMZGR4cp8d4U2b/5UD4z9ky9vAwDO0KLFFerT5wb97aUV7n2F9sOyWq0KC7N5jA0PbyK7/ee0orDwsMIjmngcj/jPZ3th0SWuGvAPlw//U1P59XGpt95667zHv/rqqwueIy0tTampnhNyr4jsdI7RwPmFBAfL6fQMSSucTgUEnL8fDwiwyBoU5P4cFRWhzHdXKD9/h8b8YTLpGgC/G5F4l4qKjuidd7Ld+/I+3abS0lLddNP1WrXqHUnS1Ve3UsuWzbRpU54kadOmPKU99ICaNGmkw4e/lyTF9empY8cc2rXrS9/fCIBqwa9NxoABA2SxWM77FzCLxXLec1itVlmt1ip9BziXd9/N1sTJf9S3Bw5q9+5/q2OnazR27Ei9/PLPKz+EhARr4uRkvZv5gez2IjVq1FD3/WGYoppGuv8HOioqQu+894oKDnynP6c9qcZNGrrPX1R45KzXBYBLyWKxKHH4XXp52euqqKhw73c4ftRLS17VX2Y9oh+OFsvh+FFznntcubmfaPOWTyVJ72et167d/9bSJXP10J+eUGREE814dLIWLFyq0tJSf90ScEkxJ8N7fm0yoqKiNH/+fPXv3/+sx/Pz89W1a1cfV4VfskkTHtXUaal65rkZatKkkeyHCrXkpVc0M/2vkqSKigpdfXUr3T10oBo1aqCjR4v1ad429bv5Ln2x++d/0evd53q1an2lWrW+Unv25nqcn+WVAfhDXJ8b1LJlMy05y9ywCROny+l0auVrL8hqter9rByNHfffRzydTqf6D0jUvL+m66MNb+nEiZN6+eXX9cj0p315CwCqGb++J+O3v/2tOnfurBkzZpz1+Oeff65rr732jMdXLoS/yAGoaXhPBoCa5nJ+T8awlgMvPMgkL3/zps+u5Ut+TTImTZqkEydOnPN469at9eGHH/qwIgAAAADe8muTccMNN5z3eGhoqG688UYfVQMAAACoBq/55DuX9RK2AAAAAKof3vgNAAAAGDjJMrxGkgEAAADAVCQZAAAAgEFNfhO3r5BkAAAAADAVTQYAAAAAU/G4FAAAAGBQtddA42xIMgAAAACYiiQDAAAAMGAJW++RZAAAAAAwFUkGAAAAYMAStt4jyQAAAABgKpIMAAAAwIDVpbxHkgEAAABUAxs2bNDtt9+upk2bymKxaPXq1R7HR4wYIYvF4rH169fPY8zRo0c1dOhQ2Ww21a9fX6NGjdLx48c9xmzbtk033HCD6tSpo+bNm2vWrFlVrpUmAwAAADBwuVw+26rixIkT6tSpk+bNm3fOMf369dOhQ4fc2yuvvOJxfOjQodq5c6eysrK0Zs0abdiwQffff7/7uMPhUN++fdWyZUvl5eXp6aef1vTp0/XCCy9UqVYelwIAAACqgVtuuUW33HLLecdYrVZFRkae9dju3bv13nvvaevWrerWrZsk6a9//atuvfVW/eUvf1HTpk21fPlylZaW6qWXXlJQUJCuueYa5efn69lnn/VoRi6EJAMAAAAwcMrls62kpEQOh8NjKykpuejac3JyFB4erjZt2mjMmDH6/vvv3cdyc3NVv359d4MhSXFxcQoICNDmzZvdY3r27KmgoCD3mPj4eO3Zs0c//PBDpeugyQAAAAD8JD09XWFhYR5benr6RZ2rX79++vvf/67s7Gw99dRTWr9+vW655RZVVFRIkux2u8LDwz2+U6tWLTVs2FB2u909JiIiwmPM6c+nx1QGj0sBAAAABr5cXSotLU2pqake+6xW60Wda8iQIe6fO3TooI4dO6pVq1bKyclRnz59vKqzqkgyAAAAAD+xWq2y2Wwe28U2Gf/rqquuUuPGjbV3715JUmRkpIqKijzGlJeX6+jRo+55HJGRkSosLPQYc/rzueZ6nA1NBgAAAGDg8uF/LqVvv/1W33//vaKioiRJsbGxKi4uVl5ennvMunXr5HQ61b17d/eYDRs2qKyszD0mKytLbdq0UYMGDSp9bZoMAAAAoBo4fvy48vPzlZ+fL0nav3+/8vPzVVBQoOPHj2vSpEnatGmTvv76a2VnZ6t///5q3bq14uPjJUnt2rVTv379dN9992nLli36+OOPNXbsWA0ZMkRNmzaVJN19990KCgrSqFGjtHPnTr322muaM2fOGY90XYjFVdUFeqsBW+hV/i4BAEx1suziVxoBgMtReel3/i7hnG5tcavPrvVOwTuVHpuTk6PevXufsT8xMVELFizQgAED9Nlnn6m4uFhNmzZV37599dhjj3lM5D569KjGjh2rt99+WwEBARo0aJDmzp2runXrusds27ZNycnJ2rp1qxo3bqxx48ZpypQpVbovmgwAqAZoMgDUNDQZP6tKk1Gd8LgUAAAAAFOxhC0AAABgUAMf9PE5kgwAAAAApiLJAAAAAAx8+TK+mookAwAAAICpSDIAAAAAg0v9krxfApIMAAAAAKYiyQAAAAAMnCQZXiPJAAAAAGAqkgwAAADAgPdkeI8kAwAAAICpSDIAAAAAA+ZkeI8kAwAAAICpSDIAAAAAA96T4T2SDAAAAACmIskAAAAADJysLuU1kgwAAAAApiLJAAAAAAzIMbxHkgEAAADAVDQZAAAAAEzF41IAAACAAS/j8x5JBgAAAABTkWQAAAAABiQZ3iPJAAAAAGAqkgwAAADAwMXL+LxGkgEAAADAVCQZAAAAgAFzMrxHkgEAAADAVCQZAAAAgIGLJMNrJBkAAAAATEWSAQAAABiwupT3SDIAAAAAmIokAwAAADBgdSnvkWQAAAAAMBVJBgAAAGDAnAzvkWQAAAAAMBVJBgAAAGDAnAzvkWQAAAAAMBVJBgAAAGDAG7+9R5IBAAAAwFQ0GQAAAABMxeNSAAAAgIGTJWy9RpIBAAAAwFQkGQAAAIABE7+9R5IBAAAAwFQkGQAAAIABczK8R5IBAAAAwFQkGQAAAIABczK8R5IBAAAAwFQkGQAAAIABczK8R5IBAAAAwFQkGQAAAIABczK8R5IBAAAAwFQkGQAAAIABczK8R5IBAAAAwFQkGQAAAIABczK8R5IBAAAAwFQkGQAAAICBy+X0dwnVHkkGAAAAUA1s2LBBt99+u5o2bSqLxaLVq1d7HHe5XJo2bZqioqIUHBysuLg4ffnllx5jjh49qqFDh8pms6l+/foaNWqUjh8/7jFm27ZtuuGGG1SnTh01b95cs2bNqnKtNBkAAABANXDixAl16tRJ8+bNO+vxWbNmae7cuVq4cKE2b96s0NBQxcfH69SpU+4xQ4cO1c6dO5WVlaU1a9Zow4YNuv/++93HHQ6H+vbtq5YtWyovL09PP/20pk+frhdeeKFKtVpcrpq3Rpct9Cp/lwAApjpZVuLvEgDAVOWl3/m7hHNq2aijz671zffbLup7FotFq1at0oABAyT9nGI0bdpUEyZM0MSJEyVJx44dU0REhDIyMjRkyBDt3r1bMTEx2rp1q7p16yZJeu+993Trrbfq22+/VdOmTbVgwQL9+c9/lt1uV1BQkCTpoYce0urVq/XFF19Uuj6SDAAAAMBPSkpK5HA4PLaSkqr/w9L+/ftlt9sVFxfn3hcWFqbu3bsrNzdXkpSbm6v69eu7GwxJiouLU0BAgDZv3uwe07NnT3eDIUnx8fHas2ePfvjhh0rXQ5MBAAAAGLhcLp9t6enpCgsL89jS09OrXLPdbpckRUREeOyPiIhwH7Pb7QoPD/c4XqtWLTVs2NBjzNnOYbxGZbC6FAAAAOAnaWlpSk1N9dhntVr9VI15aDIAAAAAA6cPX8ZntVpNaSoiIyMlSYWFhYqKinLvLywsVOfOnd1jioqKPL5XXl6uo0ePur8fGRmpwsJCjzGnP58eUxk8LgUAAABUc9HR0YqMjFR2drZ7n8Ph0ObNmxUbGytJio2NVXFxsfLy8txj1q1bJ6fTqe7du7vHbNiwQWVlZe4xWVlZatOmjRo0aFDpemgyAAAAAANfzsmoiuPHjys/P1/5+fmSfp7snZ+fr4KCAlksFo0fP16PP/643nrrLW3fvl3Dhw9X06ZN3StQtWvXTv369dN9992nLVu26OOPP9bYsWM1ZMgQNW3aVJJ09913KygoSKNGjdLOnTv12muvac6cOWc80nUhLGELANUAS9gCqGku5yVsr2hwjc+u9d0POys9NicnR7179z5jf2JiojIyMuRyufTII4/ohRdeUHFxsa6//nrNnz9fV199tXvs0aNHNXbsWL399tsKCAjQoEGDNHfuXNWtW9c9Ztu2bUpOTtbWrVvVuHFjjRs3TlOmTKnSfdFkAEA1QJMBoKa5nJuMqPoxPrvWoeJdPruWL/G4FAAAAABTsboUAAAAYODy4epSNRVJBgAAAABTkWQAAAAABjVwyrLPkWQAAAAAMBVJBgAAAGDgyzd+11QkGQAAAABMRZIBAAAAGDAnw3skGQAAAABMRZIBAAAAGDhJMrxGkgEAAADAVDQZAAAAAEzF41IAAACAARO/vUeSAQAAAMBUJBkAAACAAS/j8x5JBgAAAABTkWQAAAAABszJ8B5JBgAAAABTkWQAAAAABryMz3skGQAAAABMRZIBAAAAGLhYXcprJBkAAAAATEWSAQAAABgwJ8N7JBkAAAAATEWSAQAAABjwngzvkWQAAAAAMBVJBgAAAGDA6lLeI8kAAAAAYCqSDAAAAMCAORneI8kAAAAAYCqaDAAAAACm4nEpAAAAwIDHpbxHkgEAAADAVCQZAAAAgAE5hvdIMgAAAACYyuLioTPgopSUlCg9PV1paWmyWq3+LgcAvMafawDMQpMBXCSHw6GwsDAdO3ZMNpvN3+UAgNf4cw2AWXhcCgAAAICpaDIAAAAAmIomAwAAAICpaDKAi2S1WvXII48wORJAjcGfawDMwsRvAAAAAKYiyQAAAABgKpoMAAAAAKaiyQAAAABgKpoMAAAAAKaiyQAu0rx583TllVeqTp066t69u7Zs2eLvkgDgomzYsEG33367mjZtKovFotWrV/u7JADVHE0GcBFee+01paam6pFHHtGnn36qTp06KT4+XkVFRf4uDQCq7MSJE+rUqZPmzZvn71IA1BAsYQtchO7du+u6667T888/L0lyOp1q3ry5xo0bp4ceesjP1QHAxbNYLFq1apUGDBjg71IAVGMkGUAVlZaWKi8vT3Fxce59AQEBiouLU25urh8rAwAAuDzQZABVdOTIEVVUVCgiIsJjf0REhOx2u5+qAgAAuHzQZAAAAAAwFU0GUEWNGzdWYGCgCgsLPfYXFhYqMjLST1UBAABcPmgygCoKCgpS165dlZ2d7d7ndDqVnZ2t2NhYP1YGAABweajl7wKA6ig1NVWJiYnq1q2bfv3rX+u5557TiRMndO+99/q7NACosuPHj2vv3r3uz/v371d+fr4aNmyoFi1a+LEyANUVS9gCF+n555/X008/Lbvdrs6dO2vu3Lnq3r27v8sCgCrLyclR7969z9ifmJiojIwM3xcEoNqjyQAAAABgKuZkAAAAADAVTQYAAAAAU9FkAAAAADAVTQYAAAAAU9FkAAAAADAVTQYAAAAAU9FkAAAAADAVTQYAXGZGjBihAQMGuD/36tVL48eP93kdOTk5slgsKi4u9vm1AQDVG00GAFTSiBEjZLFYZLFYFBQUpNatW2vGjBkqLy+/pNd988039dhjj1VqLI0BAOByUMvfBQBAddKvXz8tWbJEJSUleuedd5ScnKzatWsrLS3NY1xpaamCgoJMuWbDhg1NOQ8AAL5CkgEAVWC1WhUZGamWLVtqzJgxiouL01tvveV+xOmJJ55Q06ZN1aZNG0nSgQMHdOedd6p+/fpq2LCh+vfvr6+//tp9voqKCqWmpqp+/fpq1KiRJk+eLJfL5XHN/31cqqSkRFOmTFHz5s1ltVrVunVr/e1vf9PXX3+t3r17S5IaNGggi8WiESNGSJKcTqfS09MVHR2t4OBgderUSW+88YbHdd555x1dffXVCg4OVu/evT3qBACgKmgyAMALwcHBKi0tlSRlZ2drz549ysrK0po1a1RWVqb4+HjVq1dP//rXv/Txxx+rbt266tevn/s7zzzzjDIyMvTSSy/po48+0tGjR7Vq1arzXnP48OF65ZVXNHfuXO3evVuLFi1S3bp11bx5c/3jH/+QJO3Zs0eHDh3SnDlzJEnp6en6+9//roULF2rnzp1KSUnRPffco/Xr10v6uRkaOHCgbr/9duXn52v06NF66KGHLtWvDQBQw/G4FABcBJfLpezsbK1du1bjxo3T4cOHFRoaqsWLF7sfk1q2bJmcTqcWL14si8UiSVqyZInq16+vnJwc9e3bV88995zS0tI0cOBASdLChQu1du3ac1733//+t1auXKmsrCzFxcVJkq666ir38dOPVoWHh6t+/fqSfk4+nnzySX3wwQeKjY11f+ejjz7SokWLdOONN2rBggVq1aqVnnnmGUlSmzZttH37dj311FMm/tYAAL8UNBkAUAVr1qxR3bp1VVZWJqfTqbvvvlvTp09XcnKyOnTo4DEP4/PPP9fevXtVr149j3OcOnVK+/bt07Fjx3To0CF1797dfaxWrVrq1q3bGY9MnZafn6/AwEDdeOONla557969OnnypG6++WaP/aWlpbr22mslSbt37/aoQ5K7IQEAoKpoMgCgCnr37q0FCxYoKChITZs2Va1a//1jNDQ01GPs8ePH1bVrVy1fvvyM8zRp0uSirh8cHFzl7xw/flySlJmZqSuuuMLjmNVqvag6AAA4H5oMAKiC0NBQtW7dulJju3Tpotdee03h4eGy2WxnHRMVFaXNmzerZ8+ekqTy8nLl5eWpS5cuZx3foUMHOZ1OrV+/3v24lNHpJKWiosK9LyYmRlarVQUFBedMQNq1a6e33nrLY9+mTZsufJMAAJwFE78B4BIZOnSoGjdurP79++tf//qX9u/fr5ycHD3wwAP69ttvJUkPPvigZs6cqdWrV+uLL77QH//4x/O+4+LKK69UYmKiRo4cqdWrV7vPuXLlSklSy5YtZbFYtGbNGh0+fFjHjx9XvXr1NHHiRKWkpGjp0qXat2+fPv30U/31r3/V0qVLJUlJSUn68ssvNWnSJO3Zs0crVqxQRkbGpf4VAQBqKJoMALhEQkJCtGHDBrVo0UIDBw5Uu3btNGrUKJ06dcqdbEyYMEHDhg1TYmKiYmNjVa9ePd1xxx3nPe+CBQs0ePBg/fGPf1Tbtm1133336cSJE5KkK664Qo8++qgeeughRUREaOzYsZKkxx57TA8//LDS09PVrl079evXT5mZmYqOjpYktWjRQv/4xz+0evVqderUSQsXLtSTTz55CX87AICazOI61+xCAAAAALgIJBkAAAAATEWTAQAAAMBUNBkAAAAATEWTAQAAAMBUNBkAAAAATEWTAQAAAMBUNBkAAAAATEWTAQAAAMBUNBkAAAAATEWTAQAAAMBUNBkAAAAATEWTAQAAAMBU/w8rXDufooqtqQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_labels)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.69      0.76      6688\n",
      "           1       0.25      0.46      0.32      1532\n",
      "\n",
      "    accuracy                           0.65      8220\n",
      "   macro avg       0.55      0.57      0.54      8220\n",
      "weighted avg       0.74      0.65      0.68      8220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, predictions_labels, output_dict=True)\n",
    "print(classification_report(y_test, predictions_labels))\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv('result/GRUs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
