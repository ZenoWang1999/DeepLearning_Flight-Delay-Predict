{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = pd.read_csv('data/data_encoded_scaled.csv')\n",
    "data_cleaned = pd.read_csv('data/data_cleaned.csv')\n",
    "data_feature = data_cleaned.drop(columns=['TOTAL_DELAY', 'DEP_DEL15'])\n",
    "data_target = data_cleaned['DEP_DEL15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = data_encoded_scaled.drop(columns=['DEP_DEL15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.968432919954903\n"
     ]
    }
   ],
   "source": [
    "sequence_days = 7\n",
    "daily_counts = data_feature.groupby(['MONTH', 'DAY_OF_MONTH', 'DEPARTING_AIRPORT']).size() # 一天一个机场的航班数\n",
    "average_rows = daily_counts.mean()\n",
    "\n",
    "print(average_rows*sequence_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_encoded_scaled.assign(TARGET=data_target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MONTH  DAY_OF_MONTH  DAY_OF_WEEK  DEP_TIME_BLK  DISTANCE_GROUP  \\\n",
      "0    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "1    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "2    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "3    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "4    0.0           0.0     0.166667      0.166667        0.285714   \n",
      "\n",
      "   SEGMENT_NUMBER  CONCURRENT_FLIGHTS  NUMBER_OF_SEATS  AIRPORT_FLIGHTS_MONTH  \\\n",
      "0           0.000            0.098592              1.0               0.115453   \n",
      "1           0.000            0.098592              1.0               0.115453   \n",
      "2           0.000            0.112676              1.0               0.111384   \n",
      "3           0.000            0.267606              1.0               0.333661   \n",
      "4           0.125            0.042254              1.0               0.028528   \n",
      "\n",
      "   AIRLINE_FLIGHTS_MONTH  ...  PREVIOUS_AIRPORT_4  PREVIOUS_AIRPORT_5  \\\n",
      "0               0.183515  ...                 0.0                 0.0   \n",
      "1               0.183515  ...                 0.0                 0.0   \n",
      "2               0.183515  ...                 0.0                 0.0   \n",
      "3               0.183515  ...                 0.0                 0.0   \n",
      "4               0.183515  ...                 0.0                 1.0   \n",
      "\n",
      "   PREVIOUS_AIRPORT_6  PRCP  SNOW  SNWD      TMAX      AWND  RESIDUALS  TARGET  \n",
      "0                 1.0   0.0   0.0   0.0  0.353982  0.198638   0.052138       0  \n",
      "1                 1.0   0.0   0.0   0.0  0.353982  0.198638   0.052775       0  \n",
      "2                 1.0   0.0   0.0   0.0  0.451327  0.172291   0.051816       0  \n",
      "3                 1.0   0.0   0.0   0.0  0.699115  0.370930   0.051928       0  \n",
      "4                 0.0   0.0   0.0   0.0  0.460177  0.178804   0.051987       0  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences shape: (41100, 28, 33)\n",
      "Target values shape: (41100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequence_length = int(round(average_rows*sequence_days))\n",
    "\n",
    "unique_dep_airport = data_feature['DEPARTING_AIRPORT'].unique()\n",
    "unique_flight_number = data_feature['FLIGHT_NUMBER'].unique()\n",
    "\n",
    "X_sequences = []\n",
    "y_targets = []\n",
    "\n",
    "for dep_airport in unique_dep_airport:\n",
    "    flight_data = data_full[data_feature['DEPARTING_AIRPORT'] == dep_airport]\n",
    "    flight_data_values = flight_data.iloc[:, :-1].values\n",
    "    flight_data_target = flight_data.iloc[:, -1].values\n",
    "    for i in range(len(flight_data) - sequence_length):\n",
    "        X_sequences.append(flight_data_values[i:i+sequence_length])\n",
    "        y_targets.append(flight_data_target[i+sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_targets = np.array(y_targets)\n",
    "\n",
    "print(\"Input sequences shape:\", X_sequences.shape)\n",
    "print(\"Target values shape:\", y_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = list(zip(X_sequences, y_targets))\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Separate the sequences and targets\n",
    "X_train, y_train = zip(*train_data)\n",
    "X_test, y_test = zip(*test_data)\n",
    "\n",
    "# Convert the results back to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "timesteps = X_train.shape[1]\n",
    "input_dim = X_train.shape[2]\n",
    "\n",
    "weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train) , y = y_train)\n",
    "class_weights = dict(zip(np.unique(y_train), weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 1.3487 - accuracy: 0.4540 - recall_3: 0.5649\n",
      "Epoch 1: val_loss improved from inf to 1.14188, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 4s 3ms/step - loss: 1.3489 - accuracy: 0.4540 - recall_3: 0.5650 - val_loss: 1.1419 - val_accuracy: 0.7550 - val_recall_3: 0.1254\n",
      "Epoch 2/300\n",
      " 53/822 [>.............................] - ETA: 2s - loss: 1.2594 - accuracy: 0.5094 - recall_3: 0.4916  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805/822 [============================>.] - ETA: 0s - loss: 1.2166 - accuracy: 0.5085 - recall_3: 0.5222\n",
      "Epoch 2: val_loss improved from 1.14188 to 1.07244, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 1.2143 - accuracy: 0.5084 - recall_3: 0.5213 - val_loss: 1.0724 - val_accuracy: 0.7415 - val_recall_3: 0.1828\n",
      "Epoch 3/300\n",
      "807/822 [============================>.] - ETA: 0s - loss: 1.1195 - accuracy: 0.5278 - recall_3: 0.5319\n",
      "Epoch 3: val_loss improved from 1.07244 to 1.03845, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 1.1183 - accuracy: 0.5273 - recall_3: 0.5327 - val_loss: 1.0384 - val_accuracy: 0.6308 - val_recall_3: 0.4639\n",
      "Epoch 4/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 1.0386 - accuracy: 0.5290 - recall_3: 0.5686\n",
      "Epoch 4: val_loss improved from 1.03845 to 0.95933, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 1.0376 - accuracy: 0.5294 - recall_3: 0.5685 - val_loss: 0.9593 - val_accuracy: 0.6545 - val_recall_3: 0.4254\n",
      "Epoch 5/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.9710 - accuracy: 0.5449 - recall_3: 0.5612\n",
      "Epoch 5: val_loss improved from 0.95933 to 0.92985, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.9706 - accuracy: 0.5447 - recall_3: 0.5617 - val_loss: 0.9299 - val_accuracy: 0.5661 - val_recall_3: 0.6025\n",
      "Epoch 6/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.9152 - accuracy: 0.5443 - recall_3: 0.5768\n",
      "Epoch 6: val_loss improved from 0.92985 to 0.85942, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.9148 - accuracy: 0.5447 - recall_3: 0.5772 - val_loss: 0.8594 - val_accuracy: 0.6238 - val_recall_3: 0.5131\n",
      "Epoch 7/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.8672 - accuracy: 0.5557 - recall_3: 0.5618\n",
      "Epoch 7: val_loss did not improve from 0.85942\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.8675 - accuracy: 0.5553 - recall_3: 0.5629 - val_loss: 0.8608 - val_accuracy: 0.5111 - val_recall_3: 0.6959\n",
      "Epoch 8/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.8288 - accuracy: 0.5442 - recall_3: 0.5809\n",
      "Epoch 8: val_loss improved from 0.85942 to 0.82138, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.8288 - accuracy: 0.5442 - recall_3: 0.5809 - val_loss: 0.8214 - val_accuracy: 0.5239 - val_recall_3: 0.6738\n",
      "Epoch 9/300\n",
      "805/822 [============================>.] - ETA: 0s - loss: 0.7964 - accuracy: 0.5555 - recall_3: 0.5843\n",
      "Epoch 9: val_loss improved from 0.82138 to 0.79289, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7966 - accuracy: 0.5551 - recall_3: 0.5844 - val_loss: 0.7929 - val_accuracy: 0.5268 - val_recall_3: 0.6730\n",
      "Epoch 10/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.7733 - accuracy: 0.5471 - recall_3: 0.5919\n",
      "Epoch 10: val_loss improved from 0.79289 to 0.76972, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7733 - accuracy: 0.5473 - recall_3: 0.5921 - val_loss: 0.7697 - val_accuracy: 0.5260 - val_recall_3: 0.6738\n",
      "Epoch 11/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7538 - accuracy: 0.5515 - recall_3: 0.5987\n",
      "Epoch 11: val_loss improved from 0.76972 to 0.76102, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7536 - accuracy: 0.5512 - recall_3: 0.5993 - val_loss: 0.7610 - val_accuracy: 0.5040 - val_recall_3: 0.7000\n",
      "Epoch 12/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.7396 - accuracy: 0.5452 - recall_3: 0.5851\n",
      "Epoch 12: val_loss improved from 0.76102 to 0.73482, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7400 - accuracy: 0.5456 - recall_3: 0.5854 - val_loss: 0.7348 - val_accuracy: 0.5363 - val_recall_3: 0.6557\n",
      "Epoch 13/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.7269 - accuracy: 0.5423 - recall_3: 0.5986\n",
      "Epoch 13: val_loss improved from 0.73482 to 0.70712, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.7267 - accuracy: 0.5431 - recall_3: 0.5982 - val_loss: 0.7071 - val_accuracy: 0.5873 - val_recall_3: 0.5746\n",
      "Epoch 14/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.7182 - accuracy: 0.5528 - recall_3: 0.5825\n",
      "Epoch 14: val_loss did not improve from 0.70712\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7184 - accuracy: 0.5522 - recall_3: 0.5819 - val_loss: 0.7209 - val_accuracy: 0.5316 - val_recall_3: 0.6705\n",
      "Epoch 15/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.7119 - accuracy: 0.5489 - recall_3: 0.5835\n",
      "Epoch 15: val_loss did not improve from 0.70712\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7116 - accuracy: 0.5487 - recall_3: 0.5854 - val_loss: 0.7268 - val_accuracy: 0.4998 - val_recall_3: 0.7172\n",
      "Epoch 16/300\n",
      "808/822 [============================>.] - ETA: 0s - loss: 0.7065 - accuracy: 0.5455 - recall_3: 0.5983\n",
      "Epoch 16: val_loss did not improve from 0.70712\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7060 - accuracy: 0.5452 - recall_3: 0.5987 - val_loss: 0.7074 - val_accuracy: 0.5330 - val_recall_3: 0.6615\n",
      "Epoch 17/300\n",
      "810/822 [============================>.] - ETA: 0s - loss: 0.7017 - accuracy: 0.5462 - recall_3: 0.5973\n",
      "Epoch 17: val_loss improved from 0.70712 to 0.68513, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.7018 - accuracy: 0.5471 - recall_3: 0.5964 - val_loss: 0.6851 - val_accuracy: 0.5923 - val_recall_3: 0.5787\n",
      "Epoch 18/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6981 - accuracy: 0.5487 - recall_3: 0.5979\n",
      "Epoch 18: val_loss did not improve from 0.68513\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6982 - accuracy: 0.5483 - recall_3: 0.5984 - val_loss: 0.7094 - val_accuracy: 0.5129 - val_recall_3: 0.6926\n",
      "Epoch 19/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6959 - accuracy: 0.5446 - recall_3: 0.6052\n",
      "Epoch 19: val_loss improved from 0.68513 to 0.67683, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6961 - accuracy: 0.5445 - recall_3: 0.6050 - val_loss: 0.6768 - val_accuracy: 0.6004 - val_recall_3: 0.5623\n",
      "Epoch 20/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.5518 - recall_3: 0.5872\n",
      "Epoch 20: val_loss did not improve from 0.67683\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6943 - accuracy: 0.5518 - recall_3: 0.5872 - val_loss: 0.7107 - val_accuracy: 0.4947 - val_recall_3: 0.7230\n",
      "Epoch 21/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6926 - accuracy: 0.5456 - recall_3: 0.5990\n",
      "Epoch 21: val_loss did not improve from 0.67683\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6927 - accuracy: 0.5454 - recall_3: 0.5991 - val_loss: 0.6999 - val_accuracy: 0.5201 - val_recall_3: 0.6852\n",
      "Epoch 22/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5380 - recall_3: 0.6122\n",
      "Epoch 22: val_loss did not improve from 0.67683\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6916 - accuracy: 0.5381 - recall_3: 0.6121 - val_loss: 0.6914 - val_accuracy: 0.5383 - val_recall_3: 0.6590\n",
      "Epoch 23/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6893 - accuracy: 0.5496 - recall_3: 0.6038\n",
      "Epoch 23: val_loss improved from 0.67683 to 0.67607, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6896 - accuracy: 0.5497 - recall_3: 0.6031 - val_loss: 0.6761 - val_accuracy: 0.5896 - val_recall_3: 0.5877\n",
      "Epoch 24/300\n",
      "803/822 [============================>.] - ETA: 0s - loss: 0.6902 - accuracy: 0.5474 - recall_3: 0.5820\n",
      "Epoch 24: val_loss did not improve from 0.67607\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6898 - accuracy: 0.5479 - recall_3: 0.5825 - val_loss: 0.6828 - val_accuracy: 0.5677 - val_recall_3: 0.6205\n",
      "Epoch 25/300\n",
      "807/822 [============================>.] - ETA: 0s - loss: 0.6868 - accuracy: 0.5510 - recall_3: 0.5915\n",
      "Epoch 25: val_loss improved from 0.67607 to 0.66921, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6881 - accuracy: 0.5517 - recall_3: 0.5905 - val_loss: 0.6692 - val_accuracy: 0.6014 - val_recall_3: 0.5623\n",
      "Epoch 26/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6870 - accuracy: 0.5499 - recall_3: 0.5974\n",
      "Epoch 26: val_loss did not improve from 0.66921\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6872 - accuracy: 0.5498 - recall_3: 0.5978 - val_loss: 0.6803 - val_accuracy: 0.5677 - val_recall_3: 0.6230\n",
      "Epoch 27/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6875 - accuracy: 0.5503 - recall_3: 0.5915\n",
      "Epoch 27: val_loss did not improve from 0.66921\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6873 - accuracy: 0.5503 - recall_3: 0.5919 - val_loss: 0.6865 - val_accuracy: 0.5525 - val_recall_3: 0.6426\n",
      "Epoch 28/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5503 - recall_3: 0.5948\n",
      "Epoch 28: val_loss improved from 0.66921 to 0.66539, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6861 - accuracy: 0.5509 - recall_3: 0.5931 - val_loss: 0.6654 - val_accuracy: 0.6090 - val_recall_3: 0.5574\n",
      "Epoch 29/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6853 - accuracy: 0.5482 - recall_3: 0.5976\n",
      "Epoch 29: val_loss improved from 0.66539 to 0.66178, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6853 - accuracy: 0.5482 - recall_3: 0.5976 - val_loss: 0.6618 - val_accuracy: 0.6101 - val_recall_3: 0.5484\n",
      "Epoch 30/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6852 - accuracy: 0.5513 - recall_3: 0.5945\n",
      "Epoch 30: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6849 - accuracy: 0.5509 - recall_3: 0.5956 - val_loss: 0.6882 - val_accuracy: 0.5380 - val_recall_3: 0.6566\n",
      "Epoch 31/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5472 - recall_3: 0.5974\n",
      "Epoch 31: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6853 - accuracy: 0.5472 - recall_3: 0.5976 - val_loss: 0.6883 - val_accuracy: 0.5345 - val_recall_3: 0.6623\n",
      "Epoch 32/300\n",
      "810/822 [============================>.] - ETA: 0s - loss: 0.6842 - accuracy: 0.5482 - recall_3: 0.5915\n",
      "Epoch 32: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6846 - accuracy: 0.5485 - recall_3: 0.5913 - val_loss: 0.6877 - val_accuracy: 0.5383 - val_recall_3: 0.6590\n",
      "Epoch 33/300\n",
      "812/822 [============================>.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5497 - recall_3: 0.5957\n",
      "Epoch 33: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6847 - accuracy: 0.5495 - recall_3: 0.5962 - val_loss: 0.6843 - val_accuracy: 0.5468 - val_recall_3: 0.6484\n",
      "Epoch 34/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6845 - accuracy: 0.5510 - recall_3: 0.5960\n",
      "Epoch 34: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6843 - accuracy: 0.5507 - recall_3: 0.5958 - val_loss: 0.6882 - val_accuracy: 0.5360 - val_recall_3: 0.6598\n",
      "Epoch 35/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6850 - accuracy: 0.5517 - recall_3: 0.5729\n",
      "Epoch 35: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6849 - accuracy: 0.5518 - recall_3: 0.5738 - val_loss: 0.7035 - val_accuracy: 0.4971 - val_recall_3: 0.7189\n",
      "Epoch 36/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6828 - accuracy: 0.5495 - recall_3: 0.6101\n",
      "Epoch 36: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6828 - accuracy: 0.5495 - recall_3: 0.6101 - val_loss: 0.6794 - val_accuracy: 0.5620 - val_recall_3: 0.6311\n",
      "Epoch 37/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6837 - accuracy: 0.5488 - recall_3: 0.5982\n",
      "Epoch 37: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6834 - accuracy: 0.5483 - recall_3: 0.5993 - val_loss: 0.7000 - val_accuracy: 0.5029 - val_recall_3: 0.7107\n",
      "Epoch 38/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6830 - accuracy: 0.5468 - recall_3: 0.5976\n",
      "Epoch 38: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6830 - accuracy: 0.5468 - recall_3: 0.5976 - val_loss: 0.6845 - val_accuracy: 0.5459 - val_recall_3: 0.6541\n",
      "Epoch 39/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.5498 - recall_3: 0.5948\n",
      "Epoch 39: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6828 - accuracy: 0.5487 - recall_3: 0.5946 - val_loss: 0.6818 - val_accuracy: 0.5505 - val_recall_3: 0.6475\n",
      "Epoch 40/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6824 - accuracy: 0.5538 - recall_3: 0.6054\n",
      "Epoch 40: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6828 - accuracy: 0.5533 - recall_3: 0.6052 - val_loss: 0.6898 - val_accuracy: 0.5287 - val_recall_3: 0.6680\n",
      "Epoch 41/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6824 - accuracy: 0.5485 - recall_3: 0.6054\n",
      "Epoch 41: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6821 - accuracy: 0.5490 - recall_3: 0.6060 - val_loss: 0.6664 - val_accuracy: 0.5920 - val_recall_3: 0.5869\n",
      "Epoch 42/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6824 - accuracy: 0.5480 - recall_3: 0.6013\n",
      "Epoch 42: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6822 - accuracy: 0.5482 - recall_3: 0.6015 - val_loss: 0.6861 - val_accuracy: 0.5356 - val_recall_3: 0.6590\n",
      "Epoch 43/300\n",
      "810/822 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.5497 - recall_3: 0.6012\n",
      "Epoch 43: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6818 - accuracy: 0.5502 - recall_3: 0.6009 - val_loss: 0.6776 - val_accuracy: 0.5640 - val_recall_3: 0.6180\n",
      "Epoch 44/300\n",
      "808/822 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.5507 - recall_3: 0.6099\n",
      "Epoch 44: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6811 - accuracy: 0.5512 - recall_3: 0.6095 - val_loss: 0.6670 - val_accuracy: 0.5897 - val_recall_3: 0.5959\n",
      "Epoch 45/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6817 - accuracy: 0.5511 - recall_3: 0.6099\n",
      "Epoch 45: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6817 - accuracy: 0.5511 - recall_3: 0.6099 - val_loss: 0.6893 - val_accuracy: 0.5341 - val_recall_3: 0.6746\n",
      "Epoch 46/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6815 - accuracy: 0.5547 - recall_3: 0.6101\n",
      "Epoch 46: val_loss did not improve from 0.66178\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6815 - accuracy: 0.5549 - recall_3: 0.6099 - val_loss: 0.6671 - val_accuracy: 0.5852 - val_recall_3: 0.5975\n",
      "Epoch 47/300\n",
      "807/822 [============================>.] - ETA: 0s - loss: 0.6831 - accuracy: 0.5456 - recall_3: 0.5980\n",
      "Epoch 47: val_loss improved from 0.66178 to 0.64614, saving model to model\\best_model_hybrid_rnn.h5\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6821 - accuracy: 0.5472 - recall_3: 0.5966 - val_loss: 0.6461 - val_accuracy: 0.6458 - val_recall_3: 0.4770\n",
      "Epoch 48/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.5494 - recall_3: 0.5889\n",
      "Epoch 48: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6824 - accuracy: 0.5495 - recall_3: 0.5891 - val_loss: 0.6862 - val_accuracy: 0.5315 - val_recall_3: 0.6664\n",
      "Epoch 49/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6811 - accuracy: 0.5518 - recall_3: 0.5944\n",
      "Epoch 49: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6812 - accuracy: 0.5509 - recall_3: 0.5954 - val_loss: 0.6943 - val_accuracy: 0.5134 - val_recall_3: 0.6943\n",
      "Epoch 50/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6813 - accuracy: 0.5507 - recall_3: 0.6010\n",
      "Epoch 50: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6810 - accuracy: 0.5497 - recall_3: 0.6023 - val_loss: 0.6944 - val_accuracy: 0.5163 - val_recall_3: 0.6885\n",
      "Epoch 51/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.5502 - recall_3: 0.5943\n",
      "Epoch 51: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6823 - accuracy: 0.5499 - recall_3: 0.5950 - val_loss: 0.6943 - val_accuracy: 0.5123 - val_recall_3: 0.6934\n",
      "Epoch 52/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6818 - accuracy: 0.5550 - recall_3: 0.6005\n",
      "Epoch 52: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6813 - accuracy: 0.5550 - recall_3: 0.6005 - val_loss: 0.6805 - val_accuracy: 0.5564 - val_recall_3: 0.6426\n",
      "Epoch 53/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6825 - accuracy: 0.5525 - recall_3: 0.5885\n",
      "Epoch 53: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6822 - accuracy: 0.5527 - recall_3: 0.5887 - val_loss: 0.6757 - val_accuracy: 0.5734 - val_recall_3: 0.6238\n",
      "Epoch 54/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6805 - accuracy: 0.5524 - recall_3: 0.6096\n",
      "Epoch 54: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6804 - accuracy: 0.5526 - recall_3: 0.6097 - val_loss: 0.6650 - val_accuracy: 0.5950 - val_recall_3: 0.5844\n",
      "Epoch 55/300\n",
      "811/822 [============================>.] - ETA: 0s - loss: 0.6824 - accuracy: 0.5488 - recall_3: 0.5987\n",
      "Epoch 55: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6825 - accuracy: 0.5493 - recall_3: 0.5956 - val_loss: 0.6561 - val_accuracy: 0.6223 - val_recall_3: 0.5377\n",
      "Epoch 56/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6817 - accuracy: 0.5468 - recall_3: 0.5931\n",
      "Epoch 56: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6817 - accuracy: 0.5467 - recall_3: 0.5929 - val_loss: 0.6706 - val_accuracy: 0.5782 - val_recall_3: 0.6057\n",
      "Epoch 57/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6812 - accuracy: 0.5475 - recall_3: 0.6007\n",
      "Epoch 57: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6814 - accuracy: 0.5479 - recall_3: 0.6011 - val_loss: 0.6584 - val_accuracy: 0.6141 - val_recall_3: 0.5549\n",
      "Epoch 58/300\n",
      "812/822 [============================>.] - ETA: 0s - loss: 0.6801 - accuracy: 0.5569 - recall_3: 0.5966\n",
      "Epoch 58: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6805 - accuracy: 0.5566 - recall_3: 0.5960 - val_loss: 0.6870 - val_accuracy: 0.5353 - val_recall_3: 0.6607\n",
      "Epoch 59/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6795 - accuracy: 0.5506 - recall_3: 0.6067\n",
      "Epoch 59: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6798 - accuracy: 0.5505 - recall_3: 0.6074 - val_loss: 0.6879 - val_accuracy: 0.5284 - val_recall_3: 0.6656\n",
      "Epoch 60/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6803 - accuracy: 0.5544 - recall_3: 0.6019\n",
      "Epoch 60: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6810 - accuracy: 0.5540 - recall_3: 0.6005 - val_loss: 0.6865 - val_accuracy: 0.5388 - val_recall_3: 0.6533\n",
      "Epoch 61/300\n",
      "805/822 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.5510 - recall_3: 0.6015\n",
      "Epoch 61: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6811 - accuracy: 0.5489 - recall_3: 0.6019 - val_loss: 0.6938 - val_accuracy: 0.5175 - val_recall_3: 0.6828\n",
      "Epoch 62/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.6808 - accuracy: 0.5499 - recall_3: 0.5943\n",
      "Epoch 62: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6812 - accuracy: 0.5501 - recall_3: 0.5940 - val_loss: 0.6762 - val_accuracy: 0.5681 - val_recall_3: 0.6238\n",
      "Epoch 63/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6824 - accuracy: 0.5491 - recall_3: 0.5929\n",
      "Epoch 63: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6820 - accuracy: 0.5491 - recall_3: 0.5929 - val_loss: 0.6752 - val_accuracy: 0.5709 - val_recall_3: 0.6172\n",
      "Epoch 64/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6810 - accuracy: 0.5526 - recall_3: 0.5919\n",
      "Epoch 64: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6810 - accuracy: 0.5526 - recall_3: 0.5919 - val_loss: 0.6647 - val_accuracy: 0.5978 - val_recall_3: 0.5730\n",
      "Epoch 65/300\n",
      "805/822 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.5467 - recall_3: 0.6032\n",
      "Epoch 65: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6817 - accuracy: 0.5473 - recall_3: 0.6033 - val_loss: 0.6692 - val_accuracy: 0.5832 - val_recall_3: 0.6057\n",
      "Epoch 66/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.5521 - recall_3: 0.5893\n",
      "Epoch 66: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6806 - accuracy: 0.5520 - recall_3: 0.5915 - val_loss: 0.6920 - val_accuracy: 0.5237 - val_recall_3: 0.6844\n",
      "Epoch 67/300\n",
      "811/822 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5514 - recall_3: 0.6055\n",
      "Epoch 67: val_loss did not improve from 0.64614\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6805 - accuracy: 0.5516 - recall_3: 0.6050 - val_loss: 0.6710 - val_accuracy: 0.5783 - val_recall_3: 0.6123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x171ecb95670>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(32, input_shape=(timesteps, input_dim),\n",
    "                    kernel_regularizer=regularizers.l2(0.01),\n",
    "                    recurrent_regularizer=regularizers.l2(0.01),\n",
    "                    bias_regularizer=regularizers.l2(0.01),\n",
    "                     dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', Recall()])\n",
    "\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "checkpoint = ModelCheckpoint('model/best_model_hybrid_rnn.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 1ms/step\n",
      "      Prediction  Actual\n",
      "0       0.508888       1\n",
      "1       0.475920       0\n",
      "2       0.505503       0\n",
      "3       0.337288       0\n",
      "4       0.408157       0\n",
      "...          ...     ...\n",
      "8215    0.300940       0\n",
      "8216    0.434482       1\n",
      "8217    0.405841       0\n",
      "8218    0.530091       0\n",
      "8219    0.420934       0\n",
      "\n",
      "[8220 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "best_model = load_model('model/best_model_hybrid_rnn.h5')\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Compare the predictions with the actual values\n",
    "comparison = pd.DataFrame({'Prediction': predictions.flatten(), 'Actual': y_test})\n",
    "\n",
    "# Print the comparison\n",
    "print(comparison)\n",
    "predictions_labels = [1 if p > 0.5 else 0 for p in predictions.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_labels)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.68      0.75      6622\n",
      "           1       0.26      0.46      0.33      1598\n",
      "\n",
      "    accuracy                           0.64      8220\n",
      "   macro avg       0.55      0.57      0.54      8220\n",
      "weighted avg       0.73      0.64      0.67      8220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, predictions_labels, output_dict=True)\n",
    "print(classification_report(y_test, predictions_labels))\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv('result/hybrid_rnn.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
