{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c38f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a63ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = pd.read_csv('data/data_encoded_scaled.csv')\n",
    "data_cleaned = pd.read_csv('data/data_cleaned.csv')\n",
    "data_feature = data_cleaned.drop(columns=['TOTAL_DELAY', 'DEP_DEL15'])\n",
    "data_target = data_cleaned['DEP_DEL15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd8c93f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = data_encoded_scaled.drop(columns=['RESIDUALS', 'DEP_DEL15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887cb4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.968432919954903\n"
     ]
    }
   ],
   "source": [
    "sequence_days = 7\n",
    "daily_counts = data_feature.groupby(['MONTH', 'DAY_OF_MONTH', 'DEPARTING_AIRPORT']).size()\n",
    "average_rows = daily_counts.mean()\n",
    "\n",
    "print(average_rows*sequence_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a73f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_encoded_scaled.assign(TARGET=data_target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17149eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MONTH  DAY_OF_MONTH  DAY_OF_WEEK  DEP_TIME_BLK  DISTANCE_GROUP  \\\n",
      "0    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "1    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "2    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "3    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "4    0.0           0.0     0.166667      0.166667        0.285714   \n",
      "\n",
      "   SEGMENT_NUMBER  CONCURRENT_FLIGHTS  NUMBER_OF_SEATS  AIRPORT_FLIGHTS_MONTH  \\\n",
      "0           0.000            0.098592              1.0               0.115453   \n",
      "1           0.000            0.098592              1.0               0.115453   \n",
      "2           0.000            0.112676              1.0               0.111384   \n",
      "3           0.000            0.267606              1.0               0.333661   \n",
      "4           0.125            0.042254              1.0               0.028528   \n",
      "\n",
      "   AIRLINE_FLIGHTS_MONTH  ...  PREVIOUS_AIRPORT_3  PREVIOUS_AIRPORT_4  \\\n",
      "0               0.183515  ...                 0.0                 0.0   \n",
      "1               0.183515  ...                 0.0                 0.0   \n",
      "2               0.183515  ...                 0.0                 0.0   \n",
      "3               0.183515  ...                 0.0                 0.0   \n",
      "4               0.183515  ...                 0.0                 0.0   \n",
      "\n",
      "   PREVIOUS_AIRPORT_5  PREVIOUS_AIRPORT_6  PRCP  SNOW  SNWD      TMAX  \\\n",
      "0                 0.0                 1.0   0.0   0.0   0.0  0.353982   \n",
      "1                 0.0                 1.0   0.0   0.0   0.0  0.353982   \n",
      "2                 0.0                 1.0   0.0   0.0   0.0  0.451327   \n",
      "3                 0.0                 1.0   0.0   0.0   0.0  0.699115   \n",
      "4                 1.0                 0.0   0.0   0.0   0.0  0.460177   \n",
      "\n",
      "       AWND  TARGET  \n",
      "0  0.198638       0  \n",
      "1  0.198638       0  \n",
      "2  0.172291       0  \n",
      "3  0.370930       0  \n",
      "4  0.178804       0  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e048c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences shape: (41100, 28, 32)\n",
      "Target values shape: (41100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequence_length = int(round(average_rows*sequence_days))\n",
    "\n",
    "unique_dep_airport = data_feature['DEPARTING_AIRPORT'].unique()\n",
    "unique_flight_number = data_feature['FLIGHT_NUMBER'].unique()\n",
    "\n",
    "X_sequences = []\n",
    "y_targets = []\n",
    "\n",
    "for dep_airport in unique_dep_airport:\n",
    "    flight_data = data_full[data_feature['DEPARTING_AIRPORT'] == dep_airport]\n",
    "    flight_data_values = flight_data.iloc[:, :-1].values\n",
    "    flight_data_target = flight_data.iloc[:, -1].values\n",
    "    for i in range(len(flight_data) - sequence_length):\n",
    "        X_sequences.append(flight_data_values[i:i+sequence_length])\n",
    "        y_targets.append(flight_data_target[i+sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_targets = np.array(y_targets)\n",
    "\n",
    "print(\"Input sequences shape:\", X_sequences.shape)\n",
    "print(\"Target values shape:\", y_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e5c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = list(zip(X_sequences, y_targets))\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Separate the sequences and targets\n",
    "X_train, y_train = zip(*train_data)\n",
    "X_test, y_test = zip(*test_data)\n",
    "\n",
    "# Convert the results back to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719e17d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "timesteps = X_train.shape[1]\n",
    "input_dim = X_train.shape[2]\n",
    "\n",
    "weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train) , y = y_train)\n",
    "class_weights = dict(zip(np.unique(y_train), weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28047b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "814/822 [============================>.] - ETA: 0s - loss: 1.5108 - accuracy: 0.4895 - recall: 0.6172\n",
      "Epoch 1: val_loss improved from inf to 1.26964, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 7s 7ms/step - loss: 1.5084 - accuracy: 0.4895 - recall: 0.6170 - val_loss: 1.2696 - val_accuracy: 0.4880 - val_recall: 0.6544\n",
      "Epoch 2/300\n",
      " 27/822 [..............................] - ETA: 4s - loss: 1.2520 - accuracy: 0.5208 - recall: 0.5912"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/822 [============================>.] - ETA: 0s - loss: 1.1335 - accuracy: 0.5259 - recall: 0.6177\n",
      "Epoch 2: val_loss improved from 1.26964 to 1.04065, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 9s 11ms/step - loss: 1.1334 - accuracy: 0.5261 - recall: 0.6176 - val_loss: 1.0407 - val_accuracy: 0.5128 - val_recall: 0.6460\n",
      "Epoch 3/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.9787 - accuracy: 0.5344 - recall: 0.6305\n",
      "Epoch 3: val_loss improved from 1.04065 to 0.94853, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 10s 12ms/step - loss: 0.9787 - accuracy: 0.5343 - recall: 0.6307 - val_loss: 0.9485 - val_accuracy: 0.4927 - val_recall: 0.6845\n",
      "Epoch 4/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.9022 - accuracy: 0.5301 - recall: 0.6420\n",
      "Epoch 4: val_loss improved from 0.94853 to 0.88326, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 8s 10ms/step - loss: 0.9022 - accuracy: 0.5300 - recall: 0.6421 - val_loss: 0.8833 - val_accuracy: 0.5141 - val_recall: 0.6552\n",
      "Epoch 5/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.8529 - accuracy: 0.5387 - recall: 0.6238\n",
      "Epoch 5: val_loss improved from 0.88326 to 0.84047, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.8527 - accuracy: 0.5387 - recall: 0.6239 - val_loss: 0.8405 - val_accuracy: 0.5163 - val_recall: 0.6502\n",
      "Epoch 6/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.8162 - accuracy: 0.5305 - recall: 0.6402\n",
      "Epoch 6: val_loss improved from 0.84047 to 0.79428, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.8162 - accuracy: 0.5309 - recall: 0.6405 - val_loss: 0.7943 - val_accuracy: 0.5590 - val_recall: 0.5941\n",
      "Epoch 7/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.7857 - accuracy: 0.5366 - recall: 0.6330\n",
      "Epoch 7: val_loss improved from 0.79428 to 0.76900, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 8s 10ms/step - loss: 0.7857 - accuracy: 0.5366 - recall: 0.6330 - val_loss: 0.7690 - val_accuracy: 0.5502 - val_recall: 0.6092\n",
      "Epoch 8/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7612 - accuracy: 0.5392 - recall: 0.6299\n",
      "Epoch 8: val_loss improved from 0.76900 to 0.76514, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.7606 - accuracy: 0.5393 - recall: 0.6303 - val_loss: 0.7651 - val_accuracy: 0.4965 - val_recall: 0.6770\n",
      "Epoch 9/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.7411 - accuracy: 0.5339 - recall: 0.6273\n",
      "Epoch 9: val_loss improved from 0.76514 to 0.74722, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.7411 - accuracy: 0.5339 - recall: 0.6273 - val_loss: 0.7472 - val_accuracy: 0.4951 - val_recall: 0.6778\n",
      "Epoch 10/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7257 - accuracy: 0.5346 - recall: 0.6353\n",
      "Epoch 10: val_loss improved from 0.74722 to 0.72395, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.7258 - accuracy: 0.5346 - recall: 0.6350 - val_loss: 0.7239 - val_accuracy: 0.5190 - val_recall: 0.6494\n",
      "Epoch 11/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7128 - accuracy: 0.5253 - recall: 0.6282\n",
      "Epoch 11: val_loss improved from 0.72395 to 0.70248, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.7130 - accuracy: 0.5253 - recall: 0.6283 - val_loss: 0.7025 - val_accuracy: 0.5529 - val_recall: 0.6025\n",
      "Epoch 12/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7025 - accuracy: 0.5343 - recall: 0.6277\n",
      "Epoch 12: val_loss improved from 0.70248 to 0.68793, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.7027 - accuracy: 0.5346 - recall: 0.6279 - val_loss: 0.6879 - val_accuracy: 0.5706 - val_recall: 0.5699\n",
      "Epoch 13/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6962 - accuracy: 0.5288 - recall: 0.6338\n",
      "Epoch 13: val_loss improved from 0.68793 to 0.67916, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6966 - accuracy: 0.5290 - recall: 0.6330 - val_loss: 0.6792 - val_accuracy: 0.5773 - val_recall: 0.5615\n",
      "Epoch 14/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.5336 - recall: 0.6173\n",
      "Epoch 14: val_loss did not improve from 0.67916\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6915 - accuracy: 0.5334 - recall: 0.6172 - val_loss: 0.6837 - val_accuracy: 0.5579 - val_recall: 0.5941\n",
      "Epoch 15/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6866 - accuracy: 0.5309 - recall: 0.6396\n",
      "Epoch 15: val_loss improved from 0.67916 to 0.67142, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6875 - accuracy: 0.5308 - recall: 0.6382 - val_loss: 0.6714 - val_accuracy: 0.5867 - val_recall: 0.5490\n",
      "Epoch 16/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6855 - accuracy: 0.5397 - recall: 0.6301\n",
      "Epoch 16: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6856 - accuracy: 0.5397 - recall: 0.6305 - val_loss: 0.6848 - val_accuracy: 0.5423 - val_recall: 0.6109\n",
      "Epoch 17/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6854 - accuracy: 0.5394 - recall: 0.6249\n",
      "Epoch 17: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 9s 11ms/step - loss: 0.6854 - accuracy: 0.5395 - recall: 0.6249 - val_loss: 0.6916 - val_accuracy: 0.5186 - val_recall: 0.6502\n",
      "Epoch 18/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6854 - accuracy: 0.5342 - recall: 0.6332\n",
      "Epoch 18: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6850 - accuracy: 0.5335 - recall: 0.6328 - val_loss: 0.6971 - val_accuracy: 0.5035 - val_recall: 0.6686\n",
      "Epoch 19/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6853 - accuracy: 0.5397 - recall: 0.6174\n",
      "Epoch 19: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 6s 7ms/step - loss: 0.6851 - accuracy: 0.5401 - recall: 0.6178 - val_loss: 0.6864 - val_accuracy: 0.5312 - val_recall: 0.6276\n",
      "Epoch 20/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6848 - accuracy: 0.5382 - recall: 0.6175\n",
      "Epoch 20: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 6s 7ms/step - loss: 0.6845 - accuracy: 0.5381 - recall: 0.6176 - val_loss: 0.6855 - val_accuracy: 0.5327 - val_recall: 0.6259\n",
      "Epoch 21/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6847 - accuracy: 0.5333 - recall: 0.6265\n",
      "Epoch 21: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6844 - accuracy: 0.5332 - recall: 0.6265 - val_loss: 0.6883 - val_accuracy: 0.5219 - val_recall: 0.6402\n",
      "Epoch 22/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6843 - accuracy: 0.5394 - recall: 0.6236\n",
      "Epoch 22: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6838 - accuracy: 0.5397 - recall: 0.6239 - val_loss: 0.6756 - val_accuracy: 0.5598 - val_recall: 0.5874\n",
      "Epoch 23/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6835 - accuracy: 0.5379 - recall: 0.6298\n",
      "Epoch 23: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6832 - accuracy: 0.5378 - recall: 0.6299 - val_loss: 0.6881 - val_accuracy: 0.5265 - val_recall: 0.6368\n",
      "Epoch 24/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6829 - accuracy: 0.5416 - recall: 0.6215\n",
      "Epoch 24: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6831 - accuracy: 0.5415 - recall: 0.6218 - val_loss: 0.6905 - val_accuracy: 0.5196 - val_recall: 0.6519\n",
      "Epoch 25/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6830 - accuracy: 0.5370 - recall: 0.6304\n",
      "Epoch 25: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 6s 7ms/step - loss: 0.6828 - accuracy: 0.5368 - recall: 0.6303 - val_loss: 0.6845 - val_accuracy: 0.5353 - val_recall: 0.6251\n",
      "Epoch 26/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6832 - accuracy: 0.5443 - recall: 0.6145\n",
      "Epoch 26: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6831 - accuracy: 0.5440 - recall: 0.6149 - val_loss: 0.7010 - val_accuracy: 0.4856 - val_recall: 0.6979\n",
      "Epoch 27/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.5424 - recall: 0.6285\n",
      "Epoch 27: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6816 - accuracy: 0.5424 - recall: 0.6287 - val_loss: 0.6922 - val_accuracy: 0.5176 - val_recall: 0.6502\n",
      "Epoch 28/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6820 - accuracy: 0.5385 - recall: 0.6257\n",
      "Epoch 28: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6820 - accuracy: 0.5385 - recall: 0.6257 - val_loss: 0.6869 - val_accuracy: 0.5284 - val_recall: 0.6310\n",
      "Epoch 29/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6819 - accuracy: 0.5332 - recall: 0.6191\n",
      "Epoch 29: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6820 - accuracy: 0.5332 - recall: 0.6192 - val_loss: 0.6817 - val_accuracy: 0.5432 - val_recall: 0.6126\n",
      "Epoch 30/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6811 - accuracy: 0.5453 - recall: 0.6142\n",
      "Epoch 30: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.6817 - accuracy: 0.5456 - recall: 0.6147 - val_loss: 0.6758 - val_accuracy: 0.5622 - val_recall: 0.5824\n",
      "Epoch 31/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6822 - accuracy: 0.5444 - recall: 0.6165\n",
      "Epoch 31: val_loss did not improve from 0.67142\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.6825 - accuracy: 0.5444 - recall: 0.6168 - val_loss: 0.6966 - val_accuracy: 0.5018 - val_recall: 0.6803\n",
      "Epoch 32/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.5433 - recall: 0.6295\n",
      "Epoch 32: val_loss improved from 0.67142 to 0.67086, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6816 - accuracy: 0.5432 - recall: 0.6289 - val_loss: 0.6709 - val_accuracy: 0.5748 - val_recall: 0.5615\n",
      "Epoch 33/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6823 - accuracy: 0.5437 - recall: 0.6154\n",
      "Epoch 33: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6821 - accuracy: 0.5438 - recall: 0.6152 - val_loss: 0.6873 - val_accuracy: 0.5259 - val_recall: 0.6368\n",
      "Epoch 34/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6825 - accuracy: 0.5388 - recall: 0.6243\n",
      "Epoch 34: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6824 - accuracy: 0.5387 - recall: 0.6247 - val_loss: 0.6846 - val_accuracy: 0.5322 - val_recall: 0.6226\n",
      "Epoch 35/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6815 - accuracy: 0.5403 - recall: 0.6194\n",
      "Epoch 35: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6815 - accuracy: 0.5403 - recall: 0.6194 - val_loss: 0.6730 - val_accuracy: 0.5649 - val_recall: 0.5782\n",
      "Epoch 36/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6805 - accuracy: 0.5439 - recall: 0.6135\n",
      "Epoch 36: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6810 - accuracy: 0.5435 - recall: 0.6139 - val_loss: 0.6731 - val_accuracy: 0.5648 - val_recall: 0.5824\n",
      "Epoch 37/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6825 - accuracy: 0.5435 - recall: 0.6091\n",
      "Epoch 37: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6824 - accuracy: 0.5434 - recall: 0.6089 - val_loss: 0.6951 - val_accuracy: 0.5032 - val_recall: 0.6720\n",
      "Epoch 38/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6819 - accuracy: 0.5428 - recall: 0.6181\n",
      "Epoch 38: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6818 - accuracy: 0.5426 - recall: 0.6186 - val_loss: 0.7019 - val_accuracy: 0.4802 - val_recall: 0.7038\n",
      "Epoch 39/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6823 - accuracy: 0.5362 - recall: 0.6184\n",
      "Epoch 39: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6823 - accuracy: 0.5362 - recall: 0.6184 - val_loss: 0.6994 - val_accuracy: 0.4919 - val_recall: 0.6904\n",
      "Epoch 40/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6827 - accuracy: 0.5389 - recall: 0.6141\n",
      "Epoch 40: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6827 - accuracy: 0.5389 - recall: 0.6141 - val_loss: 0.6917 - val_accuracy: 0.5108 - val_recall: 0.6552\n",
      "Epoch 41/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.5386 - recall: 0.6214\n",
      "Epoch 41: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6814 - accuracy: 0.5387 - recall: 0.6208 - val_loss: 0.6827 - val_accuracy: 0.5377 - val_recall: 0.6218\n",
      "Epoch 42/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6816 - accuracy: 0.5392 - recall: 0.6083\n",
      "Epoch 42: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6816 - accuracy: 0.5392 - recall: 0.6083 - val_loss: 0.6727 - val_accuracy: 0.5648 - val_recall: 0.5766\n",
      "Epoch 43/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6806 - accuracy: 0.5410 - recall: 0.6165\n",
      "Epoch 43: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6806 - accuracy: 0.5409 - recall: 0.6168 - val_loss: 0.6863 - val_accuracy: 0.5262 - val_recall: 0.6393\n",
      "Epoch 44/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6818 - accuracy: 0.5362 - recall: 0.6134\n",
      "Epoch 44: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6817 - accuracy: 0.5363 - recall: 0.6141 - val_loss: 0.6797 - val_accuracy: 0.5432 - val_recall: 0.6075\n",
      "Epoch 45/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6807 - accuracy: 0.5477 - recall: 0.6082\n",
      "Epoch 45: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6807 - accuracy: 0.5475 - recall: 0.6085 - val_loss: 0.7046 - val_accuracy: 0.4766 - val_recall: 0.7096\n",
      "Epoch 46/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6811 - accuracy: 0.5375 - recall: 0.6149\n",
      "Epoch 46: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6811 - accuracy: 0.5375 - recall: 0.6149 - val_loss: 0.6984 - val_accuracy: 0.4957 - val_recall: 0.6895\n",
      "Epoch 47/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6796 - accuracy: 0.5404 - recall: 0.6266\n",
      "Epoch 47: val_loss did not improve from 0.67086\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6796 - accuracy: 0.5405 - recall: 0.6269 - val_loss: 0.6943 - val_accuracy: 0.5073 - val_recall: 0.6703\n",
      "Epoch 48/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6813 - accuracy: 0.5374 - recall: 0.6307\n",
      "Epoch 48: val_loss improved from 0.67086 to 0.65308, saving model to model\\best_model_lstm.h5\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6813 - accuracy: 0.5374 - recall: 0.6307 - val_loss: 0.6531 - val_accuracy: 0.6236 - val_recall: 0.4962\n",
      "Epoch 49/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6811 - accuracy: 0.5473 - recall: 0.6033\n",
      "Epoch 49: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6810 - accuracy: 0.5475 - recall: 0.6036 - val_loss: 0.6884 - val_accuracy: 0.5216 - val_recall: 0.6444\n",
      "Epoch 50/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6807 - accuracy: 0.5411 - recall: 0.6198\n",
      "Epoch 50: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6808 - accuracy: 0.5407 - recall: 0.6194 - val_loss: 0.6887 - val_accuracy: 0.5204 - val_recall: 0.6460\n",
      "Epoch 51/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6808 - accuracy: 0.5385 - recall: 0.6206\n",
      "Epoch 51: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6808 - accuracy: 0.5385 - recall: 0.6210 - val_loss: 0.6882 - val_accuracy: 0.5205 - val_recall: 0.6460\n",
      "Epoch 52/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6812 - accuracy: 0.5405 - recall: 0.6153\n",
      "Epoch 52: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6815 - accuracy: 0.5404 - recall: 0.6147 - val_loss: 0.6862 - val_accuracy: 0.5265 - val_recall: 0.6310\n",
      "Epoch 53/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.5421 - recall: 0.6192\n",
      "Epoch 53: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6817 - accuracy: 0.5419 - recall: 0.6188 - val_loss: 0.6862 - val_accuracy: 0.5292 - val_recall: 0.6368\n",
      "Epoch 54/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6806 - accuracy: 0.5427 - recall: 0.6259\n",
      "Epoch 54: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6806 - accuracy: 0.5428 - recall: 0.6263 - val_loss: 0.6872 - val_accuracy: 0.5240 - val_recall: 0.6301\n",
      "Epoch 55/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6817 - accuracy: 0.5383 - recall: 0.6150\n",
      "Epoch 55: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6816 - accuracy: 0.5383 - recall: 0.6154 - val_loss: 0.6737 - val_accuracy: 0.5649 - val_recall: 0.5874\n",
      "Epoch 56/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6812 - accuracy: 0.5451 - recall: 0.6058\n",
      "Epoch 56: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6811 - accuracy: 0.5450 - recall: 0.6062 - val_loss: 0.7058 - val_accuracy: 0.4725 - val_recall: 0.7138\n",
      "Epoch 57/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6812 - accuracy: 0.5370 - recall: 0.6233\n",
      "Epoch 57: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6815 - accuracy: 0.5373 - recall: 0.6231 - val_loss: 0.6732 - val_accuracy: 0.5701 - val_recall: 0.5841\n",
      "Epoch 58/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6811 - accuracy: 0.5416 - recall: 0.6176\n",
      "Epoch 58: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6811 - accuracy: 0.5412 - recall: 0.6176 - val_loss: 0.6881 - val_accuracy: 0.5198 - val_recall: 0.6435\n",
      "Epoch 59/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6824 - accuracy: 0.5343 - recall: 0.6168\n",
      "Epoch 59: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6824 - accuracy: 0.5343 - recall: 0.6168 - val_loss: 0.6769 - val_accuracy: 0.5500 - val_recall: 0.5967\n",
      "Epoch 60/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6800 - accuracy: 0.5492 - recall: 0.6070\n",
      "Epoch 60: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 10s 12ms/step - loss: 0.6800 - accuracy: 0.5492 - recall: 0.6070 - val_loss: 0.7008 - val_accuracy: 0.4846 - val_recall: 0.6895\n",
      "Epoch 61/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6815 - accuracy: 0.5387 - recall: 0.6161\n",
      "Epoch 61: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 9s 11ms/step - loss: 0.6814 - accuracy: 0.5390 - recall: 0.6162 - val_loss: 0.6776 - val_accuracy: 0.5473 - val_recall: 0.6033\n",
      "Epoch 62/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6805 - accuracy: 0.5385 - recall: 0.6228\n",
      "Epoch 62: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 9s 11ms/step - loss: 0.6807 - accuracy: 0.5385 - recall: 0.6226 - val_loss: 0.6856 - val_accuracy: 0.5271 - val_recall: 0.6259\n",
      "Epoch 63/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6809 - accuracy: 0.5420 - recall: 0.6233\n",
      "Epoch 63: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 9s 11ms/step - loss: 0.6809 - accuracy: 0.5420 - recall: 0.6233 - val_loss: 0.6717 - val_accuracy: 0.5675 - val_recall: 0.5808\n",
      "Epoch 64/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6807 - accuracy: 0.5422 - recall: 0.6130\n",
      "Epoch 64: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 10s 12ms/step - loss: 0.6809 - accuracy: 0.5419 - recall: 0.6127 - val_loss: 0.6935 - val_accuracy: 0.5088 - val_recall: 0.6653\n",
      "Epoch 65/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6806 - accuracy: 0.5541 - recall: 0.6100\n",
      "Epoch 65: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.6805 - accuracy: 0.5535 - recall: 0.6103 - val_loss: 0.7245 - val_accuracy: 0.4217 - val_recall: 0.7799\n",
      "Epoch 66/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.5361 - recall: 0.6247\n",
      "Epoch 66: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 10s 12ms/step - loss: 0.6802 - accuracy: 0.5365 - recall: 0.6247 - val_loss: 0.6787 - val_accuracy: 0.5508 - val_recall: 0.5992\n",
      "Epoch 67/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6803 - accuracy: 0.5420 - recall: 0.6190\n",
      "Epoch 67: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 10s 12ms/step - loss: 0.6803 - accuracy: 0.5418 - recall: 0.6190 - val_loss: 0.6638 - val_accuracy: 0.5850 - val_recall: 0.5531\n",
      "Epoch 68/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6807 - accuracy: 0.5519 - recall: 0.6039\n",
      "Epoch 68: val_loss did not improve from 0.65308\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.6806 - accuracy: 0.5516 - recall: 0.6046 - val_loss: 0.7120 - val_accuracy: 0.4538 - val_recall: 0.7347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22d86af3c70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras import regularizers, optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import Recall\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(32, input_shape=(timesteps, input_dim),\n",
    "               kernel_regularizer=regularizers.l2(0.01),\n",
    "               recurrent_regularizer=regularizers.l2(0.01),\n",
    "               bias_regularizer=regularizers.l2(0.01),\n",
    "               dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', Recall()])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "checkpoint = ModelCheckpoint('model/best_model_lstm.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7cb3885",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 2s 5ms/step\n",
      "      Prediction  Actual\n",
      "0       0.414086       0\n",
      "1       0.508610       0\n",
      "2       0.472553       0\n",
      "3       0.405821       0\n",
      "4       0.457533       0\n",
      "...          ...     ...\n",
      "8215    0.443863       0\n",
      "8216    0.423309       0\n",
      "8217    0.466310       0\n",
      "8218    0.607402       0\n",
      "8219    0.529137       1\n",
      "\n",
      "[8220 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "best_model = load_model('model/best_model_lstm.h5')\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Compare the predictions with the actual values\n",
    "comparison = pd.DataFrame({'Prediction': predictions.flatten(), 'Actual': y_test})\n",
    "\n",
    "# Print the comparison\n",
    "print(comparison)\n",
    "predictions_labels = [1 if p > 0.5 else 0 for p in predictions.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dbb7924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_labels)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "043c253f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.64      0.73      6633\n",
      "           1       0.26      0.51      0.34      1587\n",
      "\n",
      "    accuracy                           0.62      8220\n",
      "   macro avg       0.55      0.58      0.54      8220\n",
      "weighted avg       0.73      0.62      0.65      8220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, predictions_labels, output_dict=True)\n",
    "print(classification_report(y_test, predictions_labels))\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv('result/lstm.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
