{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = pd.read_csv('data/data_encoded_scaled.csv')\n",
    "data_cleaned = pd.read_csv('data/data_cleaned.csv')\n",
    "data_feature = data_cleaned.drop(columns=['TOTAL_DELAY', 'DEP_DEL15'])\n",
    "data_target = data_cleaned['DEP_DEL15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = data_encoded_scaled.drop(columns=['DEP_DEL15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.968432919954903\n"
     ]
    }
   ],
   "source": [
    "sequence_days = 7\n",
    "daily_counts = data_feature.groupby(['MONTH', 'DAY_OF_MONTH', 'DEPARTING_AIRPORT']).size() # 一天一个机场的航班数\n",
    "average_rows = daily_counts.mean()\n",
    "\n",
    "print(average_rows*sequence_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_encoded_scaled.assign(TARGET=data_target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MONTH  DAY_OF_MONTH  DAY_OF_WEEK  DEP_TIME_BLK  DISTANCE_GROUP  \\\n",
      "0    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "1    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "2    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "3    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "4    0.0           0.0     0.166667      0.166667        0.285714   \n",
      "\n",
      "   SEGMENT_NUMBER  CONCURRENT_FLIGHTS  NUMBER_OF_SEATS  AIRPORT_FLIGHTS_MONTH  \\\n",
      "0           0.000            0.098592              1.0               0.115453   \n",
      "1           0.000            0.098592              1.0               0.115453   \n",
      "2           0.000            0.112676              1.0               0.111384   \n",
      "3           0.000            0.267606              1.0               0.333661   \n",
      "4           0.125            0.042254              1.0               0.028528   \n",
      "\n",
      "   AIRLINE_FLIGHTS_MONTH  ...  PREVIOUS_AIRPORT_4  PREVIOUS_AIRPORT_5  \\\n",
      "0               0.183515  ...                 0.0                 0.0   \n",
      "1               0.183515  ...                 0.0                 0.0   \n",
      "2               0.183515  ...                 0.0                 0.0   \n",
      "3               0.183515  ...                 0.0                 0.0   \n",
      "4               0.183515  ...                 0.0                 1.0   \n",
      "\n",
      "   PREVIOUS_AIRPORT_6  PRCP  SNOW  SNWD      TMAX      AWND  RESIDUALS  TARGET  \n",
      "0                 1.0   0.0   0.0   0.0  0.353982  0.198638   0.052138       0  \n",
      "1                 1.0   0.0   0.0   0.0  0.353982  0.198638   0.052775       0  \n",
      "2                 1.0   0.0   0.0   0.0  0.451327  0.172291   0.051816       0  \n",
      "3                 1.0   0.0   0.0   0.0  0.699115  0.370930   0.051928       0  \n",
      "4                 0.0   0.0   0.0   0.0  0.460177  0.178804   0.051987       0  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences shape: (41100, 28, 33)\n",
      "Target values shape: (41100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequence_length = int(round(average_rows*sequence_days))\n",
    "\n",
    "unique_dep_airport = data_feature['DEPARTING_AIRPORT'].unique()\n",
    "unique_flight_number = data_feature['FLIGHT_NUMBER'].unique()\n",
    "\n",
    "X_sequences = []\n",
    "y_targets = []\n",
    "\n",
    "for dep_airport in unique_dep_airport:\n",
    "    flight_data = data_full[data_feature['DEPARTING_AIRPORT'] == dep_airport]\n",
    "    flight_data_values = flight_data.iloc[:, :-1].values\n",
    "    flight_data_target = flight_data.iloc[:, -1].values\n",
    "    for i in range(len(flight_data) - sequence_length):\n",
    "        X_sequences.append(flight_data_values[i:i+sequence_length])\n",
    "        y_targets.append(flight_data_target[i+sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_targets = np.array(y_targets)\n",
    "\n",
    "print(\"Input sequences shape:\", X_sequences.shape)\n",
    "print(\"Target values shape:\", y_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = list(zip(X_sequences, y_targets))\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Separate the sequences and targets\n",
    "X_train, y_train = zip(*train_data)\n",
    "X_test, y_test = zip(*test_data)\n",
    "\n",
    "# Convert the results back to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "timesteps = X_train.shape[1]\n",
    "input_dim = X_train.shape[2]\n",
    "\n",
    "weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train) , y = y_train)\n",
    "class_weights = dict(zip(np.unique(y_train), weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 1.2330 - accuracy: 0.5004 - recall_4: 0.4710\n",
      "Epoch 1: val_loss improved from inf to 1.02191, saving model to model\\best_model_hybrid_GRUs.h5\n",
      "822/822 [==============================] - 6s 6ms/step - loss: 1.2330 - accuracy: 0.5004 - recall_4: 0.4710 - val_loss: 1.0219 - val_accuracy: 0.5214 - val_recall_4: 0.5492\n",
      "Epoch 2/300\n",
      " 28/822 [>.............................] - ETA: 4s - loss: 1.0151 - accuracy: 0.5134 - recall_4: 0.5291"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822/822 [==============================] - ETA: 0s - loss: 0.9041 - accuracy: 0.5355 - recall_4: 0.5253\n",
      "Epoch 2: val_loss improved from 1.02191 to 0.82248, saving model to model\\best_model_hybrid_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.9041 - accuracy: 0.5355 - recall_4: 0.5253 - val_loss: 0.8225 - val_accuracy: 0.5493 - val_recall_4: 0.5935\n",
      "Epoch 3/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.7831 - accuracy: 0.5356 - recall_4: 0.5765\n",
      "Epoch 3: val_loss improved from 0.82248 to 0.75623, saving model to model\\best_model_hybrid_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.7830 - accuracy: 0.5359 - recall_4: 0.5774 - val_loss: 0.7562 - val_accuracy: 0.5272 - val_recall_4: 0.6548\n",
      "Epoch 4/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.7372 - accuracy: 0.5374 - recall_4: 0.6077\n",
      "Epoch 4: val_loss improved from 0.75623 to 0.72604, saving model to model\\best_model_hybrid_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.7370 - accuracy: 0.5375 - recall_4: 0.6077 - val_loss: 0.7260 - val_accuracy: 0.5446 - val_recall_4: 0.6476\n",
      "Epoch 5/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.7189 - accuracy: 0.5317 - recall_4: 0.6082\n",
      "Epoch 5: val_loss improved from 0.72604 to 0.69446, saving model to model\\best_model_hybrid_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.7191 - accuracy: 0.5323 - recall_4: 0.6073 - val_loss: 0.6945 - val_accuracy: 0.6084 - val_recall_4: 0.5444\n",
      "Epoch 6/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.7081 - accuracy: 0.5531 - recall_4: 0.5862\n",
      "Epoch 6: val_loss did not improve from 0.69446\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.7091 - accuracy: 0.5526 - recall_4: 0.5867 - val_loss: 0.7360 - val_accuracy: 0.4529 - val_recall_4: 0.7831\n",
      "Epoch 7/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.7030 - accuracy: 0.5258 - recall_4: 0.6302\n",
      "Epoch 7: val_loss improved from 0.69446 to 0.68568, saving model to model\\best_model_hybrid_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.7030 - accuracy: 0.5258 - recall_4: 0.6302 - val_loss: 0.6857 - val_accuracy: 0.5943 - val_recall_4: 0.5847\n",
      "Epoch 8/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6988 - accuracy: 0.5444 - recall_4: 0.6133\n",
      "Epoch 8: val_loss improved from 0.68568 to 0.68387, saving model to model\\best_model_hybrid_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6987 - accuracy: 0.5447 - recall_4: 0.6136 - val_loss: 0.6839 - val_accuracy: 0.5880 - val_recall_4: 0.5855\n",
      "Epoch 9/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6943 - accuracy: 0.5563 - recall_4: 0.6015\n",
      "Epoch 9: val_loss did not improve from 0.68387\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6945 - accuracy: 0.5549 - recall_4: 0.6017 - val_loss: 0.7127 - val_accuracy: 0.4887 - val_recall_4: 0.7282\n",
      "Epoch 10/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6925 - accuracy: 0.5441 - recall_4: 0.6191\n",
      "Epoch 10: val_loss did not improve from 0.68387\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6925 - accuracy: 0.5441 - recall_4: 0.6191 - val_loss: 0.6948 - val_accuracy: 0.5351 - val_recall_4: 0.6516\n",
      "Epoch 11/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6900 - accuracy: 0.5525 - recall_4: 0.6122\n",
      "Epoch 11: val_loss did not improve from 0.68387\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6900 - accuracy: 0.5525 - recall_4: 0.6122 - val_loss: 0.6951 - val_accuracy: 0.5306 - val_recall_4: 0.6597\n",
      "Epoch 12/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6895 - accuracy: 0.5441 - recall_4: 0.6190\n",
      "Epoch 12: val_loss did not improve from 0.68387\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6894 - accuracy: 0.5439 - recall_4: 0.6197 - val_loss: 0.6914 - val_accuracy: 0.5385 - val_recall_4: 0.6597\n",
      "Epoch 13/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6874 - accuracy: 0.5501 - recall_4: 0.6117\n",
      "Epoch 13: val_loss did not improve from 0.68387\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6877 - accuracy: 0.5500 - recall_4: 0.6116 - val_loss: 0.6872 - val_accuracy: 0.5482 - val_recall_4: 0.6435\n",
      "Epoch 14/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.5350 - recall_4: 0.6252\n",
      "Epoch 14: val_loss improved from 0.68387 to 0.67414, saving model to model\\best_model_hybrid_GRUs.h5\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6874 - accuracy: 0.5350 - recall_4: 0.6252 - val_loss: 0.6741 - val_accuracy: 0.5849 - val_recall_4: 0.5887\n",
      "Epoch 15/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6859 - accuracy: 0.5463 - recall_4: 0.6019\n",
      "Epoch 15: val_loss did not improve from 0.67414\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6859 - accuracy: 0.5463 - recall_4: 0.6019 - val_loss: 0.6814 - val_accuracy: 0.5619 - val_recall_4: 0.6306\n",
      "Epoch 16/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6851 - accuracy: 0.5438 - recall_4: 0.6035\n",
      "Epoch 16: val_loss did not improve from 0.67414\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6851 - accuracy: 0.5438 - recall_4: 0.6035 - val_loss: 0.6814 - val_accuracy: 0.5576 - val_recall_4: 0.6306\n",
      "Epoch 17/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6833 - accuracy: 0.5461 - recall_4: 0.6189\n",
      "Epoch 17: val_loss improved from 0.67414 to 0.67020, saving model to model\\best_model_hybrid_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6830 - accuracy: 0.5463 - recall_4: 0.6185 - val_loss: 0.6702 - val_accuracy: 0.5868 - val_recall_4: 0.5935\n",
      "Epoch 18/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6842 - accuracy: 0.5497 - recall_4: 0.6034\n",
      "Epoch 18: val_loss did not improve from 0.67020\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6842 - accuracy: 0.5498 - recall_4: 0.6041 - val_loss: 0.6810 - val_accuracy: 0.5585 - val_recall_4: 0.6298\n",
      "Epoch 19/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.5462 - recall_4: 0.6110\n",
      "Epoch 19: val_loss did not improve from 0.67020\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6825 - accuracy: 0.5458 - recall_4: 0.6094 - val_loss: 0.6760 - val_accuracy: 0.5654 - val_recall_4: 0.6161\n",
      "Epoch 20/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6829 - accuracy: 0.5381 - recall_4: 0.6152\n",
      "Epoch 20: val_loss improved from 0.67020 to 0.65417, saving model to model\\best_model_hybrid_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6833 - accuracy: 0.5387 - recall_4: 0.6144 - val_loss: 0.6542 - val_accuracy: 0.6311 - val_recall_4: 0.5081\n",
      "Epoch 21/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.5535 - recall_4: 0.6060\n",
      "Epoch 21: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6822 - accuracy: 0.5534 - recall_4: 0.6053 - val_loss: 0.6778 - val_accuracy: 0.5660 - val_recall_4: 0.6266\n",
      "Epoch 22/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6824 - accuracy: 0.5483 - recall_4: 0.6034\n",
      "Epoch 22: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6826 - accuracy: 0.5482 - recall_4: 0.6033 - val_loss: 0.6809 - val_accuracy: 0.5509 - val_recall_4: 0.6403\n",
      "Epoch 23/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5524 - recall_4: 0.6121\n",
      "Epoch 23: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6812 - accuracy: 0.5524 - recall_4: 0.6128 - val_loss: 0.6828 - val_accuracy: 0.5479 - val_recall_4: 0.6444\n",
      "Epoch 24/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.5504 - recall_4: 0.6114\n",
      "Epoch 24: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6815 - accuracy: 0.5503 - recall_4: 0.6114 - val_loss: 0.6823 - val_accuracy: 0.5443 - val_recall_4: 0.6468\n",
      "Epoch 25/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.5444 - recall_4: 0.6190\n",
      "Epoch 25: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6804 - accuracy: 0.5445 - recall_4: 0.6183 - val_loss: 0.6783 - val_accuracy: 0.5566 - val_recall_4: 0.6290\n",
      "Epoch 26/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6806 - accuracy: 0.5499 - recall_4: 0.6097\n",
      "Epoch 26: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6803 - accuracy: 0.5498 - recall_4: 0.6102 - val_loss: 0.6842 - val_accuracy: 0.5354 - val_recall_4: 0.6548\n",
      "Epoch 27/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6805 - accuracy: 0.5433 - recall_4: 0.6044\n",
      "Epoch 27: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6809 - accuracy: 0.5439 - recall_4: 0.6027 - val_loss: 0.6624 - val_accuracy: 0.5973 - val_recall_4: 0.5782\n",
      "Epoch 28/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6810 - accuracy: 0.5512 - recall_4: 0.6028\n",
      "Epoch 28: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6807 - accuracy: 0.5511 - recall_4: 0.6025 - val_loss: 0.6788 - val_accuracy: 0.5522 - val_recall_4: 0.6339\n",
      "Epoch 29/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6801 - accuracy: 0.5515 - recall_4: 0.6149\n",
      "Epoch 29: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6800 - accuracy: 0.5518 - recall_4: 0.6148 - val_loss: 0.6796 - val_accuracy: 0.5490 - val_recall_4: 0.6323\n",
      "Epoch 30/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6796 - accuracy: 0.5510 - recall_4: 0.6106\n",
      "Epoch 30: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6800 - accuracy: 0.5514 - recall_4: 0.6102 - val_loss: 0.6769 - val_accuracy: 0.5566 - val_recall_4: 0.6315\n",
      "Epoch 31/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.5517 - recall_4: 0.6061\n",
      "Epoch 31: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6802 - accuracy: 0.5512 - recall_4: 0.6059 - val_loss: 0.6957 - val_accuracy: 0.5059 - val_recall_4: 0.7129\n",
      "Epoch 32/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6805 - accuracy: 0.5415 - recall_4: 0.6146\n",
      "Epoch 32: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6807 - accuracy: 0.5415 - recall_4: 0.6142 - val_loss: 0.6757 - val_accuracy: 0.5614 - val_recall_4: 0.6315\n",
      "Epoch 33/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6792 - accuracy: 0.5508 - recall_4: 0.6032\n",
      "Epoch 33: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6796 - accuracy: 0.5512 - recall_4: 0.6037 - val_loss: 0.6746 - val_accuracy: 0.5627 - val_recall_4: 0.6250\n",
      "Epoch 34/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6803 - accuracy: 0.5444 - recall_4: 0.6165\n",
      "Epoch 34: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6802 - accuracy: 0.5442 - recall_4: 0.6160 - val_loss: 0.6706 - val_accuracy: 0.5707 - val_recall_4: 0.6210\n",
      "Epoch 35/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6793 - accuracy: 0.5483 - recall_4: 0.6121\n",
      "Epoch 35: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6794 - accuracy: 0.5484 - recall_4: 0.6118 - val_loss: 0.6612 - val_accuracy: 0.5949 - val_recall_4: 0.5831\n",
      "Epoch 36/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6791 - accuracy: 0.5524 - recall_4: 0.6013\n",
      "Epoch 36: val_loss did not improve from 0.65417\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6791 - accuracy: 0.5524 - recall_4: 0.6013 - val_loss: 0.6855 - val_accuracy: 0.5286 - val_recall_4: 0.6774\n",
      "Epoch 37/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6790 - accuracy: 0.5440 - recall_4: 0.6186\n",
      "Epoch 37: val_loss improved from 0.65417 to 0.65407, saving model to model\\best_model_hybrid_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6793 - accuracy: 0.5446 - recall_4: 0.6181 - val_loss: 0.6541 - val_accuracy: 0.6182 - val_recall_4: 0.5435\n",
      "Epoch 38/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6797 - accuracy: 0.5516 - recall_4: 0.6035\n",
      "Epoch 38: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6798 - accuracy: 0.5514 - recall_4: 0.6033 - val_loss: 0.6867 - val_accuracy: 0.5281 - val_recall_4: 0.6879\n",
      "Epoch 39/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6774 - accuracy: 0.5461 - recall_4: 0.6176\n",
      "Epoch 39: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6774 - accuracy: 0.5463 - recall_4: 0.6181 - val_loss: 0.6921 - val_accuracy: 0.5135 - val_recall_4: 0.7048\n",
      "Epoch 40/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6792 - accuracy: 0.5449 - recall_4: 0.6243\n",
      "Epoch 40: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6791 - accuracy: 0.5452 - recall_4: 0.6237 - val_loss: 0.6567 - val_accuracy: 0.6086 - val_recall_4: 0.5556\n",
      "Epoch 41/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6797 - accuracy: 0.5542 - recall_4: 0.5962\n",
      "Epoch 41: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6797 - accuracy: 0.5542 - recall_4: 0.5962 - val_loss: 0.6736 - val_accuracy: 0.5674 - val_recall_4: 0.6169\n",
      "Epoch 42/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6783 - accuracy: 0.5571 - recall_4: 0.6004\n",
      "Epoch 42: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6783 - accuracy: 0.5568 - recall_4: 0.6002 - val_loss: 0.6936 - val_accuracy: 0.5099 - val_recall_4: 0.7032\n",
      "Epoch 43/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6788 - accuracy: 0.5513 - recall_4: 0.6081\n",
      "Epoch 43: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6785 - accuracy: 0.5510 - recall_4: 0.6090 - val_loss: 0.6924 - val_accuracy: 0.5109 - val_recall_4: 0.6976\n",
      "Epoch 44/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6793 - accuracy: 0.5499 - recall_4: 0.6058\n",
      "Epoch 44: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6793 - accuracy: 0.5500 - recall_4: 0.6061 - val_loss: 0.6873 - val_accuracy: 0.5292 - val_recall_4: 0.6774\n",
      "Epoch 45/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6791 - accuracy: 0.5546 - recall_4: 0.6026\n",
      "Epoch 45: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6795 - accuracy: 0.5544 - recall_4: 0.6017 - val_loss: 0.6875 - val_accuracy: 0.5231 - val_recall_4: 0.6847\n",
      "Epoch 46/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6789 - accuracy: 0.5477 - recall_4: 0.5970\n",
      "Epoch 46: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6787 - accuracy: 0.5479 - recall_4: 0.5970 - val_loss: 0.6764 - val_accuracy: 0.5532 - val_recall_4: 0.6347\n",
      "Epoch 47/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6787 - accuracy: 0.5520 - recall_4: 0.6042\n",
      "Epoch 47: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6787 - accuracy: 0.5519 - recall_4: 0.6047 - val_loss: 0.6924 - val_accuracy: 0.5157 - val_recall_4: 0.6903\n",
      "Epoch 48/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6787 - accuracy: 0.5536 - recall_4: 0.5993\n",
      "Epoch 48: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6790 - accuracy: 0.5539 - recall_4: 0.5988 - val_loss: 0.6759 - val_accuracy: 0.5661 - val_recall_4: 0.6226\n",
      "Epoch 49/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6779 - accuracy: 0.5634 - recall_4: 0.6022\n",
      "Epoch 49: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6783 - accuracy: 0.5626 - recall_4: 0.6023 - val_loss: 0.7273 - val_accuracy: 0.4322 - val_recall_4: 0.8153\n",
      "Epoch 50/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6795 - accuracy: 0.5442 - recall_4: 0.6063\n",
      "Epoch 50: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6795 - accuracy: 0.5442 - recall_4: 0.6063 - val_loss: 0.6698 - val_accuracy: 0.5700 - val_recall_4: 0.6153\n",
      "Epoch 51/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6791 - accuracy: 0.5454 - recall_4: 0.6104\n",
      "Epoch 51: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6789 - accuracy: 0.5452 - recall_4: 0.6106 - val_loss: 0.6751 - val_accuracy: 0.5531 - val_recall_4: 0.6347\n",
      "Epoch 52/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6791 - accuracy: 0.5456 - recall_4: 0.6039\n",
      "Epoch 52: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6791 - accuracy: 0.5456 - recall_4: 0.6039 - val_loss: 0.6761 - val_accuracy: 0.5502 - val_recall_4: 0.6403\n",
      "Epoch 53/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6794 - accuracy: 0.5474 - recall_4: 0.6170\n",
      "Epoch 53: val_loss did not improve from 0.65407\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6793 - accuracy: 0.5477 - recall_4: 0.6160 - val_loss: 0.6656 - val_accuracy: 0.5835 - val_recall_4: 0.5992\n",
      "Epoch 54/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6771 - accuracy: 0.5517 - recall_4: 0.6056\n",
      "Epoch 54: val_loss improved from 0.65407 to 0.65140, saving model to model\\best_model_hybrid_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6773 - accuracy: 0.5518 - recall_4: 0.6045 - val_loss: 0.6514 - val_accuracy: 0.6186 - val_recall_4: 0.5371\n",
      "Epoch 55/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6789 - accuracy: 0.5569 - recall_4: 0.5981\n",
      "Epoch 55: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6788 - accuracy: 0.5568 - recall_4: 0.5984 - val_loss: 0.6825 - val_accuracy: 0.5409 - val_recall_4: 0.6589\n",
      "Epoch 56/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6795 - accuracy: 0.5481 - recall_4: 0.6070\n",
      "Epoch 56: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6794 - accuracy: 0.5482 - recall_4: 0.6067 - val_loss: 0.6729 - val_accuracy: 0.5687 - val_recall_4: 0.6185\n",
      "Epoch 57/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6793 - accuracy: 0.5518 - recall_4: 0.6076\n",
      "Epoch 57: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6789 - accuracy: 0.5513 - recall_4: 0.6073 - val_loss: 0.6804 - val_accuracy: 0.5479 - val_recall_4: 0.6500\n",
      "Epoch 58/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6789 - accuracy: 0.5519 - recall_4: 0.5963\n",
      "Epoch 58: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6791 - accuracy: 0.5518 - recall_4: 0.5964 - val_loss: 0.6690 - val_accuracy: 0.5742 - val_recall_4: 0.6113\n",
      "Epoch 59/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6785 - accuracy: 0.5513 - recall_4: 0.6077\n",
      "Epoch 59: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6783 - accuracy: 0.5514 - recall_4: 0.6077 - val_loss: 0.6700 - val_accuracy: 0.5753 - val_recall_4: 0.6161\n",
      "Epoch 60/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6788 - accuracy: 0.5533 - recall_4: 0.6040\n",
      "Epoch 60: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6791 - accuracy: 0.5532 - recall_4: 0.6039 - val_loss: 0.6811 - val_accuracy: 0.5379 - val_recall_4: 0.6621\n",
      "Epoch 61/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6800 - accuracy: 0.5438 - recall_4: 0.6104\n",
      "Epoch 61: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6800 - accuracy: 0.5438 - recall_4: 0.6104 - val_loss: 0.6699 - val_accuracy: 0.5754 - val_recall_4: 0.6089\n",
      "Epoch 62/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6793 - accuracy: 0.5502 - recall_4: 0.6026\n",
      "Epoch 62: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6791 - accuracy: 0.5499 - recall_4: 0.6027 - val_loss: 0.6645 - val_accuracy: 0.5885 - val_recall_4: 0.5895\n",
      "Epoch 63/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6777 - accuracy: 0.5549 - recall_4: 0.5952\n",
      "Epoch 63: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6776 - accuracy: 0.5549 - recall_4: 0.5958 - val_loss: 0.6859 - val_accuracy: 0.5316 - val_recall_4: 0.6790\n",
      "Epoch 64/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.5529 - recall_4: 0.5963\n",
      "Epoch 64: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6782 - accuracy: 0.5528 - recall_4: 0.5964 - val_loss: 0.6872 - val_accuracy: 0.5272 - val_recall_4: 0.6839\n",
      "Epoch 65/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6791 - accuracy: 0.5489 - recall_4: 0.6009\n",
      "Epoch 65: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6789 - accuracy: 0.5491 - recall_4: 0.6011 - val_loss: 0.6675 - val_accuracy: 0.5700 - val_recall_4: 0.6048\n",
      "Epoch 66/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6795 - accuracy: 0.5528 - recall_4: 0.6069\n",
      "Epoch 66: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6795 - accuracy: 0.5528 - recall_4: 0.6069 - val_loss: 0.7075 - val_accuracy: 0.4743 - val_recall_4: 0.7629\n",
      "Epoch 67/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6773 - accuracy: 0.5531 - recall_4: 0.6173\n",
      "Epoch 67: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6776 - accuracy: 0.5523 - recall_4: 0.6154 - val_loss: 0.6664 - val_accuracy: 0.5786 - val_recall_4: 0.6040\n",
      "Epoch 68/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6779 - accuracy: 0.5517 - recall_4: 0.6142\n",
      "Epoch 68: val_loss did not improve from 0.65140\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6779 - accuracy: 0.5517 - recall_4: 0.6142 - val_loss: 0.6696 - val_accuracy: 0.5727 - val_recall_4: 0.6081\n",
      "Epoch 69/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6780 - accuracy: 0.5476 - recall_4: 0.6125\n",
      "Epoch 69: val_loss improved from 0.65140 to 0.64685, saving model to model\\best_model_hybrid_GRUs.h5\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6777 - accuracy: 0.5485 - recall_4: 0.6124 - val_loss: 0.6469 - val_accuracy: 0.6258 - val_recall_4: 0.5177\n",
      "Epoch 70/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6771 - accuracy: 0.5576 - recall_4: 0.6038\n",
      "Epoch 70: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6771 - accuracy: 0.5573 - recall_4: 0.6045 - val_loss: 0.6826 - val_accuracy: 0.5412 - val_recall_4: 0.6556\n",
      "Epoch 71/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6779 - accuracy: 0.5556 - recall_4: 0.6013\n",
      "Epoch 71: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6782 - accuracy: 0.5550 - recall_4: 0.6009 - val_loss: 0.6957 - val_accuracy: 0.5049 - val_recall_4: 0.7242\n",
      "Epoch 72/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.5487 - recall_4: 0.6048\n",
      "Epoch 72: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6783 - accuracy: 0.5487 - recall_4: 0.6045 - val_loss: 0.6874 - val_accuracy: 0.5274 - val_recall_4: 0.6831\n",
      "Epoch 73/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6792 - accuracy: 0.5529 - recall_4: 0.5980\n",
      "Epoch 73: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6795 - accuracy: 0.5523 - recall_4: 0.5984 - val_loss: 0.6956 - val_accuracy: 0.5070 - val_recall_4: 0.7194\n",
      "Epoch 74/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6787 - accuracy: 0.5493 - recall_4: 0.6024\n",
      "Epoch 74: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6788 - accuracy: 0.5492 - recall_4: 0.6021 - val_loss: 0.6717 - val_accuracy: 0.5686 - val_recall_4: 0.6250\n",
      "Epoch 75/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6779 - accuracy: 0.5507 - recall_4: 0.6063\n",
      "Epoch 75: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6779 - accuracy: 0.5508 - recall_4: 0.6063 - val_loss: 0.6730 - val_accuracy: 0.5646 - val_recall_4: 0.6234\n",
      "Epoch 76/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6792 - accuracy: 0.5505 - recall_4: 0.6038\n",
      "Epoch 76: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6792 - accuracy: 0.5506 - recall_4: 0.6037 - val_loss: 0.6671 - val_accuracy: 0.5806 - val_recall_4: 0.6161\n",
      "Epoch 77/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6772 - accuracy: 0.5534 - recall_4: 0.6105\n",
      "Epoch 77: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6772 - accuracy: 0.5533 - recall_4: 0.6098 - val_loss: 0.6542 - val_accuracy: 0.6101 - val_recall_4: 0.5468\n",
      "Epoch 78/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6787 - accuracy: 0.5546 - recall_4: 0.5984\n",
      "Epoch 78: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6785 - accuracy: 0.5543 - recall_4: 0.5986 - val_loss: 0.6973 - val_accuracy: 0.5024 - val_recall_4: 0.7282\n",
      "Epoch 79/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6782 - accuracy: 0.5498 - recall_4: 0.6112\n",
      "Epoch 79: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6781 - accuracy: 0.5503 - recall_4: 0.6098 - val_loss: 0.6522 - val_accuracy: 0.6153 - val_recall_4: 0.5419\n",
      "Epoch 80/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6785 - accuracy: 0.5530 - recall_4: 0.6004\n",
      "Epoch 80: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6785 - accuracy: 0.5530 - recall_4: 0.6004 - val_loss: 0.6759 - val_accuracy: 0.5560 - val_recall_4: 0.6347\n",
      "Epoch 81/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6778 - accuracy: 0.5520 - recall_4: 0.6067\n",
      "Epoch 81: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6778 - accuracy: 0.5519 - recall_4: 0.6061 - val_loss: 0.6811 - val_accuracy: 0.5461 - val_recall_4: 0.6565\n",
      "Epoch 82/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6796 - accuracy: 0.5527 - recall_4: 0.6075\n",
      "Epoch 82: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6796 - accuracy: 0.5527 - recall_4: 0.6073 - val_loss: 0.6667 - val_accuracy: 0.5795 - val_recall_4: 0.6000\n",
      "Epoch 83/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6783 - accuracy: 0.5529 - recall_4: 0.6064\n",
      "Epoch 83: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6784 - accuracy: 0.5527 - recall_4: 0.6061 - val_loss: 0.6829 - val_accuracy: 0.5439 - val_recall_4: 0.6581\n",
      "Epoch 84/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6781 - accuracy: 0.5505 - recall_4: 0.6003\n",
      "Epoch 84: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6783 - accuracy: 0.5505 - recall_4: 0.6000 - val_loss: 0.6766 - val_accuracy: 0.5552 - val_recall_4: 0.6411\n",
      "Epoch 85/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6784 - accuracy: 0.5596 - recall_4: 0.5988\n",
      "Epoch 85: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6784 - accuracy: 0.5596 - recall_4: 0.5988 - val_loss: 0.6726 - val_accuracy: 0.5636 - val_recall_4: 0.6250\n",
      "Epoch 86/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6777 - accuracy: 0.5494 - recall_4: 0.6162\n",
      "Epoch 86: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6785 - accuracy: 0.5491 - recall_4: 0.6148 - val_loss: 0.6726 - val_accuracy: 0.5617 - val_recall_4: 0.6306\n",
      "Epoch 87/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6786 - accuracy: 0.5499 - recall_4: 0.6028\n",
      "Epoch 87: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6787 - accuracy: 0.5497 - recall_4: 0.6035 - val_loss: 0.7064 - val_accuracy: 0.4713 - val_recall_4: 0.7637\n",
      "Epoch 88/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6774 - accuracy: 0.5488 - recall_4: 0.6106\n",
      "Epoch 88: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6774 - accuracy: 0.5488 - recall_4: 0.6106 - val_loss: 0.6829 - val_accuracy: 0.5339 - val_recall_4: 0.6685\n",
      "Epoch 89/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6794 - accuracy: 0.5511 - recall_4: 0.5921\n",
      "Epoch 89: val_loss did not improve from 0.64685\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6792 - accuracy: 0.5510 - recall_4: 0.5932 - val_loss: 0.6862 - val_accuracy: 0.5283 - val_recall_4: 0.6839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x14692593d30>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(GRU(32, input_shape=(timesteps, input_dim),\n",
    "                    kernel_regularizer=regularizers.l2(0.01),\n",
    "                    recurrent_regularizer=regularizers.l2(0.01),\n",
    "                    bias_regularizer=regularizers.l2(0.01),\n",
    "                     dropout=0.4, recurrent_dropout=0.4))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', Recall()])\n",
    "\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "checkpoint = ModelCheckpoint('model/best_model_hybrid_GRUs.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 1s 2ms/step\n",
      "      Prediction  Actual\n",
      "0       0.329776       0\n",
      "1       0.355786       0\n",
      "2       0.499537       0\n",
      "3       0.561208       0\n",
      "4       0.450245       0\n",
      "...          ...     ...\n",
      "8215    0.360723       0\n",
      "8216    0.613672       0\n",
      "8217    0.524421       0\n",
      "8218    0.587118       0\n",
      "8219    0.623372       1\n",
      "\n",
      "[8220 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "best_model = load_model('model/best_model_hybrid_GRUs.h5')\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Compare the predictions with the actual values\n",
    "comparison = pd.DataFrame({'Prediction': predictions.flatten(), 'Actual': y_test})\n",
    "\n",
    "# Print the comparison\n",
    "print(comparison)\n",
    "predictions_labels = [1 if p > 0.5 else 0 for p in predictions.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_labels)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.64      0.73      6679\n",
      "           1       0.24      0.49      0.32      1541\n",
      "\n",
      "    accuracy                           0.61      8220\n",
      "   macro avg       0.54      0.56      0.52      8220\n",
      "weighted avg       0.73      0.61      0.65      8220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, predictions_labels, output_dict=True)\n",
    "print(classification_report(y_test, predictions_labels))\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv('result/hybrid_GRUs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
