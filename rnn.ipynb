{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = pd.read_csv('data/data_encoded_scaled.csv')\n",
    "data_cleaned = pd.read_csv('data/data_cleaned.csv')\n",
    "data_feature = data_cleaned.drop(columns=['TOTAL_DELAY', 'DEP_DEL15'])\n",
    "data_target = data_cleaned['DEP_DEL15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = data_encoded_scaled.drop(columns=['RESIDUALS', 'DEP_DEL15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.968432919954903\n"
     ]
    }
   ],
   "source": [
    "sequence_days = 7\n",
    "daily_counts = data_feature.groupby(['MONTH', 'DAY_OF_MONTH', 'DEPARTING_AIRPORT']).size() # 一天一个机场的航班数\n",
    "average_rows = daily_counts.mean()\n",
    "\n",
    "print(average_rows*sequence_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_encoded_scaled.assign(TARGET=data_target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MONTH  DAY_OF_MONTH  DAY_OF_WEEK  DEP_TIME_BLK  DISTANCE_GROUP  \\\n",
      "0    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "1    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "2    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "3    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "4    0.0           0.0     0.166667      0.166667        0.285714   \n",
      "\n",
      "   SEGMENT_NUMBER  CONCURRENT_FLIGHTS  NUMBER_OF_SEATS  AIRPORT_FLIGHTS_MONTH  \\\n",
      "0           0.000            0.098592              1.0               0.115453   \n",
      "1           0.000            0.098592              1.0               0.115453   \n",
      "2           0.000            0.112676              1.0               0.111384   \n",
      "3           0.000            0.267606              1.0               0.333661   \n",
      "4           0.125            0.042254              1.0               0.028528   \n",
      "\n",
      "   AIRLINE_FLIGHTS_MONTH  ...  PREVIOUS_AIRPORT_3  PREVIOUS_AIRPORT_4  \\\n",
      "0               0.183515  ...                 0.0                 0.0   \n",
      "1               0.183515  ...                 0.0                 0.0   \n",
      "2               0.183515  ...                 0.0                 0.0   \n",
      "3               0.183515  ...                 0.0                 0.0   \n",
      "4               0.183515  ...                 0.0                 0.0   \n",
      "\n",
      "   PREVIOUS_AIRPORT_5  PREVIOUS_AIRPORT_6  PRCP  SNOW  SNWD      TMAX  \\\n",
      "0                 0.0                 1.0   0.0   0.0   0.0  0.353982   \n",
      "1                 0.0                 1.0   0.0   0.0   0.0  0.353982   \n",
      "2                 0.0                 1.0   0.0   0.0   0.0  0.451327   \n",
      "3                 0.0                 1.0   0.0   0.0   0.0  0.699115   \n",
      "4                 1.0                 0.0   0.0   0.0   0.0  0.460177   \n",
      "\n",
      "       AWND  TARGET  \n",
      "0  0.198638       0  \n",
      "1  0.198638       0  \n",
      "2  0.172291       0  \n",
      "3  0.370930       0  \n",
      "4  0.178804       0  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences shape: (41100, 28, 32)\n",
      "Target values shape: (41100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequence_length = int(round(average_rows*sequence_days))\n",
    "\n",
    "unique_dep_airport = data_feature['DEPARTING_AIRPORT'].unique()\n",
    "unique_flight_number = data_feature['FLIGHT_NUMBER'].unique()\n",
    "\n",
    "X_sequences = []\n",
    "y_targets = []\n",
    "\n",
    "for dep_airport in unique_dep_airport:\n",
    "    flight_data = data_full[data_feature['DEPARTING_AIRPORT'] == dep_airport]\n",
    "    flight_data_values = flight_data.iloc[:, :-1].values\n",
    "    flight_data_target = flight_data.iloc[:, -1].values\n",
    "    for i in range(len(flight_data) - sequence_length):\n",
    "        X_sequences.append(flight_data_values[i:i+sequence_length])\n",
    "        y_targets.append(flight_data_target[i+sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_targets = np.array(y_targets)\n",
    "\n",
    "print(\"Input sequences shape:\", X_sequences.shape)\n",
    "print(\"Target values shape:\", y_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = list(zip(X_sequences, y_targets))\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Separate the sequences and targets\n",
    "X_train, y_train = zip(*train_data)\n",
    "X_test, y_test = zip(*test_data)\n",
    "\n",
    "# Convert the results back to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "timesteps = X_train.shape[1]\n",
    "input_dim = X_train.shape[2]\n",
    "\n",
    "weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train) , y = y_train)\n",
    "class_weights = dict(zip(np.unique(y_train), weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "813/822 [============================>.] - ETA: 0s - loss: 1.3369 - accuracy: 0.4983 - recall: 0.5127\n",
      "Epoch 1: val_loss improved from inf to 1.20611, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 4s 3ms/step - loss: 1.3360 - accuracy: 0.4987 - recall: 0.5128 - val_loss: 1.2061 - val_accuracy: 0.5890 - val_recall: 0.4200\n",
      "Epoch 2/300\n",
      " 56/822 [=>............................] - ETA: 2s - loss: 1.2558 - accuracy: 0.5257 - recall: 0.5614"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816/822 [============================>.] - ETA: 0s - loss: 1.2049 - accuracy: 0.5098 - recall: 0.5261\n",
      "Epoch 2: val_loss improved from 1.20611 to 1.11062, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 1.2046 - accuracy: 0.5104 - recall: 0.5258 - val_loss: 1.1106 - val_accuracy: 0.5803 - val_recall: 0.4832\n",
      "Epoch 3/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 1.1007 - accuracy: 0.5175 - recall: 0.5477\n",
      "Epoch 3: val_loss improved from 1.11062 to 1.02265, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 1.0999 - accuracy: 0.5184 - recall: 0.5478 - val_loss: 1.0226 - val_accuracy: 0.5832 - val_recall: 0.4888\n",
      "Epoch 4/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 1.0139 - accuracy: 0.5371 - recall: 0.5364\n",
      "Epoch 4: val_loss improved from 1.02265 to 0.97485, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 1.0138 - accuracy: 0.5370 - recall: 0.5367 - val_loss: 0.9748 - val_accuracy: 0.5061 - val_recall: 0.6672\n",
      "Epoch 5/300\n",
      "810/822 [============================>.] - ETA: 0s - loss: 0.9394 - accuracy: 0.5346 - recall: 0.5821\n",
      "Epoch 5: val_loss improved from 0.97485 to 0.89914, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.9383 - accuracy: 0.5354 - recall: 0.5827 - val_loss: 0.8991 - val_accuracy: 0.5397 - val_recall: 0.6200\n",
      "Epoch 6/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.8813 - accuracy: 0.5397 - recall: 0.5752\n",
      "Epoch 6: val_loss improved from 0.89914 to 0.84207, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.8812 - accuracy: 0.5397 - recall: 0.5749 - val_loss: 0.8421 - val_accuracy: 0.5634 - val_recall: 0.5776\n",
      "Epoch 7/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.8330 - accuracy: 0.5423 - recall: 0.5874\n",
      "Epoch 7: val_loss improved from 0.84207 to 0.81056, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.8330 - accuracy: 0.5423 - recall: 0.5874 - val_loss: 0.8106 - val_accuracy: 0.5362 - val_recall: 0.6328\n",
      "Epoch 8/300\n",
      "811/822 [============================>.] - ETA: 0s - loss: 0.7969 - accuracy: 0.5369 - recall: 0.5956\n",
      "Epoch 8: val_loss improved from 0.81056 to 0.77709, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7965 - accuracy: 0.5374 - recall: 0.5971 - val_loss: 0.7771 - val_accuracy: 0.5444 - val_recall: 0.6296\n",
      "Epoch 9/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.7678 - accuracy: 0.5427 - recall: 0.5988\n",
      "Epoch 9: val_loss improved from 0.77709 to 0.75093, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.7678 - accuracy: 0.5427 - recall: 0.5993 - val_loss: 0.7509 - val_accuracy: 0.5598 - val_recall: 0.6024\n",
      "Epoch 10/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7476 - accuracy: 0.5378 - recall: 0.5918\n",
      "Epoch 10: val_loss improved from 0.75093 to 0.73772, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7475 - accuracy: 0.5382 - recall: 0.5920 - val_loss: 0.7377 - val_accuracy: 0.5455 - val_recall: 0.6224\n",
      "Epoch 11/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.7303 - accuracy: 0.5496 - recall: 0.5852\n",
      "Epoch 11: val_loss improved from 0.73772 to 0.73457, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7301 - accuracy: 0.5496 - recall: 0.5856 - val_loss: 0.7346 - val_accuracy: 0.5166 - val_recall: 0.6704\n",
      "Epoch 12/300\n",
      "810/822 [============================>.] - ETA: 0s - loss: 0.7193 - accuracy: 0.5478 - recall: 0.5906\n",
      "Epoch 12: val_loss improved from 0.73457 to 0.73137, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.7192 - accuracy: 0.5474 - recall: 0.5916 - val_loss: 0.7314 - val_accuracy: 0.4986 - val_recall: 0.6968\n",
      "Epoch 13/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.7111 - accuracy: 0.5544 - recall: 0.5880\n",
      "Epoch 13: val_loss did not improve from 0.73137\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.7113 - accuracy: 0.5536 - recall: 0.5886 - val_loss: 0.7335 - val_accuracy: 0.4769 - val_recall: 0.7320\n",
      "Epoch 14/300\n",
      "812/822 [============================>.] - ETA: 0s - loss: 0.7055 - accuracy: 0.5439 - recall: 0.6018\n",
      "Epoch 14: val_loss improved from 0.73137 to 0.69634, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7056 - accuracy: 0.5443 - recall: 0.6025 - val_loss: 0.6963 - val_accuracy: 0.5646 - val_recall: 0.5944\n",
      "Epoch 15/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.7020 - accuracy: 0.5533 - recall: 0.5828\n",
      "Epoch 15: val_loss did not improve from 0.69634\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.7025 - accuracy: 0.5525 - recall: 0.5836 - val_loss: 0.7194 - val_accuracy: 0.4904 - val_recall: 0.7112\n",
      "Epoch 16/300\n",
      "803/822 [============================>.] - ETA: 0s - loss: 0.6972 - accuracy: 0.5459 - recall: 0.5994\n",
      "Epoch 16: val_loss did not improve from 0.69634\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6975 - accuracy: 0.5454 - recall: 0.5995 - val_loss: 0.7060 - val_accuracy: 0.5189 - val_recall: 0.6624\n",
      "Epoch 17/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6958 - accuracy: 0.5481 - recall: 0.5992\n",
      "Epoch 17: val_loss improved from 0.69634 to 0.68409, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6956 - accuracy: 0.5484 - recall: 0.5991 - val_loss: 0.6841 - val_accuracy: 0.5797 - val_recall: 0.5776\n",
      "Epoch 18/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6937 - accuracy: 0.5490 - recall: 0.5956\n",
      "Epoch 18: val_loss did not improve from 0.68409\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6931 - accuracy: 0.5491 - recall: 0.5957 - val_loss: 0.6911 - val_accuracy: 0.5499 - val_recall: 0.6232\n",
      "Epoch 19/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6916 - accuracy: 0.5487 - recall: 0.5964\n",
      "Epoch 19: val_loss improved from 0.68409 to 0.68348, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6916 - accuracy: 0.5485 - recall: 0.5961 - val_loss: 0.6835 - val_accuracy: 0.5706 - val_recall: 0.5944\n",
      "Epoch 20/300\n",
      "807/822 [============================>.] - ETA: 0s - loss: 0.6912 - accuracy: 0.5487 - recall: 0.5903\n",
      "Epoch 20: val_loss improved from 0.68348 to 0.67063, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6903 - accuracy: 0.5493 - recall: 0.5912 - val_loss: 0.6706 - val_accuracy: 0.6007 - val_recall: 0.5408\n",
      "Epoch 21/300\n",
      "804/822 [============================>.] - ETA: 0s - loss: 0.6879 - accuracy: 0.5568 - recall: 0.5793\n",
      "Epoch 21: val_loss did not improve from 0.67063\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6886 - accuracy: 0.5565 - recall: 0.5783 - val_loss: 0.6885 - val_accuracy: 0.5439 - val_recall: 0.6352\n",
      "Epoch 22/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6884 - accuracy: 0.5451 - recall: 0.5995\n",
      "Epoch 22: val_loss did not improve from 0.67063\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6884 - accuracy: 0.5451 - recall: 0.5995 - val_loss: 0.6794 - val_accuracy: 0.5756 - val_recall: 0.5832\n",
      "Epoch 23/300\n",
      "807/822 [============================>.] - ETA: 0s - loss: 0.6879 - accuracy: 0.5492 - recall: 0.5902\n",
      "Epoch 23: val_loss did not improve from 0.67063\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6881 - accuracy: 0.5498 - recall: 0.5892 - val_loss: 0.6770 - val_accuracy: 0.5789 - val_recall: 0.5672\n",
      "Epoch 24/300\n",
      "808/822 [============================>.] - ETA: 0s - loss: 0.6885 - accuracy: 0.5456 - recall: 0.5911\n",
      "Epoch 24: val_loss improved from 0.67063 to 0.66044, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6877 - accuracy: 0.5465 - recall: 0.5908 - val_loss: 0.6604 - val_accuracy: 0.6282 - val_recall: 0.4752\n",
      "Epoch 25/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6870 - accuracy: 0.5576 - recall: 0.5728\n",
      "Epoch 25: val_loss did not improve from 0.66044\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6868 - accuracy: 0.5577 - recall: 0.5728 - val_loss: 0.6891 - val_accuracy: 0.5395 - val_recall: 0.6288\n",
      "Epoch 26/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6858 - accuracy: 0.5519 - recall: 0.5866\n",
      "Epoch 26: val_loss did not improve from 0.66044\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6851 - accuracy: 0.5514 - recall: 0.5868 - val_loss: 0.6830 - val_accuracy: 0.5541 - val_recall: 0.6152\n",
      "Epoch 27/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6845 - accuracy: 0.5580 - recall: 0.5888\n",
      "Epoch 27: val_loss did not improve from 0.66044\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6846 - accuracy: 0.5577 - recall: 0.5882 - val_loss: 0.6880 - val_accuracy: 0.5392 - val_recall: 0.6280\n",
      "Epoch 28/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5531 - recall: 0.5898\n",
      "Epoch 28: val_loss did not improve from 0.66044\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6848 - accuracy: 0.5528 - recall: 0.5896 - val_loss: 0.6778 - val_accuracy: 0.5680 - val_recall: 0.5944\n",
      "Epoch 29/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6843 - accuracy: 0.5565 - recall: 0.5920\n",
      "Epoch 29: val_loss did not improve from 0.66044\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6841 - accuracy: 0.5566 - recall: 0.5920 - val_loss: 0.6763 - val_accuracy: 0.5706 - val_recall: 0.5840\n",
      "Epoch 30/300\n",
      "805/822 [============================>.] - ETA: 0s - loss: 0.6832 - accuracy: 0.5535 - recall: 0.5885\n",
      "Epoch 30: val_loss did not improve from 0.66044\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6834 - accuracy: 0.5523 - recall: 0.5894 - val_loss: 0.6912 - val_accuracy: 0.5262 - val_recall: 0.6560\n",
      "Epoch 31/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6838 - accuracy: 0.5509 - recall: 0.5937\n",
      "Epoch 31: val_loss did not improve from 0.66044\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6838 - accuracy: 0.5509 - recall: 0.5937 - val_loss: 0.6733 - val_accuracy: 0.5838 - val_recall: 0.5696\n",
      "Epoch 32/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6837 - accuracy: 0.5438 - recall: 0.5985\n",
      "Epoch 32: val_loss did not improve from 0.66044\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6838 - accuracy: 0.5438 - recall: 0.5985 - val_loss: 0.6697 - val_accuracy: 0.5903 - val_recall: 0.5576\n",
      "Epoch 33/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.5515 - recall: 0.6039\n",
      "Epoch 33: val_loss improved from 0.66044 to 0.65685, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6822 - accuracy: 0.5517 - recall: 0.6038 - val_loss: 0.6569 - val_accuracy: 0.6160 - val_recall: 0.5048\n",
      "Epoch 34/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6843 - accuracy: 0.5564 - recall: 0.5789\n",
      "Epoch 34: val_loss did not improve from 0.65685\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6841 - accuracy: 0.5564 - recall: 0.5795 - val_loss: 0.6834 - val_accuracy: 0.5550 - val_recall: 0.6128\n",
      "Epoch 35/300\n",
      "808/822 [============================>.] - ETA: 0s - loss: 0.6815 - accuracy: 0.5595 - recall: 0.5825\n",
      "Epoch 35: val_loss did not improve from 0.65685\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6816 - accuracy: 0.5582 - recall: 0.5827 - val_loss: 0.6994 - val_accuracy: 0.5090 - val_recall: 0.6800\n",
      "Epoch 36/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6823 - accuracy: 0.5570 - recall: 0.5985\n",
      "Epoch 36: val_loss did not improve from 0.65685\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6820 - accuracy: 0.5572 - recall: 0.5987 - val_loss: 0.6715 - val_accuracy: 0.5900 - val_recall: 0.5568\n",
      "Epoch 37/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.5576 - recall: 0.5890\n",
      "Epoch 37: val_loss did not improve from 0.65685\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6822 - accuracy: 0.5577 - recall: 0.5890 - val_loss: 0.6765 - val_accuracy: 0.5745 - val_recall: 0.5920\n",
      "Epoch 38/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6815 - accuracy: 0.5639 - recall: 0.5840\n",
      "Epoch 38: val_loss did not improve from 0.65685\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6816 - accuracy: 0.5639 - recall: 0.5846 - val_loss: 0.6993 - val_accuracy: 0.5027 - val_recall: 0.6864\n",
      "Epoch 39/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6822 - accuracy: 0.5587 - recall: 0.5831\n",
      "Epoch 39: val_loss did not improve from 0.65685\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6822 - accuracy: 0.5587 - recall: 0.5831 - val_loss: 0.7027 - val_accuracy: 0.4959 - val_recall: 0.7016\n",
      "Epoch 40/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6829 - accuracy: 0.5467 - recall: 0.5989\n",
      "Epoch 40: val_loss did not improve from 0.65685\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6819 - accuracy: 0.5477 - recall: 0.5983 - val_loss: 0.6627 - val_accuracy: 0.6002 - val_recall: 0.5248\n",
      "Epoch 41/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.5546 - recall: 0.5776\n",
      "Epoch 41: val_loss did not improve from 0.65685\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6816 - accuracy: 0.5548 - recall: 0.5775 - val_loss: 0.6869 - val_accuracy: 0.5392 - val_recall: 0.6360\n",
      "Epoch 42/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6808 - accuracy: 0.5588 - recall: 0.5878\n",
      "Epoch 42: val_loss did not improve from 0.65685\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6812 - accuracy: 0.5584 - recall: 0.5882 - val_loss: 0.6816 - val_accuracy: 0.5509 - val_recall: 0.6064\n",
      "Epoch 43/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.6827 - accuracy: 0.5505 - recall: 0.5926\n",
      "Epoch 43: val_loss did not improve from 0.65685\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6818 - accuracy: 0.5510 - recall: 0.5920 - val_loss: 0.6612 - val_accuracy: 0.6071 - val_recall: 0.5184\n",
      "Epoch 44/300\n",
      "810/822 [============================>.] - ETA: 0s - loss: 0.6812 - accuracy: 0.5671 - recall: 0.5758\n",
      "Epoch 44: val_loss did not improve from 0.65685\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6811 - accuracy: 0.5676 - recall: 0.5771 - val_loss: 0.6822 - val_accuracy: 0.5517 - val_recall: 0.6096\n",
      "Epoch 45/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6818 - accuracy: 0.5551 - recall: 0.5896\n",
      "Epoch 45: val_loss improved from 0.65685 to 0.64911, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6817 - accuracy: 0.5556 - recall: 0.5890 - val_loss: 0.6491 - val_accuracy: 0.6448 - val_recall: 0.4520\n",
      "Epoch 46/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5652 - recall: 0.5877\n",
      "Epoch 46: val_loss did not improve from 0.64911\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6809 - accuracy: 0.5650 - recall: 0.5886 - val_loss: 0.6711 - val_accuracy: 0.5856 - val_recall: 0.5680\n",
      "Epoch 47/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.5658 - recall: 0.5741\n",
      "Epoch 47: val_loss did not improve from 0.64911\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6815 - accuracy: 0.5655 - recall: 0.5755 - val_loss: 0.6958 - val_accuracy: 0.5129 - val_recall: 0.6744\n",
      "Epoch 48/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6815 - accuracy: 0.5570 - recall: 0.5895\n",
      "Epoch 48: val_loss did not improve from 0.64911\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6808 - accuracy: 0.5568 - recall: 0.5898 - val_loss: 0.6788 - val_accuracy: 0.5531 - val_recall: 0.6104\n",
      "Epoch 49/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6818 - accuracy: 0.5550 - recall: 0.5816\n",
      "Epoch 49: val_loss did not improve from 0.64911\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6816 - accuracy: 0.5550 - recall: 0.5821 - val_loss: 0.6995 - val_accuracy: 0.5050 - val_recall: 0.7048\n",
      "Epoch 50/300\n",
      "805/822 [============================>.] - ETA: 0s - loss: 0.6805 - accuracy: 0.5514 - recall: 0.5990\n",
      "Epoch 50: val_loss did not improve from 0.64911\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6809 - accuracy: 0.5513 - recall: 0.5989 - val_loss: 0.6769 - val_accuracy: 0.5613 - val_recall: 0.6016\n",
      "Epoch 51/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.5582 - recall: 0.5930\n",
      "Epoch 51: val_loss did not improve from 0.64911\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6799 - accuracy: 0.5581 - recall: 0.5943 - val_loss: 0.6915 - val_accuracy: 0.5219 - val_recall: 0.6640\n",
      "Epoch 52/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6802 - accuracy: 0.5560 - recall: 0.5991\n",
      "Epoch 52: val_loss did not improve from 0.64911\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6801 - accuracy: 0.5559 - recall: 0.5993 - val_loss: 0.6738 - val_accuracy: 0.5672 - val_recall: 0.5952\n",
      "Epoch 53/300\n",
      "811/822 [============================>.] - ETA: 0s - loss: 0.6818 - accuracy: 0.5521 - recall: 0.5910\n",
      "Epoch 53: val_loss did not improve from 0.64911\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6809 - accuracy: 0.5517 - recall: 0.5920 - val_loss: 0.6815 - val_accuracy: 0.5481 - val_recall: 0.6192\n",
      "Epoch 54/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.5572 - recall: 0.5719\n",
      "Epoch 54: val_loss did not improve from 0.64911\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6823 - accuracy: 0.5568 - recall: 0.5728 - val_loss: 0.7028 - val_accuracy: 0.4898 - val_recall: 0.7152\n",
      "Epoch 55/300\n",
      "807/822 [============================>.] - ETA: 0s - loss: 0.6818 - accuracy: 0.5474 - recall: 0.5914\n",
      "Epoch 55: val_loss did not improve from 0.64911\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6819 - accuracy: 0.5485 - recall: 0.5904 - val_loss: 0.6662 - val_accuracy: 0.5969 - val_recall: 0.5344\n",
      "Epoch 56/300\n",
      "804/822 [============================>.] - ETA: 0s - loss: 0.6799 - accuracy: 0.5572 - recall: 0.5928\n",
      "Epoch 56: val_loss did not improve from 0.64911\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6806 - accuracy: 0.5569 - recall: 0.5935 - val_loss: 0.6897 - val_accuracy: 0.5262 - val_recall: 0.6544\n",
      "Epoch 57/300\n",
      "807/822 [============================>.] - ETA: 0s - loss: 0.6805 - accuracy: 0.5497 - recall: 0.6090\n",
      "Epoch 57: val_loss improved from 0.64911 to 0.64471, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6802 - accuracy: 0.5517 - recall: 0.6068 - val_loss: 0.6447 - val_accuracy: 0.6432 - val_recall: 0.4488\n",
      "Epoch 58/300\n",
      "807/822 [============================>.] - ETA: 0s - loss: 0.6799 - accuracy: 0.5558 - recall: 0.5833\n",
      "Epoch 58: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6808 - accuracy: 0.5554 - recall: 0.5831 - val_loss: 0.6978 - val_accuracy: 0.5094 - val_recall: 0.6912\n",
      "Epoch 59/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5563 - recall: 0.5921\n",
      "Epoch 59: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6809 - accuracy: 0.5555 - recall: 0.5924 - val_loss: 0.7108 - val_accuracy: 0.4743 - val_recall: 0.7416\n",
      "Epoch 60/300\n",
      "807/822 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5499 - recall: 0.5863\n",
      "Epoch 60: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6808 - accuracy: 0.5500 - recall: 0.5852 - val_loss: 0.6813 - val_accuracy: 0.5512 - val_recall: 0.6192\n",
      "Epoch 61/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6798 - accuracy: 0.5560 - recall: 0.5905\n",
      "Epoch 61: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6797 - accuracy: 0.5552 - recall: 0.5910 - val_loss: 0.6922 - val_accuracy: 0.5179 - val_recall: 0.6664\n",
      "Epoch 62/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6822 - accuracy: 0.5512 - recall: 0.5934\n",
      "Epoch 62: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6820 - accuracy: 0.5513 - recall: 0.5922 - val_loss: 0.6639 - val_accuracy: 0.6023 - val_recall: 0.5368\n",
      "Epoch 63/300\n",
      "812/822 [============================>.] - ETA: 0s - loss: 0.6806 - accuracy: 0.5556 - recall: 0.5883\n",
      "Epoch 63: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6805 - accuracy: 0.5559 - recall: 0.5880 - val_loss: 0.6693 - val_accuracy: 0.5818 - val_recall: 0.5648\n",
      "Epoch 64/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5594 - recall: 0.5908\n",
      "Epoch 64: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6808 - accuracy: 0.5591 - recall: 0.5912 - val_loss: 0.6975 - val_accuracy: 0.5061 - val_recall: 0.6896\n",
      "Epoch 65/300\n",
      "805/822 [============================>.] - ETA: 0s - loss: 0.6800 - accuracy: 0.5576 - recall: 0.5940\n",
      "Epoch 65: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6798 - accuracy: 0.5576 - recall: 0.5965 - val_loss: 0.6884 - val_accuracy: 0.5327 - val_recall: 0.6400\n",
      "Epoch 66/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6801 - accuracy: 0.5537 - recall: 0.5932\n",
      "Epoch 66: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6800 - accuracy: 0.5540 - recall: 0.5933 - val_loss: 0.6770 - val_accuracy: 0.5593 - val_recall: 0.5944\n",
      "Epoch 67/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6801 - accuracy: 0.5586 - recall: 0.5864\n",
      "Epoch 67: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6800 - accuracy: 0.5582 - recall: 0.5870 - val_loss: 0.6925 - val_accuracy: 0.5251 - val_recall: 0.6688\n",
      "Epoch 68/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.5517 - recall: 0.5853\n",
      "Epoch 68: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6808 - accuracy: 0.5515 - recall: 0.5858 - val_loss: 0.6859 - val_accuracy: 0.5418 - val_recall: 0.6304\n",
      "Epoch 69/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6801 - accuracy: 0.5621 - recall: 0.5817\n",
      "Epoch 69: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6800 - accuracy: 0.5619 - recall: 0.5809 - val_loss: 0.6727 - val_accuracy: 0.5722 - val_recall: 0.5848\n",
      "Epoch 70/300\n",
      "812/822 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.5572 - recall: 0.5934\n",
      "Epoch 70: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6809 - accuracy: 0.5573 - recall: 0.5941 - val_loss: 0.6918 - val_accuracy: 0.5219 - val_recall: 0.6712\n",
      "Epoch 71/300\n",
      "811/822 [============================>.] - ETA: 0s - loss: 0.6793 - accuracy: 0.5583 - recall: 0.5947\n",
      "Epoch 71: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6800 - accuracy: 0.5571 - recall: 0.5943 - val_loss: 0.6958 - val_accuracy: 0.5144 - val_recall: 0.6824\n",
      "Epoch 72/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.5508 - recall: 0.5777\n",
      "Epoch 72: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6815 - accuracy: 0.5512 - recall: 0.5775 - val_loss: 0.6724 - val_accuracy: 0.5766 - val_recall: 0.5616\n",
      "Epoch 73/300\n",
      "804/822 [============================>.] - ETA: 0s - loss: 0.6797 - accuracy: 0.5582 - recall: 0.6006\n",
      "Epoch 73: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6796 - accuracy: 0.5590 - recall: 0.5985 - val_loss: 0.6684 - val_accuracy: 0.5836 - val_recall: 0.5632\n",
      "Epoch 74/300\n",
      "807/822 [============================>.] - ETA: 0s - loss: 0.6808 - accuracy: 0.5617 - recall: 0.5773\n",
      "Epoch 74: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6810 - accuracy: 0.5611 - recall: 0.5765 - val_loss: 0.6771 - val_accuracy: 0.5575 - val_recall: 0.6080\n",
      "Epoch 75/300\n",
      "807/822 [============================>.] - ETA: 0s - loss: 0.6800 - accuracy: 0.5555 - recall: 0.5901\n",
      "Epoch 75: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6800 - accuracy: 0.5561 - recall: 0.5914 - val_loss: 0.6771 - val_accuracy: 0.5628 - val_recall: 0.5984\n",
      "Epoch 76/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6806 - accuracy: 0.5593 - recall: 0.5805\n",
      "Epoch 76: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6805 - accuracy: 0.5585 - recall: 0.5825 - val_loss: 0.6960 - val_accuracy: 0.5103 - val_recall: 0.6816\n",
      "Epoch 77/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6809 - accuracy: 0.5485 - recall: 0.5946\n",
      "Epoch 77: val_loss did not improve from 0.64471\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6816 - accuracy: 0.5484 - recall: 0.5922 - val_loss: 0.6754 - val_accuracy: 0.5663 - val_recall: 0.5784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d006ac82e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(32, input_shape=(timesteps, input_dim),\n",
    "                    kernel_regularizer=regularizers.l2(0.01),\n",
    "                    recurrent_regularizer=regularizers.l2(0.01),\n",
    "                    bias_regularizer=regularizers.l2(0.01),\n",
    "                     dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', Recall()])\n",
    "\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "checkpoint = ModelCheckpoint('model/best_model_rnn.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 1ms/step\n",
      "      Prediction  Actual\n",
      "0       0.442197       0\n",
      "1       0.487043       0\n",
      "2       0.402248       0\n",
      "3       0.336719       1\n",
      "4       0.424492       0\n",
      "...          ...     ...\n",
      "8215    0.567335       0\n",
      "8216    0.387430       0\n",
      "8217    0.446623       0\n",
      "8218    0.416579       0\n",
      "8219    0.464069       0\n",
      "\n",
      "[8220 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "best_model = load_model('model/best_model_rnn.h5')\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Compare the predictions with the actual values\n",
    "comparison = pd.DataFrame({'Prediction': predictions.flatten(), 'Actual': y_test})\n",
    "\n",
    "# Print the comparison\n",
    "print(comparison)\n",
    "predictions_labels = [1 if p > 0.5 else 0 for p in predictions.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_labels)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.68      0.76      6700\n",
      "           1       0.25      0.46      0.32      1520\n",
      "\n",
      "    accuracy                           0.64      8220\n",
      "   macro avg       0.55      0.57      0.54      8220\n",
      "weighted avg       0.74      0.64      0.68      8220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, predictions_labels, output_dict=True)\n",
    "print(classification_report(y_test, predictions_labels))\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv('result/rnn.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
