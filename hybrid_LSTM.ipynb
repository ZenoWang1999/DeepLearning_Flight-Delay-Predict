{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42c38f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a63ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = pd.read_csv('data/data_encoded_scaled.csv')\n",
    "data_cleaned = pd.read_csv('data/data_cleaned.csv')\n",
    "data_feature = data_cleaned.drop(columns=['TOTAL_DELAY', 'DEP_DEL15'])\n",
    "data_target = data_cleaned['DEP_DEL15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd8c93f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = data_encoded_scaled.drop(columns=['DEP_DEL15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "887cb4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.968432919954903\n"
     ]
    }
   ],
   "source": [
    "sequence_days = 7\n",
    "daily_counts = data_feature.groupby(['MONTH', 'DAY_OF_MONTH', 'DEPARTING_AIRPORT']).size()\n",
    "average_rows = daily_counts.mean()\n",
    "\n",
    "print(average_rows*sequence_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a73f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_encoded_scaled.assign(TARGET=data_target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17149eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MONTH  DAY_OF_MONTH  DAY_OF_WEEK  DEP_TIME_BLK  DISTANCE_GROUP  \\\n",
      "0    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "1    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "2    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "3    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "4    0.0           0.0     0.166667      0.166667        0.285714   \n",
      "\n",
      "   SEGMENT_NUMBER  CONCURRENT_FLIGHTS  NUMBER_OF_SEATS  AIRPORT_FLIGHTS_MONTH  \\\n",
      "0           0.000            0.098592              1.0               0.115453   \n",
      "1           0.000            0.098592              1.0               0.115453   \n",
      "2           0.000            0.112676              1.0               0.111384   \n",
      "3           0.000            0.267606              1.0               0.333661   \n",
      "4           0.125            0.042254              1.0               0.028528   \n",
      "\n",
      "   AIRLINE_FLIGHTS_MONTH  ...  PREVIOUS_AIRPORT_4  PREVIOUS_AIRPORT_5  \\\n",
      "0               0.183515  ...                 0.0                 0.0   \n",
      "1               0.183515  ...                 0.0                 0.0   \n",
      "2               0.183515  ...                 0.0                 0.0   \n",
      "3               0.183515  ...                 0.0                 0.0   \n",
      "4               0.183515  ...                 0.0                 1.0   \n",
      "\n",
      "   PREVIOUS_AIRPORT_6  PRCP  SNOW  SNWD      TMAX      AWND  RESIDUALS  TARGET  \n",
      "0                 1.0   0.0   0.0   0.0  0.353982  0.198638   0.052138       0  \n",
      "1                 1.0   0.0   0.0   0.0  0.353982  0.198638   0.052775       0  \n",
      "2                 1.0   0.0   0.0   0.0  0.451327  0.172291   0.051816       0  \n",
      "3                 1.0   0.0   0.0   0.0  0.699115  0.370930   0.051928       0  \n",
      "4                 0.0   0.0   0.0   0.0  0.460177  0.178804   0.051987       0  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e048c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences shape: (41100, 28, 33)\n",
      "Target values shape: (41100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequence_length = int(round(average_rows*sequence_days))\n",
    "\n",
    "unique_dep_airport = data_feature['DEPARTING_AIRPORT'].unique()\n",
    "unique_flight_number = data_feature['FLIGHT_NUMBER'].unique()\n",
    "\n",
    "X_sequences = []\n",
    "y_targets = []\n",
    "\n",
    "for dep_airport in unique_dep_airport:\n",
    "    flight_data = data_full[data_feature['DEPARTING_AIRPORT'] == dep_airport]\n",
    "    flight_data_values = flight_data.iloc[:, :-1].values\n",
    "    flight_data_target = flight_data.iloc[:, -1].values\n",
    "    for i in range(len(flight_data) - sequence_length):\n",
    "        X_sequences.append(flight_data_values[i:i+sequence_length])\n",
    "        y_targets.append(flight_data_target[i+sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_targets = np.array(y_targets)\n",
    "\n",
    "print(\"Input sequences shape:\", X_sequences.shape)\n",
    "print(\"Target values shape:\", y_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e5c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = list(zip(X_sequences, y_targets))\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Separate the sequences and targets\n",
    "X_train, y_train = zip(*train_data)\n",
    "X_test, y_test = zip(*test_data)\n",
    "\n",
    "# Convert the results back to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719e17d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "timesteps = X_train.shape[1]\n",
    "input_dim = X_train.shape[2]\n",
    "\n",
    "weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train) , y = y_train)\n",
    "class_weights = dict(zip(np.unique(y_train), weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28047b63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Epoch 1/300\n",
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "822/822 [==============================] - ETA: 0s - loss: 1.4986 - accuracy: 0.4816 - recall: 0.6199\n",
      "Epoch 1: val_loss improved from inf to 1.24642, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 15s 15ms/step - loss: 1.4986 - accuracy: 0.4816 - recall: 0.6199 - val_loss: 1.2464 - val_accuracy: 0.5044 - val_recall: 0.6469\n",
      "Epoch 2/300\n",
      " 10/822 [..............................] - ETA: 9s - loss: 1.2039 - accuracy: 0.5344 - recall: 0.7647 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820/822 [============================>.] - ETA: 0s - loss: 1.1169 - accuracy: 0.5080 - recall: 0.6437\n",
      "Epoch 2: val_loss improved from 1.24642 to 1.01227, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 1.1165 - accuracy: 0.5081 - recall: 0.6429 - val_loss: 1.0123 - val_accuracy: 0.5395 - val_recall: 0.6077\n",
      "Epoch 3/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.9656 - accuracy: 0.5169 - recall: 0.6254\n",
      "Epoch 3: val_loss improved from 1.01227 to 0.92071, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.9653 - accuracy: 0.5167 - recall: 0.6259 - val_loss: 0.9207 - val_accuracy: 0.5199 - val_recall: 0.6419\n",
      "Epoch 4/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.8921 - accuracy: 0.5281 - recall: 0.6291\n",
      "Epoch 4: val_loss improved from 0.92071 to 0.88514, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.8921 - accuracy: 0.5281 - recall: 0.6291 - val_loss: 0.8851 - val_accuracy: 0.4693 - val_recall: 0.7162\n",
      "Epoch 5/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.8458 - accuracy: 0.5238 - recall: 0.6402\n",
      "Epoch 5: val_loss improved from 0.88514 to 0.81895, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.8460 - accuracy: 0.5238 - recall: 0.6391 - val_loss: 0.8190 - val_accuracy: 0.5481 - val_recall: 0.6010\n",
      "Epoch 6/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.8115 - accuracy: 0.5283 - recall: 0.6423\n",
      "Epoch 6: val_loss improved from 0.81895 to 0.80066, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.8115 - accuracy: 0.5283 - recall: 0.6423 - val_loss: 0.8007 - val_accuracy: 0.5128 - val_recall: 0.6528\n",
      "Epoch 7/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.7817 - accuracy: 0.5358 - recall: 0.6320\n",
      "Epoch 7: val_loss improved from 0.80066 to 0.79413, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.7818 - accuracy: 0.5358 - recall: 0.6323 - val_loss: 0.7941 - val_accuracy: 0.4600 - val_recall: 0.7295\n",
      "Epoch 8/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7583 - accuracy: 0.5294 - recall: 0.6420\n",
      "Epoch 8: val_loss improved from 0.79413 to 0.75458, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.7581 - accuracy: 0.5293 - recall: 0.6417 - val_loss: 0.7546 - val_accuracy: 0.5155 - val_recall: 0.6619\n",
      "Epoch 9/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.7394 - accuracy: 0.5233 - recall: 0.6395\n",
      "Epoch 9: val_loss improved from 0.75458 to 0.71736, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.7394 - accuracy: 0.5233 - recall: 0.6395 - val_loss: 0.7174 - val_accuracy: 0.5766 - val_recall: 0.5559\n",
      "Epoch 10/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7226 - accuracy: 0.5355 - recall: 0.6234\n",
      "Epoch 10: val_loss improved from 0.71736 to 0.71377, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.7231 - accuracy: 0.5359 - recall: 0.6237 - val_loss: 0.7138 - val_accuracy: 0.5382 - val_recall: 0.6227\n",
      "Epoch 11/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.7107 - accuracy: 0.5291 - recall: 0.6371\n",
      "Epoch 11: val_loss improved from 0.71377 to 0.71059, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.7106 - accuracy: 0.5289 - recall: 0.6373 - val_loss: 0.7106 - val_accuracy: 0.5155 - val_recall: 0.6528\n",
      "Epoch 12/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.7016 - accuracy: 0.5319 - recall: 0.6273\n",
      "Epoch 12: val_loss did not improve from 0.71059\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.7018 - accuracy: 0.5318 - recall: 0.6277 - val_loss: 0.7109 - val_accuracy: 0.4854 - val_recall: 0.6937\n",
      "Epoch 13/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6946 - accuracy: 0.5268 - recall: 0.6427\n",
      "Epoch 13: val_loss improved from 0.71059 to 0.69094, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6945 - accuracy: 0.5266 - recall: 0.6427 - val_loss: 0.6909 - val_accuracy: 0.5360 - val_recall: 0.6285\n",
      "Epoch 14/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6897 - accuracy: 0.5286 - recall: 0.6415\n",
      "Epoch 14: val_loss improved from 0.69094 to 0.67894, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6899 - accuracy: 0.5288 - recall: 0.6403 - val_loss: 0.6789 - val_accuracy: 0.5701 - val_recall: 0.5751\n",
      "Epoch 15/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6878 - accuracy: 0.5362 - recall: 0.6293\n",
      "Epoch 15: val_loss did not improve from 0.67894\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6878 - accuracy: 0.5362 - recall: 0.6293 - val_loss: 0.6828 - val_accuracy: 0.5494 - val_recall: 0.6102\n",
      "Epoch 16/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6864 - accuracy: 0.5312 - recall: 0.6317\n",
      "Epoch 16: val_loss improved from 0.67894 to 0.67853, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.6867 - accuracy: 0.5314 - recall: 0.6309 - val_loss: 0.6785 - val_accuracy: 0.5567 - val_recall: 0.5851\n",
      "Epoch 17/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6852 - accuracy: 0.5329 - recall: 0.6334\n",
      "Epoch 17: val_loss did not improve from 0.67853\n",
      "822/822 [==============================] - 11s 13ms/step - loss: 0.6851 - accuracy: 0.5329 - recall: 0.6329 - val_loss: 0.6902 - val_accuracy: 0.5190 - val_recall: 0.6503\n",
      "Epoch 18/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6857 - accuracy: 0.5330 - recall: 0.6274\n",
      "Epoch 18: val_loss did not improve from 0.67853\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6859 - accuracy: 0.5330 - recall: 0.6271 - val_loss: 0.6814 - val_accuracy: 0.5444 - val_recall: 0.6102\n",
      "Epoch 19/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6844 - accuracy: 0.5338 - recall: 0.6341\n",
      "Epoch 19: val_loss did not improve from 0.67853\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6843 - accuracy: 0.5339 - recall: 0.6343 - val_loss: 0.6880 - val_accuracy: 0.5179 - val_recall: 0.6536\n",
      "Epoch 20/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6844 - accuracy: 0.5392 - recall: 0.6221\n",
      "Epoch 20: val_loss did not improve from 0.67853\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6844 - accuracy: 0.5392 - recall: 0.6221 - val_loss: 0.6983 - val_accuracy: 0.4897 - val_recall: 0.6945\n",
      "Epoch 21/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6839 - accuracy: 0.5294 - recall: 0.6294\n",
      "Epoch 21: val_loss did not improve from 0.67853\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6840 - accuracy: 0.5294 - recall: 0.6299 - val_loss: 0.6928 - val_accuracy: 0.5097 - val_recall: 0.6711\n",
      "Epoch 22/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6847 - accuracy: 0.5398 - recall: 0.6142\n",
      "Epoch 22: val_loss did not improve from 0.67853\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6848 - accuracy: 0.5396 - recall: 0.6151 - val_loss: 0.7096 - val_accuracy: 0.4567 - val_recall: 0.7295\n",
      "Epoch 23/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5247 - recall: 0.6378\n",
      "Epoch 23: val_loss did not improve from 0.67853\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6847 - accuracy: 0.5246 - recall: 0.6375 - val_loss: 0.6896 - val_accuracy: 0.5149 - val_recall: 0.6611\n",
      "Epoch 24/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6843 - accuracy: 0.5307 - recall: 0.6345\n",
      "Epoch 24: val_loss improved from 0.67853 to 0.67505, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6842 - accuracy: 0.5308 - recall: 0.6345 - val_loss: 0.6751 - val_accuracy: 0.5652 - val_recall: 0.5885\n",
      "Epoch 25/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6829 - accuracy: 0.5410 - recall: 0.6189\n",
      "Epoch 25: val_loss did not improve from 0.67505\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6829 - accuracy: 0.5411 - recall: 0.6193 - val_loss: 0.6836 - val_accuracy: 0.5377 - val_recall: 0.6285\n",
      "Epoch 26/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.5404 - recall: 0.6273\n",
      "Epoch 26: val_loss did not improve from 0.67505\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6820 - accuracy: 0.5403 - recall: 0.6271 - val_loss: 0.7003 - val_accuracy: 0.4918 - val_recall: 0.6970\n",
      "Epoch 27/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6827 - accuracy: 0.5409 - recall: 0.6296\n",
      "Epoch 27: val_loss did not improve from 0.67505\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6829 - accuracy: 0.5410 - recall: 0.6295 - val_loss: 0.6880 - val_accuracy: 0.5269 - val_recall: 0.6544\n",
      "Epoch 28/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6839 - accuracy: 0.5344 - recall: 0.6234\n",
      "Epoch 28: val_loss did not improve from 0.67505\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6839 - accuracy: 0.5341 - recall: 0.6229 - val_loss: 0.6948 - val_accuracy: 0.5059 - val_recall: 0.6895\n",
      "Epoch 29/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6833 - accuracy: 0.5358 - recall: 0.6267\n",
      "Epoch 29: val_loss did not improve from 0.67505\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6833 - accuracy: 0.5358 - recall: 0.6267 - val_loss: 0.6799 - val_accuracy: 0.5439 - val_recall: 0.6152\n",
      "Epoch 30/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6818 - accuracy: 0.5388 - recall: 0.6359\n",
      "Epoch 30: val_loss did not improve from 0.67505\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6818 - accuracy: 0.5384 - recall: 0.6359 - val_loss: 0.6889 - val_accuracy: 0.5184 - val_recall: 0.6644\n",
      "Epoch 31/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6827 - accuracy: 0.5359 - recall: 0.6229\n",
      "Epoch 31: val_loss did not improve from 0.67505\n",
      "822/822 [==============================] - 11s 14ms/step - loss: 0.6826 - accuracy: 0.5363 - recall: 0.6231 - val_loss: 0.6813 - val_accuracy: 0.5374 - val_recall: 0.6294\n",
      "Epoch 32/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6828 - accuracy: 0.5389 - recall: 0.6275\n",
      "Epoch 32: val_loss did not improve from 0.67505\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6828 - accuracy: 0.5388 - recall: 0.6279 - val_loss: 0.7057 - val_accuracy: 0.4703 - val_recall: 0.7179\n",
      "Epoch 33/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6840 - accuracy: 0.5249 - recall: 0.6417\n",
      "Epoch 33: val_loss improved from 0.67505 to 0.66538, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6838 - accuracy: 0.5249 - recall: 0.6411 - val_loss: 0.6654 - val_accuracy: 0.5900 - val_recall: 0.5409\n",
      "Epoch 34/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6828 - accuracy: 0.5405 - recall: 0.6255\n",
      "Epoch 34: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6826 - accuracy: 0.5405 - recall: 0.6257 - val_loss: 0.6776 - val_accuracy: 0.5534 - val_recall: 0.6110\n",
      "Epoch 35/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6812 - accuracy: 0.5378 - recall: 0.6343\n",
      "Epoch 35: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6812 - accuracy: 0.5378 - recall: 0.6343 - val_loss: 0.6936 - val_accuracy: 0.5040 - val_recall: 0.6728\n",
      "Epoch 36/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.5350 - recall: 0.6255\n",
      "Epoch 36: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6822 - accuracy: 0.5352 - recall: 0.6255 - val_loss: 0.6788 - val_accuracy: 0.5514 - val_recall: 0.6085\n",
      "Epoch 37/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6824 - accuracy: 0.5353 - recall: 0.6215\n",
      "Epoch 37: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6825 - accuracy: 0.5352 - recall: 0.6215 - val_loss: 0.6816 - val_accuracy: 0.5382 - val_recall: 0.6319\n",
      "Epoch 38/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.5358 - recall: 0.6157\n",
      "Epoch 38: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6828 - accuracy: 0.5357 - recall: 0.6155 - val_loss: 0.6992 - val_accuracy: 0.4868 - val_recall: 0.6970\n",
      "Epoch 39/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6831 - accuracy: 0.5366 - recall: 0.6294\n",
      "Epoch 39: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6830 - accuracy: 0.5364 - recall: 0.6289 - val_loss: 0.6928 - val_accuracy: 0.5084 - val_recall: 0.6778\n",
      "Epoch 40/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.5475 - recall: 0.6107\n",
      "Epoch 40: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 6s 7ms/step - loss: 0.6816 - accuracy: 0.5464 - recall: 0.6127 - val_loss: 0.7239 - val_accuracy: 0.4284 - val_recall: 0.7730\n",
      "Epoch 41/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6832 - accuracy: 0.5308 - recall: 0.6332\n",
      "Epoch 41: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6832 - accuracy: 0.5308 - recall: 0.6337 - val_loss: 0.6887 - val_accuracy: 0.5173 - val_recall: 0.6528\n",
      "Epoch 42/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6815 - accuracy: 0.5431 - recall: 0.6126\n",
      "Epoch 42: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6816 - accuracy: 0.5431 - recall: 0.6129 - val_loss: 0.6960 - val_accuracy: 0.4959 - val_recall: 0.6786\n",
      "Epoch 43/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6819 - accuracy: 0.5407 - recall: 0.6219\n",
      "Epoch 43: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6821 - accuracy: 0.5406 - recall: 0.6219 - val_loss: 0.7034 - val_accuracy: 0.4708 - val_recall: 0.7162\n",
      "Epoch 44/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6831 - accuracy: 0.5350 - recall: 0.6181\n",
      "Epoch 44: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6831 - accuracy: 0.5350 - recall: 0.6181 - val_loss: 0.6972 - val_accuracy: 0.4894 - val_recall: 0.6920\n",
      "Epoch 45/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.5380 - recall: 0.6143\n",
      "Epoch 45: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6828 - accuracy: 0.5375 - recall: 0.6151 - val_loss: 0.7074 - val_accuracy: 0.4600 - val_recall: 0.7304\n",
      "Epoch 46/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6818 - accuracy: 0.5372 - recall: 0.6207\n",
      "Epoch 46: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6827 - accuracy: 0.5368 - recall: 0.6209 - val_loss: 0.6955 - val_accuracy: 0.4995 - val_recall: 0.6845\n",
      "Epoch 47/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6815 - accuracy: 0.5378 - recall: 0.6251\n",
      "Epoch 47: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6818 - accuracy: 0.5376 - recall: 0.6253 - val_loss: 0.6852 - val_accuracy: 0.5280 - val_recall: 0.6444\n",
      "Epoch 48/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6812 - accuracy: 0.5419 - recall: 0.6249\n",
      "Epoch 48: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6810 - accuracy: 0.5418 - recall: 0.6251 - val_loss: 0.6887 - val_accuracy: 0.5144 - val_recall: 0.6603\n",
      "Epoch 49/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6810 - accuracy: 0.5363 - recall: 0.6237\n",
      "Epoch 49: val_loss did not improve from 0.66538\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6817 - accuracy: 0.5366 - recall: 0.6241 - val_loss: 0.6706 - val_accuracy: 0.5716 - val_recall: 0.5793\n",
      "Epoch 50/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6817 - accuracy: 0.5361 - recall: 0.6350\n",
      "Epoch 50: val_loss improved from 0.66538 to 0.65900, saving model to model\\best_model_hybrid_lstm.h5\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6816 - accuracy: 0.5363 - recall: 0.6339 - val_loss: 0.6590 - val_accuracy: 0.6020 - val_recall: 0.5326\n",
      "Epoch 51/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6826 - accuracy: 0.5383 - recall: 0.6172\n",
      "Epoch 51: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6823 - accuracy: 0.5381 - recall: 0.6171 - val_loss: 0.6896 - val_accuracy: 0.5111 - val_recall: 0.6686\n",
      "Epoch 52/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6816 - accuracy: 0.5436 - recall: 0.6199\n",
      "Epoch 52: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6816 - accuracy: 0.5436 - recall: 0.6199 - val_loss: 0.7027 - val_accuracy: 0.4798 - val_recall: 0.7137\n",
      "Epoch 53/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6804 - accuracy: 0.5387 - recall: 0.6320\n",
      "Epoch 53: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6809 - accuracy: 0.5392 - recall: 0.6323 - val_loss: 0.6828 - val_accuracy: 0.5332 - val_recall: 0.6319\n",
      "Epoch 54/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6806 - accuracy: 0.5417 - recall: 0.6281\n",
      "Epoch 54: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 5s 6ms/step - loss: 0.6806 - accuracy: 0.5417 - recall: 0.6281 - val_loss: 0.6769 - val_accuracy: 0.5550 - val_recall: 0.6043\n",
      "Epoch 55/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6817 - accuracy: 0.5407 - recall: 0.6130\n",
      "Epoch 55: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6819 - accuracy: 0.5405 - recall: 0.6127 - val_loss: 0.6720 - val_accuracy: 0.5696 - val_recall: 0.5760\n",
      "Epoch 56/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6805 - accuracy: 0.5432 - recall: 0.6195\n",
      "Epoch 56: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 5s 7ms/step - loss: 0.6802 - accuracy: 0.5437 - recall: 0.6203 - val_loss: 0.6799 - val_accuracy: 0.5414 - val_recall: 0.6185\n",
      "Epoch 57/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.5354 - recall: 0.6226\n",
      "Epoch 57: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 9s 11ms/step - loss: 0.6817 - accuracy: 0.5356 - recall: 0.6223 - val_loss: 0.6645 - val_accuracy: 0.5856 - val_recall: 0.5568\n",
      "Epoch 58/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6814 - accuracy: 0.5409 - recall: 0.6242\n",
      "Epoch 58: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6810 - accuracy: 0.5410 - recall: 0.6237 - val_loss: 0.6704 - val_accuracy: 0.5696 - val_recall: 0.5810\n",
      "Epoch 59/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.5464 - recall: 0.6075\n",
      "Epoch 59: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 12s 14ms/step - loss: 0.6816 - accuracy: 0.5462 - recall: 0.6073 - val_loss: 0.7127 - val_accuracy: 0.4518 - val_recall: 0.7437\n",
      "Epoch 60/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6822 - accuracy: 0.5345 - recall: 0.6313\n",
      "Epoch 60: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 12s 15ms/step - loss: 0.6820 - accuracy: 0.5347 - recall: 0.6313 - val_loss: 0.6828 - val_accuracy: 0.5257 - val_recall: 0.6411\n",
      "Epoch 61/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6808 - accuracy: 0.5388 - recall: 0.6176\n",
      "Epoch 61: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 9s 11ms/step - loss: 0.6810 - accuracy: 0.5386 - recall: 0.6185 - val_loss: 0.6987 - val_accuracy: 0.4862 - val_recall: 0.7053\n",
      "Epoch 62/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6823 - accuracy: 0.5327 - recall: 0.6193\n",
      "Epoch 62: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 10s 12ms/step - loss: 0.6820 - accuracy: 0.5328 - recall: 0.6197 - val_loss: 0.6667 - val_accuracy: 0.5789 - val_recall: 0.5618\n",
      "Epoch 63/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6820 - accuracy: 0.5401 - recall: 0.6227\n",
      "Epoch 63: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 10s 12ms/step - loss: 0.6818 - accuracy: 0.5401 - recall: 0.6227 - val_loss: 0.6793 - val_accuracy: 0.5411 - val_recall: 0.6285\n",
      "Epoch 64/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6824 - accuracy: 0.5356 - recall: 0.6199\n",
      "Epoch 64: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 10s 12ms/step - loss: 0.6823 - accuracy: 0.5365 - recall: 0.6207 - val_loss: 0.6681 - val_accuracy: 0.5771 - val_recall: 0.5634\n",
      "Epoch 65/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6823 - accuracy: 0.5374 - recall: 0.6226\n",
      "Epoch 65: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 10s 12ms/step - loss: 0.6818 - accuracy: 0.5378 - recall: 0.6227 - val_loss: 0.6717 - val_accuracy: 0.5666 - val_recall: 0.5785\n",
      "Epoch 66/300\n",
      "818/822 [============================>.] - ETA: 0s - loss: 0.6822 - accuracy: 0.5397 - recall: 0.6070\n",
      "Epoch 66: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 10s 12ms/step - loss: 0.6825 - accuracy: 0.5402 - recall: 0.6077 - val_loss: 0.6755 - val_accuracy: 0.5570 - val_recall: 0.6060\n",
      "Epoch 67/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6819 - accuracy: 0.5446 - recall: 0.6133\n",
      "Epoch 67: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 10s 13ms/step - loss: 0.6818 - accuracy: 0.5447 - recall: 0.6133 - val_loss: 0.6721 - val_accuracy: 0.5686 - val_recall: 0.5801\n",
      "Epoch 68/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6813 - accuracy: 0.5472 - recall: 0.6105\n",
      "Epoch 68: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 10s 12ms/step - loss: 0.6813 - accuracy: 0.5472 - recall: 0.6105 - val_loss: 0.6944 - val_accuracy: 0.5017 - val_recall: 0.6770\n",
      "Epoch 69/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6814 - accuracy: 0.5360 - recall: 0.6243\n",
      "Epoch 69: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 10s 12ms/step - loss: 0.6814 - accuracy: 0.5360 - recall: 0.6243 - val_loss: 0.6796 - val_accuracy: 0.5449 - val_recall: 0.6202\n",
      "Epoch 70/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6813 - accuracy: 0.5443 - recall: 0.6222\n",
      "Epoch 70: val_loss did not improve from 0.65900\n",
      "822/822 [==============================] - 9s 12ms/step - loss: 0.6814 - accuracy: 0.5442 - recall: 0.6219 - val_loss: 0.6900 - val_accuracy: 0.5126 - val_recall: 0.6686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dd87d35ca0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras import regularizers, optimizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.metrics import Recall\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(32, input_shape=(timesteps, input_dim),\n",
    "               kernel_regularizer=regularizers.l2(0.01),\n",
    "               recurrent_regularizer=regularizers.l2(0.01),\n",
    "               bias_regularizer=regularizers.l2(0.01),\n",
    "               dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', Recall()])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "checkpoint = ModelCheckpoint('model/best_model_hybrid_lstm.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7cb3885",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 1s 4ms/step\n",
      "      Prediction  Actual\n",
      "0       0.478685       0\n",
      "1       0.309808       0\n",
      "2       0.535298       0\n",
      "3       0.496480       0\n",
      "4       0.496179       0\n",
      "...          ...     ...\n",
      "8215    0.507135       0\n",
      "8216    0.545234       0\n",
      "8217    0.550317       0\n",
      "8218    0.561816       0\n",
      "8219    0.294682       0\n",
      "\n",
      "[8220 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "best_model = load_model('model/best_model_hybrid_lstm.h5')\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Compare the predictions with the actual values\n",
    "comparison = pd.DataFrame({'Prediction': predictions.flatten(), 'Actual': y_test})\n",
    "\n",
    "# Print the comparison\n",
    "print(comparison)\n",
    "predictions_labels = [1 if p > 0.5 else 0 for p in predictions.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dbb7924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_labels)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "043c253f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.62      0.72      6692\n",
      "           1       0.25      0.55      0.34      1528\n",
      "\n",
      "    accuracy                           0.60      8220\n",
      "   macro avg       0.55      0.59      0.53      8220\n",
      "weighted avg       0.74      0.60      0.65      8220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, predictions_labels, output_dict=True)\n",
    "print(classification_report(y_test, predictions_labels))\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv('result/hybrid_lstm.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
