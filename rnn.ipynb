{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = pd.read_csv('data/data_encoded_scaled.csv')\n",
    "data_cleaned = pd.read_csv('data/data_cleaned.csv')\n",
    "data_feature = data_cleaned.drop(columns=['TOTAL_DELAY', 'DEP_DEL15'])\n",
    "data_target = data_cleaned['DEP_DEL15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded_scaled = data_encoded_scaled.drop(columns=['RESIDUALS', 'DEP_DEL15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.968432919954903\n"
     ]
    }
   ],
   "source": [
    "sequence_days = 7\n",
    "daily_counts = data_feature.groupby(['MONTH', 'DAY_OF_MONTH', 'DEPARTING_AIRPORT']).size() # 一天一个机场的航班数\n",
    "average_rows = daily_counts.mean()\n",
    "\n",
    "print(average_rows*sequence_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full = data_encoded_scaled.assign(TARGET=data_target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MONTH  DAY_OF_MONTH  DAY_OF_WEEK  DEP_TIME_BLK  DISTANCE_GROUP  \\\n",
      "0    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "1    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "2    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "3    0.0           0.0     0.166667      0.166667        0.428571   \n",
      "4    0.0           0.0     0.166667      0.166667        0.285714   \n",
      "\n",
      "   SEGMENT_NUMBER  CONCURRENT_FLIGHTS  NUMBER_OF_SEATS  AIRPORT_FLIGHTS_MONTH  \\\n",
      "0           0.000            0.098592              1.0               0.115453   \n",
      "1           0.000            0.098592              1.0               0.115453   \n",
      "2           0.000            0.112676              1.0               0.111384   \n",
      "3           0.000            0.267606              1.0               0.333661   \n",
      "4           0.125            0.042254              1.0               0.028528   \n",
      "\n",
      "   AIRLINE_FLIGHTS_MONTH  ...  PREVIOUS_AIRPORT_3  PREVIOUS_AIRPORT_4  \\\n",
      "0               0.183515  ...                 0.0                 0.0   \n",
      "1               0.183515  ...                 0.0                 0.0   \n",
      "2               0.183515  ...                 0.0                 0.0   \n",
      "3               0.183515  ...                 0.0                 0.0   \n",
      "4               0.183515  ...                 0.0                 0.0   \n",
      "\n",
      "   PREVIOUS_AIRPORT_5  PREVIOUS_AIRPORT_6  PRCP  SNOW  SNWD      TMAX  \\\n",
      "0                 0.0                 1.0   0.0   0.0   0.0  0.353982   \n",
      "1                 0.0                 1.0   0.0   0.0   0.0  0.353982   \n",
      "2                 0.0                 1.0   0.0   0.0   0.0  0.451327   \n",
      "3                 0.0                 1.0   0.0   0.0   0.0  0.699115   \n",
      "4                 1.0                 0.0   0.0   0.0   0.0  0.460177   \n",
      "\n",
      "       AWND  TARGET  \n",
      "0  0.198638       0  \n",
      "1  0.198638       0  \n",
      "2  0.172291       0  \n",
      "3  0.370930       0  \n",
      "4  0.178804       0  \n",
      "\n",
      "[5 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences shape: (41100, 28, 32)\n",
      "Target values shape: (41100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequence_length = int(round(average_rows*sequence_days))\n",
    "\n",
    "unique_dep_airport = data_feature['DEPARTING_AIRPORT'].unique()\n",
    "unique_flight_number = data_feature['FLIGHT_NUMBER'].unique()\n",
    "\n",
    "X_sequences = []\n",
    "y_targets = []\n",
    "\n",
    "for dep_airport in unique_dep_airport:\n",
    "    flight_data = data_full[data_feature['DEPARTING_AIRPORT'] == dep_airport]\n",
    "    flight_data_values = flight_data.iloc[:, :-1].values\n",
    "    flight_data_target = flight_data.iloc[:, -1].values\n",
    "    for i in range(len(flight_data) - sequence_length):\n",
    "        X_sequences.append(flight_data_values[i:i+sequence_length])\n",
    "        y_targets.append(flight_data_target[i+sequence_length])\n",
    "\n",
    "X_sequences = np.array(X_sequences)\n",
    "y_targets = np.array(y_targets)\n",
    "\n",
    "print(\"Input sequences shape:\", X_sequences.shape)\n",
    "print(\"Target values shape:\", y_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = list(zip(X_sequences, y_targets))\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Separate the sequences and targets\n",
    "X_train, y_train = zip(*train_data)\n",
    "X_test, y_test = zip(*test_data)\n",
    "\n",
    "# Convert the results back to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.keras.models import Sequential\n",
    "from tensorflow.compat.v1.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.metrics import Recall\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "timesteps = X_train.shape[1]\n",
    "input_dim = X_train.shape[2]\n",
    "\n",
    "weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(y_train) , y = y_train)\n",
    "class_weights = dict(zip(np.unique(y_train), weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "810/822 [============================>.] - ETA: 0s - loss: 1.3429 - accuracy: 0.4755 - recall_3: 0.5380\n",
      "Epoch 1: val_loss improved from inf to 1.22907, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 4s 3ms/step - loss: 1.3414 - accuracy: 0.4760 - recall_3: 0.5388 - val_loss: 1.2291 - val_accuracy: 0.5503 - val_recall_3: 0.5209\n",
      "Epoch 2/300\n",
      " 53/822 [>.............................] - ETA: 2s - loss: 1.2775 - accuracy: 0.4941 - recall_3: 0.5401"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Semester 3\\Deep Learning\\flight_delay\\venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "816/822 [============================>.] - ETA: 0s - loss: 1.2122 - accuracy: 0.5116 - recall_3: 0.5271\n",
      "Epoch 2: val_loss improved from 1.22907 to 1.11313, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 1.2117 - accuracy: 0.5120 - recall_3: 0.5272 - val_loss: 1.1131 - val_accuracy: 0.6248 - val_recall_3: 0.4567\n",
      "Epoch 3/300\n",
      "805/822 [============================>.] - ETA: 0s - loss: 1.1155 - accuracy: 0.5220 - recall_3: 0.5173\n",
      "Epoch 3: val_loss improved from 1.11313 to 1.05356, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 1.1147 - accuracy: 0.5222 - recall_3: 0.5187 - val_loss: 1.0536 - val_accuracy: 0.5540 - val_recall_3: 0.5995\n",
      "Epoch 4/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 1.0298 - accuracy: 0.5312 - recall_3: 0.5303\n",
      "Epoch 4: val_loss improved from 1.05356 to 0.98773, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 1.0300 - accuracy: 0.5311 - recall_3: 0.5302 - val_loss: 0.9877 - val_accuracy: 0.5366 - val_recall_3: 0.6469\n",
      "Epoch 5/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.9598 - accuracy: 0.5300 - recall_3: 0.5624\n",
      "Epoch 5: val_loss improved from 0.98773 to 0.92597, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.9595 - accuracy: 0.5301 - recall_3: 0.5650 - val_loss: 0.9260 - val_accuracy: 0.5274 - val_recall_3: 0.6637\n",
      "Epoch 6/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.8997 - accuracy: 0.5313 - recall_3: 0.5775\n",
      "Epoch 6: val_loss improved from 0.92597 to 0.85699, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.8985 - accuracy: 0.5317 - recall_3: 0.5779 - val_loss: 0.8570 - val_accuracy: 0.5824 - val_recall_3: 0.5947\n",
      "Epoch 7/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.8486 - accuracy: 0.5448 - recall_3: 0.5721\n",
      "Epoch 7: val_loss improved from 0.85699 to 0.82554, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.8485 - accuracy: 0.5450 - recall_3: 0.5721 - val_loss: 0.8255 - val_accuracy: 0.5344 - val_recall_3: 0.6629\n",
      "Epoch 8/300\n",
      "811/822 [============================>.] - ETA: 0s - loss: 0.8084 - accuracy: 0.5373 - recall_3: 0.5904\n",
      "Epoch 8: val_loss improved from 0.82554 to 0.77988, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.8084 - accuracy: 0.5377 - recall_3: 0.5886 - val_loss: 0.7799 - val_accuracy: 0.5725 - val_recall_3: 0.6059\n",
      "Epoch 9/300\n",
      "812/822 [============================>.] - ETA: 0s - loss: 0.7774 - accuracy: 0.5423 - recall_3: 0.5977\n",
      "Epoch 9: val_loss improved from 0.77988 to 0.75301, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7767 - accuracy: 0.5423 - recall_3: 0.5986 - val_loss: 0.7530 - val_accuracy: 0.5745 - val_recall_3: 0.6116\n",
      "Epoch 10/300\n",
      "808/822 [============================>.] - ETA: 0s - loss: 0.7543 - accuracy: 0.5523 - recall_3: 0.5756\n",
      "Epoch 10: val_loss did not improve from 0.75301\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7535 - accuracy: 0.5511 - recall_3: 0.5785 - val_loss: 0.7668 - val_accuracy: 0.4735 - val_recall_3: 0.7665\n",
      "Epoch 11/300\n",
      "805/822 [============================>.] - ETA: 0s - loss: 0.7359 - accuracy: 0.5491 - recall_3: 0.5841\n",
      "Epoch 11: val_loss improved from 0.75301 to 0.74416, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7356 - accuracy: 0.5479 - recall_3: 0.5862 - val_loss: 0.7442 - val_accuracy: 0.4965 - val_recall_3: 0.7392\n",
      "Epoch 12/300\n",
      "811/822 [============================>.] - ETA: 0s - loss: 0.7237 - accuracy: 0.5363 - recall_3: 0.6011\n",
      "Epoch 12: val_loss improved from 0.74416 to 0.70780, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7232 - accuracy: 0.5370 - recall_3: 0.6008 - val_loss: 0.7078 - val_accuracy: 0.5690 - val_recall_3: 0.6059\n",
      "Epoch 13/300\n",
      "811/822 [============================>.] - ETA: 0s - loss: 0.7136 - accuracy: 0.5460 - recall_3: 0.5907\n",
      "Epoch 13: val_loss improved from 0.70780 to 0.70418, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7137 - accuracy: 0.5465 - recall_3: 0.5902 - val_loss: 0.7042 - val_accuracy: 0.5596 - val_recall_3: 0.6196\n",
      "Epoch 14/300\n",
      "811/822 [============================>.] - ETA: 0s - loss: 0.7077 - accuracy: 0.5509 - recall_3: 0.5828\n",
      "Epoch 14: val_loss did not improve from 0.70418\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7080 - accuracy: 0.5495 - recall_3: 0.5833 - val_loss: 0.7267 - val_accuracy: 0.4786 - val_recall_3: 0.7648\n",
      "Epoch 15/300\n",
      "810/822 [============================>.] - ETA: 0s - loss: 0.7040 - accuracy: 0.5356 - recall_3: 0.5913\n",
      "Epoch 15: val_loss improved from 0.70418 to 0.68865, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7041 - accuracy: 0.5363 - recall_3: 0.5914 - val_loss: 0.6887 - val_accuracy: 0.5710 - val_recall_3: 0.6027\n",
      "Epoch 16/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.7002 - accuracy: 0.5410 - recall_3: 0.5868\n",
      "Epoch 16: val_loss did not improve from 0.68865\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.7002 - accuracy: 0.5410 - recall_3: 0.5868 - val_loss: 0.6900 - val_accuracy: 0.5587 - val_recall_3: 0.6340\n",
      "Epoch 17/300\n",
      "811/822 [============================>.] - ETA: 0s - loss: 0.6960 - accuracy: 0.5460 - recall_3: 0.5889\n",
      "Epoch 17: val_loss did not improve from 0.68865\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6961 - accuracy: 0.5451 - recall_3: 0.5902 - val_loss: 0.7043 - val_accuracy: 0.5148 - val_recall_3: 0.7103\n",
      "Epoch 18/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6948 - accuracy: 0.5430 - recall_3: 0.5973\n",
      "Epoch 18: val_loss improved from 0.68865 to 0.67808, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6947 - accuracy: 0.5436 - recall_3: 0.5974 - val_loss: 0.6781 - val_accuracy: 0.5880 - val_recall_3: 0.5690\n",
      "Epoch 19/300\n",
      "816/822 [============================>.] - ETA: 0s - loss: 0.6920 - accuracy: 0.5429 - recall_3: 0.6076\n",
      "Epoch 19: val_loss did not improve from 0.67808\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6918 - accuracy: 0.5431 - recall_3: 0.6067 - val_loss: 0.6794 - val_accuracy: 0.5738 - val_recall_3: 0.5955\n",
      "Epoch 20/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6908 - accuracy: 0.5456 - recall_3: 0.5831\n",
      "Epoch 20: val_loss did not improve from 0.67808\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6908 - accuracy: 0.5456 - recall_3: 0.5831 - val_loss: 0.7147 - val_accuracy: 0.4691 - val_recall_3: 0.7729\n",
      "Epoch 21/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6896 - accuracy: 0.5457 - recall_3: 0.5964\n",
      "Epoch 21: val_loss did not improve from 0.67808\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6896 - accuracy: 0.5457 - recall_3: 0.5964 - val_loss: 0.6891 - val_accuracy: 0.5421 - val_recall_3: 0.6573\n",
      "Epoch 22/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6898 - accuracy: 0.5482 - recall_3: 0.5889\n",
      "Epoch 22: val_loss did not improve from 0.67808\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6903 - accuracy: 0.5481 - recall_3: 0.5888 - val_loss: 0.6979 - val_accuracy: 0.5175 - val_recall_3: 0.7039\n",
      "Epoch 23/300\n",
      "804/822 [============================>.] - ETA: 0s - loss: 0.6889 - accuracy: 0.5476 - recall_3: 0.5863\n",
      "Epoch 23: val_loss did not improve from 0.67808\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6890 - accuracy: 0.5469 - recall_3: 0.5890 - val_loss: 0.6951 - val_accuracy: 0.5182 - val_recall_3: 0.7079\n",
      "Epoch 24/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6876 - accuracy: 0.5424 - recall_3: 0.6059\n",
      "Epoch 24: val_loss improved from 0.67808 to 0.66871, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6872 - accuracy: 0.5425 - recall_3: 0.6059 - val_loss: 0.6687 - val_accuracy: 0.5944 - val_recall_3: 0.5690\n",
      "Epoch 25/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.6866 - accuracy: 0.5540 - recall_3: 0.5809\n",
      "Epoch 25: val_loss did not improve from 0.66871\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6867 - accuracy: 0.5528 - recall_3: 0.5815 - val_loss: 0.6967 - val_accuracy: 0.5131 - val_recall_3: 0.7103\n",
      "Epoch 26/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6862 - accuracy: 0.5389 - recall_3: 0.6043\n",
      "Epoch 26: val_loss did not improve from 0.66871\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6865 - accuracy: 0.5397 - recall_3: 0.6029 - val_loss: 0.6748 - val_accuracy: 0.5759 - val_recall_3: 0.6035\n",
      "Epoch 27/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.6868 - accuracy: 0.5529 - recall_3: 0.5882\n",
      "Epoch 27: val_loss did not improve from 0.66871\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6865 - accuracy: 0.5514 - recall_3: 0.5914 - val_loss: 0.6996 - val_accuracy: 0.5021 - val_recall_3: 0.7287\n",
      "Epoch 28/300\n",
      "813/822 [============================>.] - ETA: 0s - loss: 0.6870 - accuracy: 0.5480 - recall_3: 0.5882\n",
      "Epoch 28: val_loss did not improve from 0.66871\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6866 - accuracy: 0.5482 - recall_3: 0.5884 - val_loss: 0.6776 - val_accuracy: 0.5669 - val_recall_3: 0.6188\n",
      "Epoch 29/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5429 - recall_3: 0.5988\n",
      "Epoch 29: val_loss improved from 0.66871 to 0.65477, saving model to model\\best_model_rnn.h5\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6845 - accuracy: 0.5433 - recall_3: 0.5984 - val_loss: 0.6548 - val_accuracy: 0.6396 - val_recall_3: 0.4848\n",
      "Epoch 30/300\n",
      "808/822 [============================>.] - ETA: 0s - loss: 0.6863 - accuracy: 0.5569 - recall_3: 0.5782\n",
      "Epoch 30: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6860 - accuracy: 0.5569 - recall_3: 0.5779 - val_loss: 0.6737 - val_accuracy: 0.5774 - val_recall_3: 0.5955\n",
      "Epoch 31/300\n",
      "808/822 [============================>.] - ETA: 0s - loss: 0.6856 - accuracy: 0.5470 - recall_3: 0.5872\n",
      "Epoch 31: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6850 - accuracy: 0.5468 - recall_3: 0.5876 - val_loss: 0.6720 - val_accuracy: 0.5850 - val_recall_3: 0.5762\n",
      "Epoch 32/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.6840 - accuracy: 0.5496 - recall_3: 0.5911\n",
      "Epoch 32: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6840 - accuracy: 0.5498 - recall_3: 0.5898 - val_loss: 0.6759 - val_accuracy: 0.5751 - val_recall_3: 0.5979\n",
      "Epoch 33/300\n",
      "812/822 [============================>.] - ETA: 0s - loss: 0.6842 - accuracy: 0.5564 - recall_3: 0.5870\n",
      "Epoch 33: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6841 - accuracy: 0.5558 - recall_3: 0.5870 - val_loss: 0.6817 - val_accuracy: 0.5522 - val_recall_3: 0.6324\n",
      "Epoch 34/300\n",
      "810/822 [============================>.] - ETA: 0s - loss: 0.6859 - accuracy: 0.5493 - recall_3: 0.5829\n",
      "Epoch 34: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6856 - accuracy: 0.5485 - recall_3: 0.5841 - val_loss: 0.6877 - val_accuracy: 0.5344 - val_recall_3: 0.6742\n",
      "Epoch 35/300\n",
      "803/822 [============================>.] - ETA: 0s - loss: 0.6853 - accuracy: 0.5486 - recall_3: 0.5893\n",
      "Epoch 35: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6848 - accuracy: 0.5492 - recall_3: 0.5904 - val_loss: 0.6773 - val_accuracy: 0.5684 - val_recall_3: 0.6228\n",
      "Epoch 36/300\n",
      "817/822 [============================>.] - ETA: 0s - loss: 0.6836 - accuracy: 0.5530 - recall_3: 0.5844\n",
      "Epoch 36: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6834 - accuracy: 0.5528 - recall_3: 0.5845 - val_loss: 0.6943 - val_accuracy: 0.5116 - val_recall_3: 0.7047\n",
      "Epoch 37/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6857 - accuracy: 0.5395 - recall_3: 0.5875\n",
      "Epoch 37: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6856 - accuracy: 0.5397 - recall_3: 0.5874 - val_loss: 0.6779 - val_accuracy: 0.5617 - val_recall_3: 0.6388\n",
      "Epoch 38/300\n",
      "821/822 [============================>.] - ETA: 0s - loss: 0.6843 - accuracy: 0.5397 - recall_3: 0.6019\n",
      "Epoch 38: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6844 - accuracy: 0.5398 - recall_3: 0.6023 - val_loss: 0.6610 - val_accuracy: 0.6081 - val_recall_3: 0.5377\n",
      "Epoch 39/300\n",
      "822/822 [==============================] - ETA: 0s - loss: 0.6840 - accuracy: 0.5472 - recall_3: 0.5805\n",
      "Epoch 39: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6840 - accuracy: 0.5472 - recall_3: 0.5805 - val_loss: 0.6677 - val_accuracy: 0.5861 - val_recall_3: 0.5875\n",
      "Epoch 40/300\n",
      "820/822 [============================>.] - ETA: 0s - loss: 0.6835 - accuracy: 0.5521 - recall_3: 0.5861\n",
      "Epoch 40: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6836 - accuracy: 0.5520 - recall_3: 0.5860 - val_loss: 0.6919 - val_accuracy: 0.5128 - val_recall_3: 0.7030\n",
      "Epoch 41/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6840 - accuracy: 0.5439 - recall_3: 0.5916\n",
      "Epoch 41: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6839 - accuracy: 0.5443 - recall_3: 0.5906 - val_loss: 0.6594 - val_accuracy: 0.6223 - val_recall_3: 0.5305\n",
      "Epoch 42/300\n",
      "809/822 [============================>.] - ETA: 0s - loss: 0.6840 - accuracy: 0.5455 - recall_3: 0.5853\n",
      "Epoch 42: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6842 - accuracy: 0.5460 - recall_3: 0.5841 - val_loss: 0.6668 - val_accuracy: 0.5943 - val_recall_3: 0.5730\n",
      "Epoch 43/300\n",
      "815/822 [============================>.] - ETA: 0s - loss: 0.6832 - accuracy: 0.5557 - recall_3: 0.5851\n",
      "Epoch 43: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6833 - accuracy: 0.5559 - recall_3: 0.5853 - val_loss: 0.6762 - val_accuracy: 0.5602 - val_recall_3: 0.6300\n",
      "Epoch 44/300\n",
      "806/822 [============================>.] - ETA: 0s - loss: 0.6815 - accuracy: 0.5530 - recall_3: 0.5951\n",
      "Epoch 44: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6823 - accuracy: 0.5521 - recall_3: 0.5948 - val_loss: 0.6806 - val_accuracy: 0.5546 - val_recall_3: 0.6453\n",
      "Epoch 45/300\n",
      "807/822 [============================>.] - ETA: 0s - loss: 0.6838 - accuracy: 0.5469 - recall_3: 0.5927\n",
      "Epoch 45: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6837 - accuracy: 0.5468 - recall_3: 0.5912 - val_loss: 0.6719 - val_accuracy: 0.5801 - val_recall_3: 0.5955\n",
      "Epoch 46/300\n",
      "814/822 [============================>.] - ETA: 0s - loss: 0.6827 - accuracy: 0.5486 - recall_3: 0.5856\n",
      "Epoch 46: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6831 - accuracy: 0.5490 - recall_3: 0.5841 - val_loss: 0.6704 - val_accuracy: 0.5847 - val_recall_3: 0.5923\n",
      "Epoch 47/300\n",
      "811/822 [============================>.] - ETA: 0s - loss: 0.6846 - accuracy: 0.5514 - recall_3: 0.5761\n",
      "Epoch 47: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6840 - accuracy: 0.5504 - recall_3: 0.5771 - val_loss: 0.6999 - val_accuracy: 0.4903 - val_recall_3: 0.7432\n",
      "Epoch 48/300\n",
      "819/822 [============================>.] - ETA: 0s - loss: 0.6833 - accuracy: 0.5460 - recall_3: 0.5963\n",
      "Epoch 48: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 2s 3ms/step - loss: 0.6833 - accuracy: 0.5461 - recall_3: 0.5966 - val_loss: 0.6773 - val_accuracy: 0.5627 - val_recall_3: 0.6300\n",
      "Epoch 49/300\n",
      "804/822 [============================>.] - ETA: 0s - loss: 0.6827 - accuracy: 0.5490 - recall_3: 0.5940\n",
      "Epoch 49: val_loss did not improve from 0.65477\n",
      "822/822 [==============================] - 3s 3ms/step - loss: 0.6826 - accuracy: 0.5496 - recall_3: 0.5950 - val_loss: 0.6873 - val_accuracy: 0.5286 - val_recall_3: 0.6854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2428da5e0a0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the RNN model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(32, input_shape=(timesteps, input_dim),\n",
    "                    kernel_regularizer=regularizers.l2(0.01),\n",
    "                    recurrent_regularizer=regularizers.l2(0.01),\n",
    "                    bias_regularizer=regularizers.l2(0.01),\n",
    "                     dropout=0.4, recurrent_dropout=0.4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', Recall()])\n",
    "\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "checkpoint = ModelCheckpoint('model/best_model_rnn.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping, checkpoint], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 0s 1ms/step\n",
      "      Prediction  Actual\n",
      "0       0.494957       0\n",
      "1       0.407993       0\n",
      "2       0.582953       0\n",
      "3       0.579393       0\n",
      "4       0.486182       1\n",
      "...          ...     ...\n",
      "8215    0.477967       0\n",
      "8216    0.460715       0\n",
      "8217    0.330314       0\n",
      "8218    0.523494       0\n",
      "8219    0.449205       0\n",
      "\n",
      "[8220 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the model\n",
    "best_model = load_model('model/best_model_rnn.h5')\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Compare the predictions with the actual values\n",
    "comparison = pd.DataFrame({'Prediction': predictions.flatten(), 'Actual': y_test})\n",
    "\n",
    "# Print the comparison\n",
    "print(comparison)\n",
    "predictions_labels = [1 if p > 0.5 else 0 for p in predictions.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions_labels)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.67      0.75      6715\n",
      "           1       0.24      0.46      0.32      1505\n",
      "\n",
      "    accuracy                           0.63      8220\n",
      "   macro avg       0.54      0.57      0.53      8220\n",
      "weighted avg       0.74      0.63      0.67      8220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, predictions_labels, output_dict=True)\n",
    "print(classification_report(y_test, predictions_labels))\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv('result/rnn.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
